{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b6c6f-3ab7-42b2-b932-f6facafa85d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HalfCheetah environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phalle.y/.local/lib/python3.8/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PPO model...\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phalle.y/.local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training logger...\n",
      "Starting training for 1,000,000 timesteps...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -345     |\n",
      "| time/              |          |\n",
      "|    fps             | 689      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -343        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 524         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010030727 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.5        |\n",
      "|    explained_variance   | -0.0253     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.99        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 42.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -355        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009176685 |\n",
      "|    clip_fraction        | 0.0795      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.48       |\n",
      "|    explained_variance   | 0.0949      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.72        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -331        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009326238 |\n",
      "|    clip_fraction        | 0.086       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.45       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -329       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00978829 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.43      |\n",
      "|    explained_variance   | 0.626      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.55       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.985      |\n",
      "|    value_loss           | 15.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -323        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009633602 |\n",
      "|    clip_fraction        | 0.0973      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.4        |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.37        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -321        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008937281 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.37       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.67        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 19.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -322        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 430         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008048431 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.35       |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 25.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -328        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009552892 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.33       |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.971       |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -329        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012223689 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.33       |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.39        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -323        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013321775 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.28       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.12        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    std                  | 0.957       |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class TrainingLogger(BaseCallback):\n",
    "    def __init__(self, log_filepath=\"default2.txt\", save_dir=\"model_checkpoints\", \n",
    "                 save_freq=50000, model_prefix=\"halfcheetah_ppo2\"):\n",
    "        super().__init__(verbose=0)\n",
    "        self.log_filepath = log_filepath\n",
    "        self.save_dir = save_dir\n",
    "        self.save_freq = save_freq\n",
    "        self.model_prefix = model_prefix\n",
    "        self.episode_total_reward = 0\n",
    "        self.control_costs = 0\n",
    "        self.last_save = 0\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "    def _on_step(self):\n",
    "        # Save model at regular intervals\n",
    "        if self.num_timesteps >= self.last_save + self.save_freq:\n",
    "            try:\n",
    "                # Save the model\n",
    "                model_path = os.path.join(self.save_dir, \n",
    "                                        f\"{self.model_prefix}_{self.num_timesteps}\")\n",
    "                self.model.save(model_path)\n",
    "                print(f\"Model saved at {model_path}\")\n",
    "                self.last_save = self.num_timesteps\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model at timestep {self.num_timesteps}: {e}\")\n",
    "        \n",
    "        # Log rewards and metrics\n",
    "        current_reward = self.locals.get(\"rewards\")[0] if self.locals.get(\"rewards\") is not None else 0\n",
    "        self.episode_total_reward += current_reward\n",
    "        \n",
    "        environment_info = self.locals.get(\"infos\")[0] if self.locals.get(\"infos\") is not None else {}\n",
    "        episode_done = self.locals.get(\"dones\")[0] if self.locals.get(\"dones\") is not None else False\n",
    "        \n",
    "        # Track control costs\n",
    "        if \"reward_ctrl\" in environment_info:\n",
    "            self.control_costs += abs(environment_info[\"reward_ctrl\"])\n",
    "        \n",
    "        if episode_done:\n",
    "            distance_traveled = environment_info.get(\"x_position\", 0)\n",
    "            with open(self.log_filepath, \"a\") as log_file:\n",
    "                log_file.write(f\"{self.num_timesteps},{self.episode_total_reward:.4f},{distance_traveled:.4f},{self.control_costs:.4f}\\n\")\n",
    "            self.episode_total_reward = 0\n",
    "            self.control_costs = 0\n",
    "        return True\n",
    "    \n",
    "    def _on_training_start(self):\n",
    "        with open(self.log_filepath, \"w\") as log_file:\n",
    "            log_file.write(\"timestep,reward,distance,control_cost\\n\")\n",
    "\n",
    "def train_halfcheetah_with_ppo():\n",
    "    try:\n",
    "        # Setup environment - using standard environment without reward modification\n",
    "        print(\"Creating HalfCheetah environment...\")\n",
    "        cheetah_environment = gym.make(\"HalfCheetah-v4\")\n",
    "\n",
    "        # Initialize PPO model with default hyperparameters\n",
    "        print(\"Initializing PPO model...\")\n",
    "        training_model = PPO(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=cheetah_environment,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Setup logger with checkpoint saving capability\n",
    "        print(\"Setting up training logger...\")\n",
    "        progress_logger = TrainingLogger(\n",
    "            log_filepath=\"halfcheetah_ppo_training2.txt\",\n",
    "            save_dir=\"model_checkpoints\",\n",
    "            save_freq=50000,\n",
    "            model_prefix=\"halfcheetah_ppo2\"\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Starting training for 1,000,000 timesteps...\")\n",
    "        training_model.learn(total_timesteps=1000000, callback=progress_logger)\n",
    "\n",
    "        # Save the final model\n",
    "        print(\"Saving final trained model...\")\n",
    "        training_model.save(\"halfcheetah_ppo_final2\")\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Training logs saved to {progress_logger.log_filepath}\")\n",
    "        print(f\"Model checkpoints saved in {progress_logger.save_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training error occurred: {str(e)}\")\n",
    "        print(\"Training was interrupted, but intermediate models should be saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_halfcheetah_with_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23cc9ed-0538-4ab0-a36d-29715f25dded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
