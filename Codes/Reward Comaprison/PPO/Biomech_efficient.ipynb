{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3615d-a15d-4caf-951f-7a540608e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating efficient cheetah environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phalle.y/.local/lib/python3.8/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PPO model...\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phalle.y/.local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training logger...\n",
      "Starting training for 1,000,000 timesteps...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 341      |\n",
      "| time/              |          |\n",
      "|    fps             | 567      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 356         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 429         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011172911 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.49       |\n",
      "|    explained_variance   | 0.00276     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.95        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 34.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 301         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010932899 |\n",
      "|    clip_fraction        | 0.0998      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.45       |\n",
      "|    explained_variance   | 0.0156      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.77        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 19          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 284        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00895239 |\n",
      "|    clip_fraction        | 0.075      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.41      |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.6       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.98       |\n",
      "|    value_loss           | 36.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 300         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010269343 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.38       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 38.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 312         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 364         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014097981 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.33       |\n",
      "|    explained_variance   | -0.056      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.85        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    std                  | 0.963       |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 304         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008684678 |\n",
      "|    clip_fraction        | 0.0805      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.27       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.61        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 27.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 265         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006841203 |\n",
      "|    clip_fraction        | 0.0395      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.4        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 261         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 338         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007127811 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 49.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 266         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 333         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009167129 |\n",
      "|    clip_fraction        | 0.0927      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.26       |\n",
      "|    explained_variance   | 0.484       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 268         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009890766 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.24       |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    std                  | 0.955       |\n",
      "|    value_loss           | 22.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 280         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 325         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008621554 |\n",
      "|    clip_fraction        | 0.0745      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.23       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 274         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 322         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011824681 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.21       |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.949       |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class EnhancedTrainingLogger(BaseCallback):\n",
    "    def __init__(self, log_filepath=\"efficient_cheetah_ppo_training2.txt\", save_dir=\"model_checkpoints\", \n",
    "                 save_freq=50000, model_prefix=\"efficient_cheetah_ppo2\"):\n",
    "        super().__init__(verbose=0)\n",
    "        self.log_filepath = log_filepath\n",
    "        self.save_dir = save_dir\n",
    "        self.save_freq = save_freq\n",
    "        self.model_prefix = model_prefix\n",
    "        self.episode_total_reward = 0\n",
    "        self.control_costs = 0\n",
    "        self.last_save = 0\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "    def _on_step(self):\n",
    "        # Save model at regular intervals\n",
    "        if self.num_timesteps >= self.last_save + self.save_freq:\n",
    "            try:\n",
    "                # Save the model\n",
    "                model_path = os.path.join(self.save_dir, \n",
    "                                        f\"{self.model_prefix}_{self.num_timesteps}\")\n",
    "                self.model.save(model_path)\n",
    "                print(f\"Model saved at {model_path}\")\n",
    "                self.last_save = self.num_timesteps\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model at timestep {self.num_timesteps}: {e}\")\n",
    "        \n",
    "        # Log rewards and metrics\n",
    "        current_reward = self.locals.get(\"rewards\")[0] if self.locals.get(\"rewards\") is not None else 0\n",
    "        self.episode_total_reward += current_reward\n",
    "        \n",
    "        environment_info = self.locals.get(\"infos\")[0] if self.locals.get(\"infos\") is not None else {}\n",
    "        episode_done = self.locals.get(\"dones\")[0] if self.locals.get(\"dones\") is not None else False\n",
    "        \n",
    "        # Track control costs\n",
    "        if \"reward_ctrl\" in environment_info:\n",
    "            self.control_costs += abs(environment_info[\"reward_ctrl\"])\n",
    "        \n",
    "        if episode_done:\n",
    "            distance_traveled = environment_info.get(\"x_position\", 0)\n",
    "            with open(self.log_filepath, \"a\") as log_file:\n",
    "                log_file.write(f\"{self.num_timesteps},{self.episode_total_reward:.4f},{distance_traveled:.4f},{self.control_costs:.4f}\\n\")\n",
    "            self.episode_total_reward = 0\n",
    "            self.control_costs = 0\n",
    "        return True\n",
    "    \n",
    "    def _on_training_start(self):\n",
    "        with open(self.log_filepath, \"w\") as log_file:\n",
    "            log_file.write(\"timestep,reward,distance,control_cost\\n\")\n",
    "\n",
    "class EfficientCheetahEnv(gym.Wrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__(gym.make(\"HalfCheetah-v4\"))\n",
    "        \n",
    "    def step(self, action):\n",
    "        observation, original_reward, terminated, truncated, info = self.env.step(action)\n",
    "        modified_reward = self.ideal_reward_function(observation, action, original_reward, info)\n",
    "        return observation, modified_reward, terminated, truncated, info\n",
    "    \n",
    "    def ideal_reward_function(self, observation, action, original_reward, info):\n",
    "        # 1. Maintain core elements of original reward (forward progress minus control costs)\n",
    "        original_reward_weight = 1.0\n",
    "        \n",
    "        # 2. Efficient torso orientation - true torso angle from observation[1]\n",
    "        torso_angle = observation[1]  # rooty - actual torso orientation\n",
    "        # Slightly forward-leaning posture (about 0.2 rad) is efficient for running\n",
    "        posture_reward = 0.3 * np.exp(-5 * (torso_angle - 0.2)**2)\n",
    "        \n",
    "        # 3. Coordinated leg movement - emulate galloping gait\n",
    "        # Synchronize front and back leg patterns\n",
    "        back_thigh_vel = observation[11]  # Angular velocity of back thigh\n",
    "        front_thigh_vel = observation[14]  # Angular velocity of front thigh\n",
    "        # Reward opposite movement (galloping pattern)\n",
    "        gait_reward = 0.2 * np.exp(-2 * (back_thigh_vel + front_thigh_vel)**2)\n",
    "        \n",
    "        # 4. Energy efficiency - discourage wasteful actions\n",
    "        # Penalize large action magnitudes while still allowing powerful movements when needed\n",
    "        energy_efficiency = 0.1 * (1.0 - min(1.0, np.mean(np.abs(action))))\n",
    "        \n",
    "        # 5. Anti-inversion component - prevent flipping\n",
    "        # Penalize when torso starts to flip upside down\n",
    "        anti_inversion = -0.5 * min(0, np.cos(torso_angle))\n",
    "        \n",
    "        # 6. Foot ground contact reward - efficient push-off\n",
    "        back_foot_angle = observation[4]   # bfoot angle\n",
    "        front_foot_angle = observation[7]  # ffoot angle\n",
    "        # Reward proper foot positioning for push-off\n",
    "        stance_reward = 0.1 * (np.cos(back_foot_angle) + np.cos(front_foot_angle))\n",
    "        \n",
    "        # Combined reward\n",
    "        reward = (posture_reward + gait_reward + energy_efficiency + \n",
    "                  anti_inversion + stance_reward + \n",
    "                  original_reward_weight * original_reward)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "def train_efficient_cheetah_with_ppo():\n",
    "    try:\n",
    "        # Setup environment\n",
    "        print(\"Creating efficient cheetah environment...\")\n",
    "        cheetah_environment = EfficientCheetahEnv()\n",
    "\n",
    "        # Initialize PPO model with default hyperparameters\n",
    "        print(\"Initializing PPO model...\")\n",
    "        training_model = PPO(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=cheetah_environment,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Setup logger with checkpoint saving capability\n",
    "        print(\"Setting up training logger...\")\n",
    "        progress_logger = EnhancedTrainingLogger(\n",
    "            log_filepath=\"efficient_cheetah_ppo_training2.txt\",\n",
    "            save_dir=\"model_checkpoints\",\n",
    "            save_freq=50000,\n",
    "            model_prefix=\"efficient_cheetah_ppo2\"\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Starting training for 1,000,000 timesteps...\")\n",
    "        training_model.learn(total_timesteps=1000000, callback=progress_logger)\n",
    "\n",
    "        # Save the final model\n",
    "        print(\"Saving final trained model...\")\n",
    "        training_model.save(\"efficient_cheetah_ppo_final2\")\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Training logs saved to {progress_logger.log_filepath}\")\n",
    "        print(f\"Model checkpoints saved in {progress_logger.save_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training error occurred: {str(e)}\")\n",
    "        print(\"Training was interrupted, but intermediate models should be saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_efficient_cheetah_with_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533d440-97a9-45f0-93d0-30d69526ad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
