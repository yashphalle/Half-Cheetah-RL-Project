{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06d6a1-09c1-48ee-bd99-49c6c7bcd2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gait optimization cheetah environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phalle.y/.local/lib/python3.8/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PPO model...\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phalle.y/.local/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training logger...\n",
      "Starting training for 1,000,000 timesteps...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -118     |\n",
      "| time/              |          |\n",
      "|    fps             | 453      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -151        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011106249 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.51       |\n",
      "|    explained_variance   | -0.0518     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.74        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 16.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -133        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008024044 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.49       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.18        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 23.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -150        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008648537 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.46       |\n",
      "|    explained_variance   | 0.324       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.27        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 20.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -164        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 308         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007002216 |\n",
      "|    clip_fraction        | 0.0519      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.44       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 33.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -168        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008894959 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.44       |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 34.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -154         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 303          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073387357 |\n",
      "|    clip_fraction        | 0.0449       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.44        |\n",
      "|    explained_variance   | 0.369        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.7         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 51.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | -147         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 297          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073396065 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -8.43        |\n",
      "|    explained_variance   | 0.534        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.2         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0158      |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 35.9         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | -148      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 293       |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 62        |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0104258 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.41     |\n",
      "|    explained_variance   | 0.443     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.45      |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.0226   |\n",
      "|    std                  | 0.982     |\n",
      "|    value_loss           | 24.2      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | -154        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010453738 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -8.4        |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.32        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 16.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -147       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 287        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00781635 |\n",
      "|    clip_fraction        | 0.0649     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.37      |\n",
      "|    explained_variance   | 0.49       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.46       |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.972      |\n",
      "|    value_loss           | 22.7       |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class EnhancedTrainingLogger(BaseCallback):\n",
    "    def __init__(self, log_filepath=\"gait_optimization_ppo_training2.txt\", save_dir=\"model_checkpoints\", \n",
    "                 save_freq=50000, model_prefix=\"gait_optimization_ppo2\"):\n",
    "        super().__init__(verbose=0)\n",
    "        self.log_filepath = log_filepath\n",
    "        self.save_dir = save_dir\n",
    "        self.save_freq = save_freq\n",
    "        self.model_prefix = model_prefix\n",
    "        self.episode_total_reward = 0\n",
    "        self.control_costs = 0\n",
    "        self.last_save = 0\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "    def _on_step(self):\n",
    "        # Save model at regular intervals\n",
    "        if self.num_timesteps >= self.last_save + self.save_freq:\n",
    "            try:\n",
    "                # Save the model\n",
    "                model_path = os.path.join(self.save_dir, \n",
    "                                        f\"{self.model_prefix}_{self.num_timesteps}\")\n",
    "                self.model.save(model_path)\n",
    "                print(f\"Model saved at {model_path}\")\n",
    "                self.last_save = self.num_timesteps\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model at timestep {self.num_timesteps}: {e}\")\n",
    "        \n",
    "        # Log rewards and metrics\n",
    "        current_reward = self.locals.get(\"rewards\")[0] if self.locals.get(\"rewards\") is not None else 0\n",
    "        self.episode_total_reward += current_reward\n",
    "        \n",
    "        environment_info = self.locals.get(\"infos\")[0] if self.locals.get(\"infos\") is not None else {}\n",
    "        episode_done = self.locals.get(\"dones\")[0] if self.locals.get(\"dones\") is not None else False\n",
    "        \n",
    "        # Track control costs\n",
    "        if \"reward_ctrl\" in environment_info:\n",
    "            self.control_costs += abs(environment_info[\"reward_ctrl\"])\n",
    "        \n",
    "        if episode_done:\n",
    "            distance_traveled = environment_info.get(\"x_position\", 0)\n",
    "            with open(self.log_filepath, \"a\") as log_file:\n",
    "                log_file.write(f\"{self.num_timesteps},{self.episode_total_reward:.4f},{distance_traveled:.4f},{self.control_costs:.4f}\\n\")\n",
    "            self.episode_total_reward = 0\n",
    "            self.control_costs = 0\n",
    "        return True\n",
    "    \n",
    "    def _on_training_start(self):\n",
    "        with open(self.log_filepath, \"w\") as log_file:\n",
    "            log_file.write(\"timestep,reward,distance,control_cost\\n\")\n",
    "\n",
    "class GaitOptimizationCheetahEnv(gym.Wrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__(gym.make(\"HalfCheetah-v4\"))\n",
    "        # Keep track of previous observations to measure smoothness\n",
    "        self.prev_obs = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.prev_obs = obs\n",
    "        return obs, info\n",
    "        \n",
    "    def step(self, action):\n",
    "        observation, original_reward, terminated, truncated, info = self.env.step(action)\n",
    "        modified_reward = self.gait_optimization_reward(observation, action, original_reward, info)\n",
    "        \n",
    "        # Update previous observation\n",
    "        self.prev_obs = observation\n",
    "        \n",
    "        return observation, modified_reward, terminated, truncated, info\n",
    "    \n",
    "    def gait_optimization_reward(self, observation, action, original_reward, info):\n",
    "        # 1. Original reward component\n",
    "        original_reward_weight = 1.0\n",
    "        \n",
    "        # 2. Movement smoothness component\n",
    "        # Calculate joint acceleration (change in velocities)\n",
    "        if self.prev_obs is not None:\n",
    "            current_velocities = observation[10:17]\n",
    "            prev_velocities = self.prev_obs[10:17]\n",
    "            joint_accelerations = np.abs(current_velocities - prev_velocities)\n",
    "            # Penalize high accelerations (jerky movements)\n",
    "            smoothness_reward = 0.2 * np.exp(-0.5 * np.mean(joint_accelerations))\n",
    "        else:\n",
    "            smoothness_reward = 0\n",
    "        \n",
    "        # 3. Energy efficiency through spring-like behavior\n",
    "        # In biological systems, muscles and tendons store and release energy like springs\n",
    "        # This term rewards joint movements that follow a spring-like pattern\n",
    "        joint_angles = observation[2:8]  # All joint angles\n",
    "        joint_velocities = observation[10:17]  # All joint velocities\n",
    "        \n",
    "        # Calculate spring-like behavior score\n",
    "        # When a joint is extended, it should be moving back (negative velocity)\n",
    "        # When a joint is flexed, it should be extending (positive velocity)\n",
    "        # This creates a spring-like oscillation\n",
    "        spring_score = 0\n",
    "        for i in range(len(joint_angles)):\n",
    "            # If joint is extended (positive angle), it should have negative velocity\n",
    "            # If joint is flexed (negative angle), it should have positive velocity\n",
    "            # This creates a spring-like oscillation\n",
    "            spring_like = -1 * joint_angles[i] * joint_velocities[i]\n",
    "            spring_score += max(0, spring_like)\n",
    "        \n",
    "        spring_reward = 0.15 * min(1.0, spring_score / 3.0)\n",
    "        \n",
    "        # 4. Gait symmetry reward - encourage alternating leg movements\n",
    "        # Calculate phase relationships between front and back legs\n",
    "        if len(joint_velocities) >= 5:\n",
    "            # Back thigh vs front thigh\n",
    "            back_thigh_vel = observation[11]\n",
    "            front_thigh_vel = observation[14]\n",
    "            # Reward when they're out of phase (one positive, one negative)\n",
    "            phase_score = back_thigh_vel * front_thigh_vel\n",
    "            symmetry_reward = 0.1 * np.exp(-2.0 * max(0, phase_score))\n",
    "        else:\n",
    "            symmetry_reward = 0\n",
    "        \n",
    "        # 5. Power-to-speed efficiency\n",
    "        forward_reward = info.get(\"reward_run\", 0)\n",
    "        control_cost = abs(info.get(\"reward_ctrl\", 0))\n",
    "        \n",
    "        # If making forward progress, calculate efficiency\n",
    "        if forward_reward > 0 and control_cost > 0:\n",
    "            # Higher ratio means more forward movement for energy expended\n",
    "            efficiency_ratio = forward_reward / (control_cost + 0.1)  # Add 0.1 to avoid division by zero\n",
    "            efficiency_reward = 0.2 * min(1.0, efficiency_ratio / 10.0)\n",
    "        else:\n",
    "            efficiency_reward = 0\n",
    "                \n",
    "        # Combined reward\n",
    "        reward = (original_reward_weight * original_reward + \n",
    "                 smoothness_reward + spring_reward + \n",
    "                 symmetry_reward + efficiency_reward)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "def train_gait_optimization_with_ppo():\n",
    "    try:\n",
    "        # Setup environment\n",
    "        print(\"Creating gait optimization cheetah environment...\")\n",
    "        cheetah_environment = GaitOptimizationCheetahEnv()\n",
    "\n",
    "        # Initialize PPO model with default hyperparameters\n",
    "        print(\"Initializing PPO model...\")\n",
    "        training_model = PPO(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=cheetah_environment,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Setup logger with checkpoint saving capability\n",
    "        print(\"Setting up training logger...\")\n",
    "        progress_logger = EnhancedTrainingLogger(\n",
    "            log_filepath=\"gait_optimization_ppo_training2.txt\",\n",
    "            save_dir=\"model_checkpoints\", \n",
    "            save_freq=50000,\n",
    "            model_prefix=\"gait_optimization_ppo2\"\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(\"Starting training for 1,000,000 timesteps...\")\n",
    "        training_model.learn(total_timesteps=1000000, callback=progress_logger)\n",
    "\n",
    "        # Save the final model\n",
    "        print(\"Saving final trained model...\")\n",
    "        training_model.save(\"gait_optimization_ppo_final2\")\n",
    "\n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Training logs saved to {progress_logger.log_filepath}\")\n",
    "        print(f\"Model checkpoints saved in {progress_logger.save_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training error occurred: {str(e)}\")\n",
    "        print(\"Training was interrupted, but intermediate models should be saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_gait_optimization_with_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f1dbe-0ba8-4722-9262-caf937cbd6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
