{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45de45-0356-46fa-bb92-67b993ad664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Timestep: 1000, Episode Reward: -365.123669\n",
      "Timestep: 2000, Episode Reward: -332.606008\n",
      "Timestep: 3000, Episode Reward: -83.56559\n",
      "Timestep: 4000, Episode Reward: -272.759661\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -264     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 3523     |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Timestep: 5000, Episode Reward: -224.622577\n",
      "Timestep: 6000, Episode Reward: -274.958887\n",
      "Timestep: 7000, Episode Reward: -312.808609\n",
      "Timestep: 8000, Episode Reward: -241.482016\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -263     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 3572     |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Timestep: 9000, Episode Reward: -340.832234\n",
      "Timestep: 10000, Episode Reward: -315.994784\n",
      "Timestep: 11000, Episode Reward: -21.652946\n",
      "Timestep: 12000, Episode Reward: -549.777782\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -278     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 1250     |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.36    |\n",
      "|    critic_loss     | 0.222    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1000     |\n",
      "---------------------------------\n",
      "Timestep: 13000, Episode Reward: -525.680306\n",
      "Timestep: 14000, Episode Reward: -473.296953\n",
      "Timestep: 15000, Episode Reward: -558.362688\n",
      "Timestep: 16000, Episode Reward: -553.410353\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -340     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 470      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.3    |\n",
      "|    critic_loss     | 0.875    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5000     |\n",
      "---------------------------------\n",
      "Timestep: 17000, Episode Reward: -175.774168\n",
      "Timestep: 18000, Episode Reward: -117.524724\n",
      "Timestep: 19000, Episode Reward: -369.134207\n",
      "Timestep: 20000, Episode Reward: -336.042045\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -322     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 342      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.9    |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9000     |\n",
      "---------------------------------\n",
      "Timestep: 21000, Episode Reward: 133.449775\n",
      "Timestep: 22000, Episode Reward: -193.869987\n",
      "Timestep: 23000, Episode Reward: 220.134558\n",
      "Timestep: 24000, Episode Reward: 499.383648\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -241     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 289      |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 24000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.8    |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13000    |\n",
      "---------------------------------\n",
      "Timestep: 25000, Episode Reward: 389.724312\n",
      "Timestep: 26000, Episode Reward: 501.452696\n",
      "Timestep: 27000, Episode Reward: 870.842888\n",
      "Timestep: 28000, Episode Reward: 191.907782\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -137     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 260      |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.3    |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 17000    |\n",
      "---------------------------------\n",
      "Timestep: 29000, Episode Reward: 633.270577\n",
      "Timestep: 30000, Episode Reward: 239.716173\n",
      "Timestep: 31000, Episode Reward: 893.222694\n",
      "Timestep: 32000, Episode Reward: -534.243658\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -81.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 242      |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 32000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.1    |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 21000    |\n",
      "---------------------------------\n",
      "Timestep: 33000, Episode Reward: 1003.922367\n",
      "Timestep: 34000, Episode Reward: 763.145907\n",
      "Timestep: 35000, Episode Reward: -516.09266\n",
      "Timestep: 36000, Episode Reward: 64.793549\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -35.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 230      |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 36000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -111     |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 25000    |\n",
      "---------------------------------\n",
      "Timestep: 37000, Episode Reward: 937.155838\n",
      "Timestep: 38000, Episode Reward: 1134.862438\n",
      "Timestep: 39000, Episode Reward: -100.83561\n",
      "Timestep: 40000, Episode Reward: 1016.134159\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 42.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 221      |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 3.45     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 29000    |\n",
      "---------------------------------\n",
      "Timestep: 41000, Episode Reward: -56.294054\n",
      "Timestep: 42000, Episode Reward: 1458.182028\n",
      "Timestep: 43000, Episode Reward: 1202.163103\n",
      "Timestep: 44000, Episode Reward: 530.282293\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 4.26     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 33000    |\n",
      "---------------------------------\n",
      "Timestep: 45000, Episode Reward: 1303.605683\n",
      "Timestep: 46000, Episode Reward: 1246.657592\n",
      "Timestep: 47000, Episode Reward: 788.106852\n",
      "Timestep: 48000, Episode Reward: -450.640301\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 208      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 37000    |\n",
      "---------------------------------\n",
      "Timestep: 49000, Episode Reward: -288.652134\n",
      "Timestep: 50000, Episode Reward: -438.631275\n",
      "Timestep: 51000, Episode Reward: 999.095384\n",
      "Timestep: 52000, Episode Reward: 1495.127023\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 183      |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 204      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -177     |\n",
      "|    critic_loss     | 6.1      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 41000    |\n",
      "---------------------------------\n",
      "Timestep: 53000, Episode Reward: 1614.10209\n",
      "Timestep: 54000, Episode Reward: 137.449777\n",
      "Timestep: 55000, Episode Reward: -323.567125\n",
      "Timestep: 56000, Episode Reward: 1339.530844\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 219      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -192     |\n",
      "|    critic_loss     | 6.36     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 45000    |\n",
      "---------------------------------\n",
      "Timestep: 57000, Episode Reward: 1381.06729\n",
      "Timestep: 58000, Episode Reward: 354.410384\n",
      "Timestep: 59000, Episode Reward: 754.707044\n",
      "Timestep: 60000, Episode Reward: -261.175758\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 241      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 5.25     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 49000    |\n",
      "---------------------------------\n",
      "Timestep: 61000, Episode Reward: 138.720404\n",
      "Timestep: 62000, Episode Reward: 1975.333063\n",
      "Timestep: 63000, Episode Reward: 1326.394862\n",
      "Timestep: 64000, Episode Reward: 375.522935\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 286      |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 328      |\n",
      "|    total_timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -205     |\n",
      "|    critic_loss     | 5.09     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 53000    |\n",
      "---------------------------------\n",
      "Timestep: 65000, Episode Reward: 1965.119539\n",
      "Timestep: 66000, Episode Reward: 204.711372\n",
      "Timestep: 67000, Episode Reward: 1965.149868\n",
      "Timestep: 68000, Episode Reward: 2115.257436\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 361      |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -209     |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 57000    |\n",
      "---------------------------------\n",
      "Timestep: 69000, Episode Reward: -203.610758\n",
      "Timestep: 70000, Episode Reward: 1588.876433\n",
      "Timestep: 71000, Episode Reward: 2197.315879\n",
      "Timestep: 72000, Episode Reward: 1331.72365\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 409      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 72000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -214     |\n",
      "|    critic_loss     | 5.61     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 61000    |\n",
      "---------------------------------\n",
      "Timestep: 73000, Episode Reward: -478.611514\n",
      "Timestep: 74000, Episode Reward: 2284.308222\n",
      "Timestep: 75000, Episode Reward: 2373.393062\n",
      "Timestep: 76000, Episode Reward: 2627.47336\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 477      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 401      |\n",
      "|    total_timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 5.95     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 65000    |\n",
      "---------------------------------\n",
      "Timestep: 77000, Episode Reward: -129.15356\n",
      "Timestep: 78000, Episode Reward: 2155.549861\n",
      "Timestep: 79000, Episode Reward: 2452.541748\n",
      "Timestep: 80000, Episode Reward: -458.762669\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 504      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 6.33     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 69000    |\n",
      "---------------------------------\n",
      "Timestep: 81000, Episode Reward: 1595.145612\n",
      "Timestep: 82000, Episode Reward: 2627.001164\n",
      "Timestep: 83000, Episode Reward: 367.353198\n",
      "Timestep: 84000, Episode Reward: 233.354961\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 537      |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 450      |\n",
      "|    total_timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -225     |\n",
      "|    critic_loss     | 7.14     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 73000    |\n",
      "---------------------------------\n",
      "Timestep: 85000, Episode Reward: 2553.994965\n",
      "Timestep: 86000, Episode Reward: 2907.817769\n",
      "Timestep: 87000, Episode Reward: 2611.770548\n",
      "Timestep: 88000, Episode Reward: 3071.313551\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 639      |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 474      |\n",
      "|    total_timesteps | 88000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 7.81     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 77000    |\n",
      "---------------------------------\n",
      "Timestep: 89000, Episode Reward: 2728.933906\n",
      "Timestep: 90000, Episode Reward: 2435.15153\n",
      "Timestep: 91000, Episode Reward: 2894.20958\n",
      "Timestep: 92000, Episode Reward: 3012.96897\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 499      |\n",
      "|    total_timesteps | 92000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 8.31     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 81000    |\n",
      "---------------------------------\n",
      "Timestep: 93000, Episode Reward: 3375.643156\n",
      "Timestep: 94000, Episode Reward: 2813.444519\n",
      "Timestep: 95000, Episode Reward: 3088.153734\n",
      "Timestep: 96000, Episode Reward: 3302.469038\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 832      |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 523      |\n",
      "|    total_timesteps | 96000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -251     |\n",
      "|    critic_loss     | 8.81     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 85000    |\n",
      "---------------------------------\n",
      "Timestep: 97000, Episode Reward: 2375.345048\n",
      "Timestep: 98000, Episode Reward: 1812.81809\n",
      "Timestep: 99000, Episode Reward: 3403.693665\n",
      "Timestep: 100000, Episode Reward: 2244.997383\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 898      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 548      |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -262     |\n",
      "|    critic_loss     | 9.88     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 89000    |\n",
      "---------------------------------\n",
      "Timestep: 101000, Episode Reward: 3624.702094\n",
      "Timestep: 102000, Episode Reward: 2991.599799\n",
      "Timestep: 103000, Episode Reward: 3441.301693\n",
      "Timestep: 104000, Episode Reward: 2905.758633\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 572      |\n",
      "|    total_timesteps | 104000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 93000    |\n",
      "---------------------------------\n",
      "Timestep: 105000, Episode Reward: 3354.345712\n",
      "Timestep: 106000, Episode Reward: 2110.292267\n",
      "Timestep: 107000, Episode Reward: 1827.797176\n",
      "Timestep: 108000, Episode Reward: 3541.040021\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 597      |\n",
      "|    total_timesteps | 108000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 97000    |\n",
      "---------------------------------\n",
      "Timestep: 109000, Episode Reward: 3453.694026\n",
      "Timestep: 110000, Episode Reward: 3320.45465\n",
      "Timestep: 111000, Episode Reward: 3586.48194\n",
      "Timestep: 112000, Episode Reward: 4018.182535\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 621      |\n",
      "|    total_timesteps | 112000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 101000   |\n",
      "---------------------------------\n",
      "Timestep: 113000, Episode Reward: 3105.265655\n",
      "Timestep: 114000, Episode Reward: 2628.800644\n",
      "Timestep: 115000, Episode Reward: 3583.643631\n",
      "Timestep: 116000, Episode Reward: 3101.115146\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 179      |\n",
      "|    time_elapsed    | 646      |\n",
      "|    total_timesteps | 116000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 13.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 105000   |\n",
      "---------------------------------\n",
      "Timestep: 117000, Episode Reward: 3964.878091\n",
      "Timestep: 118000, Episode Reward: 4069.570898\n",
      "Timestep: 119000, Episode Reward: 1639.350067\n",
      "Timestep: 120000, Episode Reward: 3998.840139\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 670      |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 109000   |\n",
      "---------------------------------\n",
      "Timestep: 121000, Episode Reward: 3753.666101\n",
      "Timestep: 122000, Episode Reward: 3799.049842\n",
      "Timestep: 123000, Episode Reward: 4359.083887\n",
      "Timestep: 124000, Episode Reward: 4272.771097\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 695      |\n",
      "|    total_timesteps | 124000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 15.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 113000   |\n",
      "---------------------------------\n",
      "Timestep: 125000, Episode Reward: 4545.494823\n",
      "Timestep: 126000, Episode Reward: 3971.134967\n",
      "Timestep: 127000, Episode Reward: 4098.013729\n",
      "Timestep: 128000, Episode Reward: 3682.967297\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 720      |\n",
      "|    total_timesteps | 128000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 117000   |\n",
      "---------------------------------\n",
      "Timestep: 129000, Episode Reward: -545.913202\n",
      "Timestep: 130000, Episode Reward: 4462.131932\n",
      "Timestep: 131000, Episode Reward: 4144.513232\n",
      "Timestep: 132000, Episode Reward: 4655.839043\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 744      |\n",
      "|    total_timesteps | 132000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 121000   |\n",
      "---------------------------------\n",
      "Timestep: 133000, Episode Reward: 4508.92793\n",
      "Timestep: 134000, Episode Reward: 4404.378304\n",
      "Timestep: 135000, Episode Reward: 4055.533695\n",
      "Timestep: 136000, Episode Reward: 4747.116799\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 769      |\n",
      "|    total_timesteps | 136000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 125000   |\n",
      "---------------------------------\n",
      "Timestep: 137000, Episode Reward: 5001.517821\n",
      "Timestep: 138000, Episode Reward: 4704.339113\n",
      "Timestep: 139000, Episode Reward: 4640.841349\n",
      "Timestep: 140000, Episode Reward: 5404.080718\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -399     |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 129000   |\n",
      "---------------------------------\n",
      "Timestep: 141000, Episode Reward: 4415.239454\n",
      "Timestep: 142000, Episode Reward: 5064.214062\n",
      "Timestep: 143000, Episode Reward: 5341.070127\n",
      "Timestep: 144000, Episode Reward: 4894.528799\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 818      |\n",
      "|    total_timesteps | 144000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -414     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 133000   |\n",
      "---------------------------------\n",
      "Timestep: 145000, Episode Reward: 4301.450541\n",
      "Timestep: 146000, Episode Reward: 4869.959352\n",
      "Timestep: 147000, Episode Reward: 4824.31707\n",
      "Timestep: 148000, Episode Reward: 4771.490117\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 842      |\n",
      "|    total_timesteps | 148000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 137000   |\n",
      "---------------------------------\n",
      "Timestep: 149000, Episode Reward: 5482.513029\n",
      "Timestep: 150000, Episode Reward: 5002.179027\n",
      "Timestep: 151000, Episode Reward: 3912.234583\n",
      "Timestep: 152000, Episode Reward: 4910.846882\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 866      |\n",
      "|    total_timesteps | 152000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 141000   |\n",
      "---------------------------------\n",
      "Timestep: 153000, Episode Reward: 4919.636128\n",
      "Timestep: 154000, Episode Reward: 4959.415516\n",
      "Timestep: 155000, Episode Reward: 5085.037321\n",
      "Timestep: 156000, Episode Reward: 4889.140545\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 891      |\n",
      "|    total_timesteps | 156000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 145000   |\n",
      "---------------------------------\n",
      "Timestep: 157000, Episode Reward: 5666.004803\n",
      "Timestep: 158000, Episode Reward: 5405.774448\n",
      "Timestep: 159000, Episode Reward: 5355.656565\n",
      "Timestep: 160000, Episode Reward: 5359.262334\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 915      |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -467     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 149000   |\n",
      "---------------------------------\n",
      "Timestep: 161000, Episode Reward: 4541.663026\n",
      "Timestep: 162000, Episode Reward: 5390.548938\n",
      "Timestep: 163000, Episode Reward: 5201.787362\n",
      "Timestep: 164000, Episode Reward: 5603.376605\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 164000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 153000   |\n",
      "---------------------------------\n",
      "Timestep: 165000, Episode Reward: 5484.437904\n",
      "Timestep: 166000, Episode Reward: 5423.751123\n",
      "Timestep: 167000, Episode Reward: 5315.805063\n",
      "Timestep: 168000, Episode Reward: 5489.859185\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 964      |\n",
      "|    total_timesteps | 168000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 157000   |\n",
      "---------------------------------\n",
      "Timestep: 169000, Episode Reward: 5882.65967\n",
      "Timestep: 170000, Episode Reward: 5649.178928\n",
      "Timestep: 171000, Episode Reward: 5506.739667\n",
      "Timestep: 172000, Episode Reward: 4500.070587\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 989      |\n",
      "|    total_timesteps | 172000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 21.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 161000   |\n",
      "---------------------------------\n",
      "Timestep: 173000, Episode Reward: 5475.952092\n",
      "Timestep: 174000, Episode Reward: 5878.308269\n",
      "Timestep: 175000, Episode Reward: 5844.1898\n",
      "Timestep: 176000, Episode Reward: 5589.023727\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 1013     |\n",
      "|    total_timesteps | 176000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 165000   |\n",
      "---------------------------------\n",
      "Timestep: 177000, Episode Reward: 6004.021503\n",
      "Timestep: 178000, Episode Reward: 5676.093001\n",
      "Timestep: 179000, Episode Reward: 6032.591919\n",
      "Timestep: 180000, Episode Reward: 5877.651699\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 1038     |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -525     |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 169000   |\n",
      "---------------------------------\n",
      "Timestep: 181000, Episode Reward: 5926.908092\n",
      "Timestep: 182000, Episode Reward: 5885.523949\n",
      "Timestep: 183000, Episode Reward: 5804.838836\n",
      "Timestep: 184000, Episode Reward: 6254.424333\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 1062     |\n",
      "|    total_timesteps | 184000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -536     |\n",
      "|    critic_loss     | 22       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 173000   |\n",
      "---------------------------------\n",
      "Timestep: 185000, Episode Reward: 5785.752142\n",
      "Timestep: 186000, Episode Reward: 6005.693451\n",
      "Timestep: 187000, Episode Reward: 5790.383363\n",
      "Timestep: 188000, Episode Reward: 6003.564903\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 1087     |\n",
      "|    total_timesteps | 188000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -547     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 177000   |\n",
      "---------------------------------\n",
      "Timestep: 189000, Episode Reward: 5211.239784\n",
      "Timestep: 190000, Episode Reward: 6336.06324\n",
      "Timestep: 191000, Episode Reward: 6376.178708\n",
      "Timestep: 192000, Episode Reward: 5784.869433\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 1111     |\n",
      "|    total_timesteps | 192000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -557     |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 181000   |\n",
      "---------------------------------\n",
      "Timestep: 193000, Episode Reward: 6147.095463\n",
      "Timestep: 194000, Episode Reward: 6808.669349\n",
      "Timestep: 195000, Episode Reward: 6482.446762\n",
      "Timestep: 196000, Episode Reward: 6303.945516\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 1136     |\n",
      "|    total_timesteps | 196000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -567     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 185000   |\n",
      "---------------------------------\n",
      "Timestep: 197000, Episode Reward: 5654.453467\n",
      "Timestep: 198000, Episode Reward: 6319.982529\n",
      "Timestep: 199000, Episode Reward: 6638.11933\n",
      "Timestep: 200000, Episode Reward: 6120.555676\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 1160     |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -578     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 189000   |\n",
      "---------------------------------\n",
      "Timestep: 201000, Episode Reward: 6236.6311\n",
      "Timestep: 202000, Episode Reward: 5549.517557\n",
      "Timestep: 203000, Episode Reward: 6467.506238\n",
      "Timestep: 204000, Episode Reward: 6638.371402\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 1185     |\n",
      "|    total_timesteps | 204000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -589     |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 193000   |\n",
      "---------------------------------\n",
      "Timestep: 205000, Episode Reward: 6337.57379\n",
      "Timestep: 206000, Episode Reward: 6796.445917\n",
      "Timestep: 207000, Episode Reward: 6609.538817\n",
      "Timestep: 208000, Episode Reward: 6402.727634\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 1209     |\n",
      "|    total_timesteps | 208000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -598     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 197000   |\n",
      "---------------------------------\n",
      "Timestep: 209000, Episode Reward: 6257.609072\n",
      "Timestep: 210000, Episode Reward: 6368.090204\n",
      "Timestep: 211000, Episode Reward: 6516.461245\n",
      "Timestep: 212000, Episode Reward: 5816.041036\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 1234     |\n",
      "|    total_timesteps | 212000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -609     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 201000   |\n",
      "---------------------------------\n",
      "Timestep: 213000, Episode Reward: 6252.397999\n",
      "Timestep: 214000, Episode Reward: 6313.353329\n",
      "Timestep: 215000, Episode Reward: 6652.840215\n",
      "Timestep: 216000, Episode Reward: 6572.212794\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 1258     |\n",
      "|    total_timesteps | 216000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -618     |\n",
      "|    critic_loss     | 24.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 205000   |\n",
      "---------------------------------\n",
      "Timestep: 217000, Episode Reward: 5546.033871\n",
      "Timestep: 218000, Episode Reward: 3256.725459\n",
      "Timestep: 219000, Episode Reward: 6059.259759\n",
      "Timestep: 220000, Episode Reward: 7043.63801\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 1283     |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -625     |\n",
      "|    critic_loss     | 25.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 209000   |\n",
      "---------------------------------\n",
      "Timestep: 221000, Episode Reward: 6656.312491\n",
      "Timestep: 222000, Episode Reward: 6445.939053\n",
      "Timestep: 223000, Episode Reward: 6583.241003\n",
      "Timestep: 224000, Episode Reward: 6730.547623\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 1307     |\n",
      "|    total_timesteps | 224000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -632     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 213000   |\n",
      "---------------------------------\n",
      "Timestep: 225000, Episode Reward: 7021.24902\n",
      "Timestep: 226000, Episode Reward: 6759.135702\n",
      "Timestep: 227000, Episode Reward: 4416.46792\n",
      "Timestep: 228000, Episode Reward: 6575.130896\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 1332     |\n",
      "|    total_timesteps | 228000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -638     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 217000   |\n",
      "---------------------------------\n",
      "Timestep: 229000, Episode Reward: 6689.802789\n",
      "Timestep: 230000, Episode Reward: 7051.98879\n",
      "Timestep: 231000, Episode Reward: 6840.713983\n",
      "Timestep: 232000, Episode Reward: 6913.655905\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1357     |\n",
      "|    total_timesteps | 232000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -645     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 221000   |\n",
      "---------------------------------\n",
      "Timestep: 233000, Episode Reward: 6682.471129\n",
      "Timestep: 234000, Episode Reward: 6797.406648\n",
      "Timestep: 235000, Episode Reward: 6884.425406\n",
      "Timestep: 236000, Episode Reward: 7007.393975\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1381     |\n",
      "|    total_timesteps | 236000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -652     |\n",
      "|    critic_loss     | 27       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 225000   |\n",
      "---------------------------------\n",
      "Timestep: 237000, Episode Reward: 7237.399069\n",
      "Timestep: 238000, Episode Reward: 6356.279822\n",
      "Timestep: 239000, Episode Reward: 7125.619645\n",
      "Timestep: 240000, Episode Reward: 6979.646006\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1406     |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -659     |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 229000   |\n",
      "---------------------------------\n",
      "Timestep: 241000, Episode Reward: 7066.973988\n",
      "Timestep: 242000, Episode Reward: 6981.61511\n",
      "Timestep: 243000, Episode Reward: 7384.990262\n",
      "Timestep: 244000, Episode Reward: 7480.957093\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1430     |\n",
      "|    total_timesteps | 244000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -666     |\n",
      "|    critic_loss     | 27.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 233000   |\n",
      "---------------------------------\n",
      "Timestep: 245000, Episode Reward: 7201.328642\n",
      "Timestep: 246000, Episode Reward: 6736.819157\n",
      "Timestep: 247000, Episode Reward: 6822.444128\n",
      "Timestep: 248000, Episode Reward: 6763.340409\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1455     |\n",
      "|    total_timesteps | 248000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -673     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 237000   |\n",
      "---------------------------------\n",
      "Timestep: 249000, Episode Reward: 7576.211439\n",
      "Timestep: 250000, Episode Reward: 7169.890991\n",
      "Timestep: 251000, Episode Reward: 7267.495978\n",
      "Timestep: 252000, Episode Reward: 7532.960374\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1479     |\n",
      "|    total_timesteps | 252000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -681     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 241000   |\n",
      "---------------------------------\n",
      "Timestep: 253000, Episode Reward: 3725.443743\n",
      "Timestep: 254000, Episode Reward: 7453.963354\n",
      "Timestep: 255000, Episode Reward: 7075.661674\n",
      "Timestep: 256000, Episode Reward: 7151.382523\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1504     |\n",
      "|    total_timesteps | 256000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -688     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 245000   |\n",
      "---------------------------------\n",
      "Timestep: 257000, Episode Reward: 7539.084972\n",
      "Timestep: 258000, Episode Reward: 7244.843422\n",
      "Timestep: 259000, Episode Reward: 7621.957254\n",
      "Timestep: 260000, Episode Reward: 6927.540497\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 1528     |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -695     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 249000   |\n",
      "---------------------------------\n",
      "Timestep: 261000, Episode Reward: 8026.591803\n",
      "Timestep: 262000, Episode Reward: 7665.842304\n",
      "Timestep: 263000, Episode Reward: 7625.616602\n",
      "Timestep: 264000, Episode Reward: 7495.416897\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1553     |\n",
      "|    total_timesteps | 264000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -702     |\n",
      "|    critic_loss     | 30.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 253000   |\n",
      "---------------------------------\n",
      "Timestep: 265000, Episode Reward: 7471.098015\n",
      "Timestep: 266000, Episode Reward: 7435.691478\n",
      "Timestep: 267000, Episode Reward: 7189.35987\n",
      "Timestep: 268000, Episode Reward: 7307.70454\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1577     |\n",
      "|    total_timesteps | 268000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -710     |\n",
      "|    critic_loss     | 30.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 257000   |\n",
      "---------------------------------\n",
      "Timestep: 269000, Episode Reward: 7424.758675\n",
      "Timestep: 270000, Episode Reward: 7722.223698\n",
      "Timestep: 271000, Episode Reward: 7618.338378\n",
      "Timestep: 272000, Episode Reward: 7728.048835\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1602     |\n",
      "|    total_timesteps | 272000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -718     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 261000   |\n",
      "---------------------------------\n",
      "Timestep: 273000, Episode Reward: 7688.251038\n",
      "Timestep: 274000, Episode Reward: 7486.911413\n",
      "Timestep: 275000, Episode Reward: 7286.94083\n",
      "Timestep: 276000, Episode Reward: 7520.125502\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1626     |\n",
      "|    total_timesteps | 276000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -726     |\n",
      "|    critic_loss     | 31.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 265000   |\n",
      "---------------------------------\n",
      "Timestep: 277000, Episode Reward: 7241.711741\n",
      "Timestep: 278000, Episode Reward: 7619.353068\n",
      "Timestep: 279000, Episode Reward: 7133.432086\n",
      "Timestep: 280000, Episode Reward: 7525.698577\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1650     |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -733     |\n",
      "|    critic_loss     | 30.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 269000   |\n",
      "---------------------------------\n",
      "Timestep: 281000, Episode Reward: 7813.971076\n",
      "Timestep: 282000, Episode Reward: 7682.899354\n",
      "Timestep: 283000, Episode Reward: 7592.359498\n",
      "Timestep: 284000, Episode Reward: 7260.573817\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1675     |\n",
      "|    total_timesteps | 284000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -739     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 273000   |\n",
      "---------------------------------\n",
      "Timestep: 285000, Episode Reward: 7897.326764\n",
      "Timestep: 286000, Episode Reward: 7619.60165\n",
      "Timestep: 287000, Episode Reward: 8012.978931\n",
      "Timestep: 288000, Episode Reward: 7637.658692\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1699     |\n",
      "|    total_timesteps | 288000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -746     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 277000   |\n",
      "---------------------------------\n",
      "Timestep: 289000, Episode Reward: 7982.478282\n",
      "Timestep: 290000, Episode Reward: 7664.037568\n",
      "Timestep: 291000, Episode Reward: 8382.592891\n",
      "Timestep: 292000, Episode Reward: 7687.560392\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1723     |\n",
      "|    total_timesteps | 292000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -753     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 281000   |\n",
      "---------------------------------\n",
      "Timestep: 293000, Episode Reward: 8372.840466\n",
      "Timestep: 294000, Episode Reward: 7969.562977\n",
      "Timestep: 295000, Episode Reward: 7426.542252\n",
      "Timestep: 296000, Episode Reward: 7955.390342\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1748     |\n",
      "|    total_timesteps | 296000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -759     |\n",
      "|    critic_loss     | 31       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 285000   |\n",
      "---------------------------------\n",
      "Timestep: 297000, Episode Reward: 8097.605346\n",
      "Timestep: 298000, Episode Reward: 8152.301734\n",
      "Timestep: 299000, Episode Reward: 8330.35002\n",
      "Timestep: 300000, Episode Reward: 8359.572658\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1772     |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -767     |\n",
      "|    critic_loss     | 31.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 289000   |\n",
      "---------------------------------\n",
      "Timestep: 301000, Episode Reward: 8243.429167\n",
      "Timestep: 302000, Episode Reward: 8095.165272\n",
      "Timestep: 303000, Episode Reward: 7783.522076\n",
      "Timestep: 304000, Episode Reward: 8306.931607\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1796     |\n",
      "|    total_timesteps | 304000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -774     |\n",
      "|    critic_loss     | 32.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 293000   |\n",
      "---------------------------------\n",
      "Timestep: 305000, Episode Reward: 8083.41843\n",
      "Timestep: 306000, Episode Reward: 7988.251029\n",
      "Timestep: 307000, Episode Reward: 7821.133917\n",
      "Timestep: 308000, Episode Reward: 8329.708226\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1821     |\n",
      "|    total_timesteps | 308000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -782     |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 297000   |\n",
      "---------------------------------\n",
      "Timestep: 309000, Episode Reward: 8175.61345\n",
      "Timestep: 310000, Episode Reward: 7861.641498\n",
      "Timestep: 311000, Episode Reward: 8602.121796\n",
      "Timestep: 312000, Episode Reward: 8249.894154\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 1845     |\n",
      "|    total_timesteps | 312000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -790     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 301000   |\n",
      "---------------------------------\n",
      "Timestep: 313000, Episode Reward: 7890.91414\n",
      "Timestep: 314000, Episode Reward: 8458.821369\n",
      "Timestep: 315000, Episode Reward: 7652.62671\n",
      "Timestep: 316000, Episode Reward: 7747.043724\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 1870     |\n",
      "|    total_timesteps | 316000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -797     |\n",
      "|    critic_loss     | 33       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 305000   |\n",
      "---------------------------------\n",
      "Timestep: 317000, Episode Reward: 688.166713\n",
      "Timestep: 318000, Episode Reward: 8368.555029\n",
      "Timestep: 319000, Episode Reward: 7087.703336\n",
      "Timestep: 320000, Episode Reward: 8273.395373\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 1894     |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -802     |\n",
      "|    critic_loss     | 34.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 309000   |\n",
      "---------------------------------\n",
      "Timestep: 321000, Episode Reward: 8121.797173\n",
      "Timestep: 322000, Episode Reward: 8035.542956\n",
      "Timestep: 323000, Episode Reward: 8372.817527\n",
      "Timestep: 324000, Episode Reward: 8192.065712\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 1918     |\n",
      "|    total_timesteps | 324000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -808     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 313000   |\n",
      "---------------------------------\n",
      "Timestep: 325000, Episode Reward: 8177.440939\n",
      "Timestep: 326000, Episode Reward: 8544.360227\n",
      "Timestep: 327000, Episode Reward: 8245.945008\n",
      "Timestep: 328000, Episode Reward: 4017.530245\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 1943     |\n",
      "|    total_timesteps | 328000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -812     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 317000   |\n",
      "---------------------------------\n",
      "Timestep: 329000, Episode Reward: 8521.54987\n",
      "Timestep: 330000, Episode Reward: 8346.391495\n",
      "Timestep: 331000, Episode Reward: 8035.758141\n",
      "Timestep: 332000, Episode Reward: 8674.207814\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 1967     |\n",
      "|    total_timesteps | 332000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -815     |\n",
      "|    critic_loss     | 36       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 321000   |\n",
      "---------------------------------\n",
      "Timestep: 333000, Episode Reward: 8590.952465\n",
      "Timestep: 334000, Episode Reward: 8421.182995\n",
      "Timestep: 335000, Episode Reward: 8221.874564\n",
      "Timestep: 336000, Episode Reward: 8122.554232\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 1991     |\n",
      "|    total_timesteps | 336000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -820     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 325000   |\n",
      "---------------------------------\n",
      "Timestep: 337000, Episode Reward: 8214.657653\n",
      "Timestep: 338000, Episode Reward: 8537.366619\n",
      "Timestep: 339000, Episode Reward: 8502.680721\n",
      "Timestep: 340000, Episode Reward: 8304.681657\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2016     |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -826     |\n",
      "|    critic_loss     | 36.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 329000   |\n",
      "---------------------------------\n",
      "Timestep: 341000, Episode Reward: 8317.113159\n",
      "Timestep: 342000, Episode Reward: 8187.15172\n",
      "Timestep: 343000, Episode Reward: 8977.233257\n",
      "Timestep: 344000, Episode Reward: 8710.329886\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2040     |\n",
      "|    total_timesteps | 344000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -831     |\n",
      "|    critic_loss     | 36       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 333000   |\n",
      "---------------------------------\n",
      "Timestep: 345000, Episode Reward: 8772.165836\n",
      "Timestep: 346000, Episode Reward: 8551.205004\n",
      "Timestep: 347000, Episode Reward: 8519.37259\n",
      "Timestep: 348000, Episode Reward: 8359.967661\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2064     |\n",
      "|    total_timesteps | 348000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -837     |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 337000   |\n",
      "---------------------------------\n",
      "Timestep: 349000, Episode Reward: 8514.40199\n",
      "Timestep: 350000, Episode Reward: 8563.698963\n",
      "Timestep: 351000, Episode Reward: 8506.628381\n",
      "Timestep: 352000, Episode Reward: 8816.416758\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 352      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2088     |\n",
      "|    total_timesteps | 352000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -842     |\n",
      "|    critic_loss     | 36.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 341000   |\n",
      "---------------------------------\n",
      "Timestep: 353000, Episode Reward: 8525.062988\n",
      "Timestep: 354000, Episode Reward: 8952.61853\n",
      "Timestep: 355000, Episode Reward: 8550.416733\n",
      "Timestep: 356000, Episode Reward: 8757.645279\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 356      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2112     |\n",
      "|    total_timesteps | 356000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -847     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 345000   |\n",
      "---------------------------------\n",
      "Timestep: 357000, Episode Reward: 8947.276687\n",
      "Timestep: 358000, Episode Reward: 8541.114609\n",
      "Timestep: 359000, Episode Reward: 7712.895882\n",
      "Timestep: 360000, Episode Reward: 8660.498258\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2137     |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -852     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 349000   |\n",
      "---------------------------------\n",
      "Timestep: 361000, Episode Reward: 8517.458197\n",
      "Timestep: 362000, Episode Reward: 8502.239972\n",
      "Timestep: 363000, Episode Reward: 8754.836866\n",
      "Timestep: 364000, Episode Reward: 8901.370856\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 364      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2161     |\n",
      "|    total_timesteps | 364000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -856     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 353000   |\n",
      "---------------------------------\n",
      "Timestep: 365000, Episode Reward: 7281.322672\n",
      "Timestep: 366000, Episode Reward: 8600.85532\n",
      "Timestep: 367000, Episode Reward: 8604.298485\n",
      "Timestep: 368000, Episode Reward: 8837.134086\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 368      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2185     |\n",
      "|    total_timesteps | 368000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -861     |\n",
      "|    critic_loss     | 36       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 357000   |\n",
      "---------------------------------\n",
      "Timestep: 369000, Episode Reward: 9042.961534\n",
      "Timestep: 370000, Episode Reward: 8208.993654\n",
      "Timestep: 371000, Episode Reward: 8149.92787\n",
      "Timestep: 372000, Episode Reward: 8557.961518\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 372      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2209     |\n",
      "|    total_timesteps | 372000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -865     |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 361000   |\n",
      "---------------------------------\n",
      "Timestep: 373000, Episode Reward: 8557.421503\n",
      "Timestep: 374000, Episode Reward: 8989.243098\n",
      "Timestep: 375000, Episode Reward: 8550.466212\n",
      "Timestep: 376000, Episode Reward: 8535.040088\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 376      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2233     |\n",
      "|    total_timesteps | 376000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -870     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 365000   |\n",
      "---------------------------------\n",
      "Timestep: 377000, Episode Reward: 8739.961933\n",
      "Timestep: 378000, Episode Reward: 8743.450443\n",
      "Timestep: 379000, Episode Reward: 8543.544746\n",
      "Timestep: 380000, Episode Reward: 9299.397623\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2257     |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -875     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 369000   |\n",
      "---------------------------------\n",
      "Timestep: 381000, Episode Reward: 9072.226224\n",
      "Timestep: 382000, Episode Reward: 8773.627957\n",
      "Timestep: 383000, Episode Reward: 8587.455104\n",
      "Timestep: 384000, Episode Reward: 8796.405186\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2281     |\n",
      "|    total_timesteps | 384000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -878     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 373000   |\n",
      "---------------------------------\n",
      "Timestep: 385000, Episode Reward: 8443.555279\n",
      "Timestep: 386000, Episode Reward: 8749.174166\n",
      "Timestep: 387000, Episode Reward: 8685.215309\n",
      "Timestep: 388000, Episode Reward: 8919.86555\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 388      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2305     |\n",
      "|    total_timesteps | 388000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -882     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 377000   |\n",
      "---------------------------------\n",
      "Timestep: 389000, Episode Reward: 8340.844556\n",
      "Timestep: 390000, Episode Reward: 8710.379123\n",
      "Timestep: 391000, Episode Reward: 8114.759593\n",
      "Timestep: 392000, Episode Reward: 9096.888207\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 392      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2330     |\n",
      "|    total_timesteps | 392000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -886     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 381000   |\n",
      "---------------------------------\n",
      "Timestep: 393000, Episode Reward: 9027.273554\n",
      "Timestep: 394000, Episode Reward: 8851.02655\n",
      "Timestep: 395000, Episode Reward: 8780.18403\n",
      "Timestep: 396000, Episode Reward: 8682.155715\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 396      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2354     |\n",
      "|    total_timesteps | 396000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -890     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 385000   |\n",
      "---------------------------------\n",
      "Timestep: 397000, Episode Reward: 8740.891007\n",
      "Timestep: 398000, Episode Reward: 9159.88186\n",
      "Timestep: 399000, Episode Reward: 9062.661685\n",
      "Timestep: 400000, Episode Reward: 8994.747497\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2378     |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -893     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 389000   |\n",
      "---------------------------------\n",
      "Timestep: 401000, Episode Reward: 9000.589968\n",
      "Timestep: 402000, Episode Reward: 9095.37255\n",
      "Timestep: 403000, Episode Reward: 9063.361683\n",
      "Timestep: 404000, Episode Reward: 9079.192115\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 404      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2402     |\n",
      "|    total_timesteps | 404000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -896     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 393000   |\n",
      "---------------------------------\n",
      "Timestep: 405000, Episode Reward: 8990.252652\n",
      "Timestep: 406000, Episode Reward: 8455.978106\n",
      "Timestep: 407000, Episode Reward: 8730.567449\n",
      "Timestep: 408000, Episode Reward: 8642.027654\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 408      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2426     |\n",
      "|    total_timesteps | 408000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -899     |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 397000   |\n",
      "---------------------------------\n",
      "Timestep: 409000, Episode Reward: 9101.931842\n",
      "Timestep: 410000, Episode Reward: 9297.053182\n",
      "Timestep: 411000, Episode Reward: 8907.929107\n",
      "Timestep: 412000, Episode Reward: 9149.58456\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 412      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2450     |\n",
      "|    total_timesteps | 412000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -902     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 401000   |\n",
      "---------------------------------\n",
      "Timestep: 413000, Episode Reward: 8794.269065\n",
      "Timestep: 414000, Episode Reward: 9092.48238\n",
      "Timestep: 415000, Episode Reward: 8505.279798\n",
      "Timestep: 416000, Episode Reward: 9105.944778\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 416      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2474     |\n",
      "|    total_timesteps | 416000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -906     |\n",
      "|    critic_loss     | 33.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 405000   |\n",
      "---------------------------------\n",
      "Timestep: 417000, Episode Reward: 8658.350429\n",
      "Timestep: 418000, Episode Reward: 9420.068293\n",
      "Timestep: 419000, Episode Reward: 9246.292512\n",
      "Timestep: 420000, Episode Reward: 9207.870092\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2498     |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -909     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 409000   |\n",
      "---------------------------------\n",
      "Timestep: 421000, Episode Reward: 9001.516853\n",
      "Timestep: 422000, Episode Reward: 8732.096605\n",
      "Timestep: 423000, Episode Reward: 9119.575983\n",
      "Timestep: 424000, Episode Reward: 9045.722625\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 424      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2522     |\n",
      "|    total_timesteps | 424000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -912     |\n",
      "|    critic_loss     | 35       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 413000   |\n",
      "---------------------------------\n",
      "Timestep: 425000, Episode Reward: 7067.889644\n",
      "Timestep: 426000, Episode Reward: 9303.754532\n",
      "Timestep: 427000, Episode Reward: 8746.067264\n",
      "Timestep: 428000, Episode Reward: 9351.67589\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 428      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2547     |\n",
      "|    total_timesteps | 428000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -913     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 417000   |\n",
      "---------------------------------\n",
      "Timestep: 429000, Episode Reward: 8904.525655\n",
      "Timestep: 430000, Episode Reward: 8713.232106\n",
      "Timestep: 431000, Episode Reward: 9241.78314\n",
      "Timestep: 432000, Episode Reward: 8974.576269\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 432      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2571     |\n",
      "|    total_timesteps | 432000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -916     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 421000   |\n",
      "---------------------------------\n",
      "Timestep: 433000, Episode Reward: 9013.407226\n",
      "Timestep: 434000, Episode Reward: 9171.032081\n",
      "Timestep: 435000, Episode Reward: 9021.34598\n",
      "Timestep: 436000, Episode Reward: 8935.57557\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 436      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 2595     |\n",
      "|    total_timesteps | 436000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -918     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 425000   |\n",
      "---------------------------------\n",
      "Timestep: 437000, Episode Reward: 8628.962915\n",
      "Timestep: 438000, Episode Reward: 9260.714583\n",
      "Timestep: 439000, Episode Reward: 8570.679042\n",
      "Timestep: 440000, Episode Reward: 9011.144392\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2619     |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -920     |\n",
      "|    critic_loss     | 35.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 429000   |\n",
      "---------------------------------\n",
      "Timestep: 441000, Episode Reward: 9258.798866\n",
      "Timestep: 442000, Episode Reward: 9304.035849\n",
      "Timestep: 443000, Episode Reward: 9498.751508\n",
      "Timestep: 444000, Episode Reward: 9302.934728\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 444      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2643     |\n",
      "|    total_timesteps | 444000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -923     |\n",
      "|    critic_loss     | 35       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 433000   |\n",
      "---------------------------------\n",
      "Timestep: 445000, Episode Reward: 9252.538544\n",
      "Timestep: 446000, Episode Reward: 9452.990827\n",
      "Timestep: 447000, Episode Reward: 9342.761382\n",
      "Timestep: 448000, Episode Reward: 9156.69544\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 448      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2667     |\n",
      "|    total_timesteps | 448000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -927     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 437000   |\n",
      "---------------------------------\n",
      "Timestep: 449000, Episode Reward: 9386.103636\n",
      "Timestep: 450000, Episode Reward: 9434.739188\n",
      "Timestep: 451000, Episode Reward: 9585.439124\n",
      "Timestep: 452000, Episode Reward: 9246.749461\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 452      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2691     |\n",
      "|    total_timesteps | 452000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -931     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 441000   |\n",
      "---------------------------------\n",
      "Timestep: 453000, Episode Reward: 9394.409231\n",
      "Timestep: 454000, Episode Reward: 9027.068358\n",
      "Timestep: 455000, Episode Reward: 9453.753046\n",
      "Timestep: 456000, Episode Reward: 9285.987667\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 456      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2715     |\n",
      "|    total_timesteps | 456000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -934     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 445000   |\n",
      "---------------------------------\n",
      "Timestep: 457000, Episode Reward: 9542.221078\n",
      "Timestep: 458000, Episode Reward: 9076.11408\n",
      "Timestep: 459000, Episode Reward: 9232.941907\n",
      "Timestep: 460000, Episode Reward: 9333.25022\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2739     |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -937     |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 449000   |\n",
      "---------------------------------\n",
      "Timestep: 461000, Episode Reward: 8862.662668\n",
      "Timestep: 462000, Episode Reward: 8804.092522\n",
      "Timestep: 463000, Episode Reward: 9440.043282\n",
      "Timestep: 464000, Episode Reward: 9314.927191\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 464      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2764     |\n",
      "|    total_timesteps | 464000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -939     |\n",
      "|    critic_loss     | 35.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 453000   |\n",
      "---------------------------------\n",
      "Timestep: 465000, Episode Reward: 9182.981938\n",
      "Timestep: 466000, Episode Reward: 9396.106833\n",
      "Timestep: 467000, Episode Reward: 9579.355602\n",
      "Timestep: 468000, Episode Reward: 9511.132038\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 468      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2788     |\n",
      "|    total_timesteps | 468000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -942     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 457000   |\n",
      "---------------------------------\n",
      "Timestep: 469000, Episode Reward: 9369.064052\n",
      "Timestep: 470000, Episode Reward: 9454.735986\n",
      "Timestep: 471000, Episode Reward: 9715.355039\n",
      "Timestep: 472000, Episode Reward: 9481.782185\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 472      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2812     |\n",
      "|    total_timesteps | 472000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -944     |\n",
      "|    critic_loss     | 34       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 461000   |\n",
      "---------------------------------\n",
      "Timestep: 473000, Episode Reward: 9267.772593\n",
      "Timestep: 474000, Episode Reward: 9433.819751\n",
      "Timestep: 475000, Episode Reward: 9665.313124\n",
      "Timestep: 476000, Episode Reward: 9729.251927\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 476      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2836     |\n",
      "|    total_timesteps | 476000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -947     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 465000   |\n",
      "---------------------------------\n",
      "Timestep: 477000, Episode Reward: 9919.108364\n",
      "Timestep: 478000, Episode Reward: 9466.018817\n",
      "Timestep: 479000, Episode Reward: 9485.341678\n",
      "Timestep: 480000, Episode Reward: 9408.019664\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2860     |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -950     |\n",
      "|    critic_loss     | 33.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 469000   |\n",
      "---------------------------------\n",
      "Timestep: 481000, Episode Reward: 9265.550935\n",
      "Timestep: 482000, Episode Reward: 9555.456609\n",
      "Timestep: 483000, Episode Reward: 9573.439872\n",
      "Timestep: 484000, Episode Reward: 9185.175325\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 484      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2884     |\n",
      "|    total_timesteps | 484000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -953     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 473000   |\n",
      "---------------------------------\n",
      "Timestep: 485000, Episode Reward: 9088.648118\n",
      "Timestep: 486000, Episode Reward: 9421.997956\n",
      "Timestep: 487000, Episode Reward: 9390.383786\n",
      "Timestep: 488000, Episode Reward: 9725.880957\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 488      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2908     |\n",
      "|    total_timesteps | 488000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -957     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 477000   |\n",
      "---------------------------------\n",
      "Timestep: 489000, Episode Reward: 9699.099886\n",
      "Timestep: 490000, Episode Reward: 9666.689573\n",
      "Timestep: 491000, Episode Reward: 9589.583718\n",
      "Timestep: 492000, Episode Reward: 9257.146226\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 492      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2933     |\n",
      "|    total_timesteps | 492000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -960     |\n",
      "|    critic_loss     | 35.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 481000   |\n",
      "---------------------------------\n",
      "Timestep: 493000, Episode Reward: 9205.192728\n",
      "Timestep: 494000, Episode Reward: 9034.298616\n",
      "Timestep: 495000, Episode Reward: 9642.046523\n",
      "Timestep: 496000, Episode Reward: 9523.961103\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 496      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2957     |\n",
      "|    total_timesteps | 496000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -963     |\n",
      "|    critic_loss     | 34.1     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 485000   |\n",
      "---------------------------------\n",
      "Timestep: 497000, Episode Reward: 9634.00739\n",
      "Timestep: 498000, Episode Reward: 9315.832635\n",
      "Timestep: 499000, Episode Reward: 9553.810392\n",
      "Timestep: 500000, Episode Reward: 9479.158849\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 2981     |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -965     |\n",
      "|    critic_loss     | 34       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 489000   |\n",
      "---------------------------------\n",
      "Timestep: 501000, Episode Reward: 9844.310205\n",
      "Timestep: 502000, Episode Reward: 9148.302592\n",
      "Timestep: 503000, Episode Reward: 9447.851263\n",
      "Timestep: 504000, Episode Reward: 9658.709096\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 504      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3005     |\n",
      "|    total_timesteps | 504000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -968     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 493000   |\n",
      "---------------------------------\n",
      "Timestep: 505000, Episode Reward: 9516.103959\n",
      "Timestep: 506000, Episode Reward: 9500.008604\n",
      "Timestep: 507000, Episode Reward: 9866.118623\n",
      "Timestep: 508000, Episode Reward: 9567.219049\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 508      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3029     |\n",
      "|    total_timesteps | 508000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -971     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 497000   |\n",
      "---------------------------------\n",
      "Timestep: 509000, Episode Reward: 9714.74789\n",
      "Timestep: 510000, Episode Reward: 9521.81507\n",
      "Timestep: 511000, Episode Reward: 9510.990332\n",
      "Timestep: 512000, Episode Reward: 9395.383543\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 512      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3053     |\n",
      "|    total_timesteps | 512000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -974     |\n",
      "|    critic_loss     | 35       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 501000   |\n",
      "---------------------------------\n",
      "Timestep: 513000, Episode Reward: 9115.852998\n",
      "Timestep: 514000, Episode Reward: 9989.127561\n",
      "Timestep: 515000, Episode Reward: 9551.159856\n",
      "Timestep: 516000, Episode Reward: 9816.766645\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 516      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3077     |\n",
      "|    total_timesteps | 516000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -976     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 505000   |\n",
      "---------------------------------\n",
      "Timestep: 517000, Episode Reward: 9492.652765\n",
      "Timestep: 518000, Episode Reward: 9775.236435\n",
      "Timestep: 519000, Episode Reward: 10005.185407\n",
      "Timestep: 520000, Episode Reward: 9957.183041\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3102     |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -977     |\n",
      "|    critic_loss     | 34       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 509000   |\n",
      "---------------------------------\n",
      "Timestep: 521000, Episode Reward: 8674.609578\n",
      "Timestep: 522000, Episode Reward: 9719.401775\n",
      "Timestep: 523000, Episode Reward: 9559.268383\n",
      "Timestep: 524000, Episode Reward: 9627.517793\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 524      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3126     |\n",
      "|    total_timesteps | 524000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -979     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 513000   |\n",
      "---------------------------------\n",
      "Timestep: 525000, Episode Reward: 9617.808652\n",
      "Timestep: 526000, Episode Reward: 9617.650416\n",
      "Timestep: 527000, Episode Reward: 9822.220379\n",
      "Timestep: 528000, Episode Reward: 9173.360632\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 528      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3150     |\n",
      "|    total_timesteps | 528000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -982     |\n",
      "|    critic_loss     | 35       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 517000   |\n",
      "---------------------------------\n",
      "Timestep: 529000, Episode Reward: 9372.430249\n",
      "Timestep: 530000, Episode Reward: 9694.512339\n",
      "Timestep: 531000, Episode Reward: 9466.969589\n",
      "Timestep: 532000, Episode Reward: 9849.40453\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 532      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3174     |\n",
      "|    total_timesteps | 532000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -985     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 521000   |\n",
      "---------------------------------\n",
      "Timestep: 533000, Episode Reward: 9477.595058\n",
      "Timestep: 534000, Episode Reward: 9989.565997\n",
      "Timestep: 535000, Episode Reward: 9748.503013\n",
      "Timestep: 536000, Episode Reward: 9466.817697\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 536      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3198     |\n",
      "|    total_timesteps | 536000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -987     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 525000   |\n",
      "---------------------------------\n",
      "Timestep: 537000, Episode Reward: 9635.158116\n",
      "Timestep: 538000, Episode Reward: 9838.208341\n",
      "Timestep: 539000, Episode Reward: 9601.915614\n",
      "Timestep: 540000, Episode Reward: 9542.236573\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3222     |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -990     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 529000   |\n",
      "---------------------------------\n",
      "Timestep: 541000, Episode Reward: 9685.630521\n",
      "Timestep: 542000, Episode Reward: 9477.043326\n",
      "Timestep: 543000, Episode Reward: 9232.575883\n",
      "Timestep: 544000, Episode Reward: 9359.304201\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 544      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3246     |\n",
      "|    total_timesteps | 544000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -994     |\n",
      "|    critic_loss     | 35.4     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 533000   |\n",
      "---------------------------------\n",
      "Timestep: 545000, Episode Reward: 9748.611341\n",
      "Timestep: 546000, Episode Reward: 9084.51572\n",
      "Timestep: 547000, Episode Reward: 10217.445655\n",
      "Timestep: 548000, Episode Reward: 9856.632503\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 548      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3270     |\n",
      "|    total_timesteps | 548000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -996     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 537000   |\n",
      "---------------------------------\n",
      "Timestep: 549000, Episode Reward: 7849.77427\n",
      "Timestep: 550000, Episode Reward: 9648.671857\n",
      "Timestep: 551000, Episode Reward: 9415.236246\n",
      "Timestep: 552000, Episode Reward: 9318.863869\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 552      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3295     |\n",
      "|    total_timesteps | 552000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -997     |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 541000   |\n",
      "---------------------------------\n",
      "Timestep: 553000, Episode Reward: 9199.682753\n",
      "Timestep: 554000, Episode Reward: 9818.752792\n",
      "Timestep: 555000, Episode Reward: 9769.441496\n",
      "Timestep: 556000, Episode Reward: 9810.54594\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 556      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3319     |\n",
      "|    total_timesteps | 556000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -999     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 545000   |\n",
      "---------------------------------\n",
      "Timestep: 557000, Episode Reward: 9840.802797\n",
      "Timestep: 558000, Episode Reward: 9651.7221\n",
      "Timestep: 559000, Episode Reward: 10000.623898\n",
      "Timestep: 560000, Episode Reward: 9730.354133\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3343     |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1e+03   |\n",
      "|    critic_loss     | 35.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 549000   |\n",
      "---------------------------------\n",
      "Timestep: 561000, Episode Reward: 9258.123668\n",
      "Timestep: 562000, Episode Reward: 9762.283477\n",
      "Timestep: 563000, Episode Reward: 9885.777917\n",
      "Timestep: 564000, Episode Reward: 9837.850322\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 564      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 3367     |\n",
      "|    total_timesteps | 564000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1e+03   |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 553000   |\n",
      "---------------------------------\n",
      "Timestep: 565000, Episode Reward: 9892.658219\n",
      "Timestep: 566000, Episode Reward: 10022.414933\n",
      "Timestep: 567000, Episode Reward: 9880.600345\n",
      "Timestep: 568000, Episode Reward: 9700.280764\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.56e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 568       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3392      |\n",
      "|    total_timesteps | 568000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 35.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 557000    |\n",
      "----------------------------------\n",
      "Timestep: 569000, Episode Reward: 9925.849037\n",
      "Timestep: 570000, Episode Reward: 9297.558799\n",
      "Timestep: 571000, Episode Reward: 9981.739654\n",
      "Timestep: 572000, Episode Reward: 9877.190118\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.57e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 572       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3416      |\n",
      "|    total_timesteps | 572000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 35.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 561000    |\n",
      "----------------------------------\n",
      "Timestep: 573000, Episode Reward: 9452.820377\n",
      "Timestep: 574000, Episode Reward: 9811.209348\n",
      "Timestep: 575000, Episode Reward: 9764.173709\n",
      "Timestep: 576000, Episode Reward: 9797.713354\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.58e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 576       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3440      |\n",
      "|    total_timesteps | 576000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 35.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 565000    |\n",
      "----------------------------------\n",
      "Timestep: 577000, Episode Reward: 9870.117924\n",
      "Timestep: 578000, Episode Reward: 9396.915818\n",
      "Timestep: 579000, Episode Reward: 9930.443864\n",
      "Timestep: 580000, Episode Reward: 9857.100876\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 580       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3464      |\n",
      "|    total_timesteps | 580000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 34.8      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 569000    |\n",
      "----------------------------------\n",
      "Timestep: 581000, Episode Reward: 9743.186646\n",
      "Timestep: 582000, Episode Reward: 9768.25039\n",
      "Timestep: 583000, Episode Reward: 9768.318647\n",
      "Timestep: 584000, Episode Reward: 10080.783072\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 584       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3488      |\n",
      "|    total_timesteps | 584000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 34.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 573000    |\n",
      "----------------------------------\n",
      "Timestep: 585000, Episode Reward: 9955.421034\n",
      "Timestep: 586000, Episode Reward: 8979.361641\n",
      "Timestep: 587000, Episode Reward: 9796.81305\n",
      "Timestep: 588000, Episode Reward: 1207.21082\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.53e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 588       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3513      |\n",
      "|    total_timesteps | 588000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 34.6      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 577000    |\n",
      "----------------------------------\n",
      "Timestep: 589000, Episode Reward: 9618.835321\n",
      "Timestep: 590000, Episode Reward: 9948.723217\n",
      "Timestep: 591000, Episode Reward: 9416.062869\n",
      "Timestep: 592000, Episode Reward: 9445.587762\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.53e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 592       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3537      |\n",
      "|    total_timesteps | 592000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 35.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 581000    |\n",
      "----------------------------------\n",
      "Timestep: 593000, Episode Reward: 10102.147863\n",
      "Timestep: 594000, Episode Reward: 9661.857504\n",
      "Timestep: 595000, Episode Reward: 9877.844297\n",
      "Timestep: 596000, Episode Reward: 9665.729653\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.55e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 596       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3561      |\n",
      "|    total_timesteps | 596000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 36.5      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 585000    |\n",
      "----------------------------------\n",
      "Timestep: 597000, Episode Reward: 10025.642802\n",
      "Timestep: 598000, Episode Reward: 9686.115274\n",
      "Timestep: 599000, Episode Reward: 9914.83955\n",
      "Timestep: 600000, Episode Reward: 9879.018392\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.56e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 600       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3585      |\n",
      "|    total_timesteps | 600000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 36.3      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 589000    |\n",
      "----------------------------------\n",
      "Timestep: 601000, Episode Reward: 9372.794159\n",
      "Timestep: 602000, Episode Reward: 10059.026374\n",
      "Timestep: 603000, Episode Reward: 10362.123273\n",
      "Timestep: 604000, Episode Reward: 10029.120445\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.58e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 604       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3609      |\n",
      "|    total_timesteps | 604000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 36.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 593000    |\n",
      "----------------------------------\n",
      "Timestep: 605000, Episode Reward: 9850.434429\n",
      "Timestep: 606000, Episode Reward: 9822.788609\n",
      "Timestep: 607000, Episode Reward: 9557.139773\n",
      "Timestep: 608000, Episode Reward: 10260.356894\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 608       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3633      |\n",
      "|    total_timesteps | 608000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 35.5      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 597000    |\n",
      "----------------------------------\n",
      "Timestep: 609000, Episode Reward: 9918.305977\n",
      "Timestep: 610000, Episode Reward: 10057.359537\n",
      "Timestep: 611000, Episode Reward: 10217.533024\n",
      "Timestep: 612000, Episode Reward: 9571.156458\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.61e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 612       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3658      |\n",
      "|    total_timesteps | 612000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 36.9      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 601000    |\n",
      "----------------------------------\n",
      "Timestep: 613000, Episode Reward: 9768.241348\n",
      "Timestep: 614000, Episode Reward: 10194.738987\n",
      "Timestep: 615000, Episode Reward: 9787.816916\n",
      "Timestep: 616000, Episode Reward: 10089.735359\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.62e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 616       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3682      |\n",
      "|    total_timesteps | 616000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 34.8      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 605000    |\n",
      "----------------------------------\n",
      "Timestep: 617000, Episode Reward: 9997.200203\n",
      "Timestep: 618000, Episode Reward: 10139.288318\n",
      "Timestep: 619000, Episode Reward: 9730.291182\n",
      "Timestep: 620000, Episode Reward: 9980.123653\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.63e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 620       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3706      |\n",
      "|    total_timesteps | 620000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 36        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 609000    |\n",
      "----------------------------------\n",
      "Timestep: 621000, Episode Reward: 10057.12352\n",
      "Timestep: 622000, Episode Reward: 10375.448988\n",
      "Timestep: 623000, Episode Reward: 9553.498778\n",
      "Timestep: 624000, Episode Reward: 9709.37802\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.65e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 624       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3730      |\n",
      "|    total_timesteps | 624000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 35.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 613000    |\n",
      "----------------------------------\n",
      "Timestep: 625000, Episode Reward: 7424.218683\n",
      "Timestep: 626000, Episode Reward: 9676.083771\n",
      "Timestep: 627000, Episode Reward: 9818.110618\n",
      "Timestep: 628000, Episode Reward: 9953.473091\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.63e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 628       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3754      |\n",
      "|    total_timesteps | 628000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 36        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 617000    |\n",
      "----------------------------------\n",
      "Timestep: 629000, Episode Reward: 10089.370077\n",
      "Timestep: 630000, Episode Reward: 10372.952886\n",
      "Timestep: 631000, Episode Reward: 10204.33018\n",
      "Timestep: 632000, Episode Reward: 9756.881344\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.66e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 632       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3779      |\n",
      "|    total_timesteps | 632000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 35        |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 621000    |\n",
      "----------------------------------\n",
      "Timestep: 633000, Episode Reward: 10032.369104\n",
      "Timestep: 634000, Episode Reward: 8762.778595\n",
      "Timestep: 635000, Episode Reward: 10153.514087\n",
      "Timestep: 636000, Episode Reward: 10182.166586\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 9.66e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 636       |\n",
      "|    fps             | 167       |\n",
      "|    time_elapsed    | 3803      |\n",
      "|    total_timesteps | 636000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 35.1      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 625000    |\n",
      "----------------------------------\n",
      "Timestep: 637000, Episode Reward: 10238.795675\n",
      "Timestep: 638000, Episode Reward: 10213.774847\n",
      "Timestep: 639000, Episode Reward: 10322.565609\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Custom callback to log rewards\n",
    "class RewardLogger(BaseCallback):\n",
    "    def __init__(self, log_file_path, verbose=0):\n",
    "        super(RewardLogger, self).__init__(verbose)\n",
    "        # Create log directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "        self.log_file = open(log_file_path, \"w\")\n",
    "        self.log_file.write(\"timestep,reward\\n\")  # CSV header\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.total_timesteps = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        if self.locals.get(\"dones\"):\n",
    "          \n",
    "            if self.locals.get(\"dones\")[0]:\n",
    "                # Get episode info\n",
    "                episode_info = self.locals.get(\"infos\")[0].get(\"episode\")\n",
    "                if episode_info:\n",
    "                    self.episode_rewards.append(episode_info[\"r\"])\n",
    "                    self.episode_lengths.append(episode_info[\"l\"])\n",
    "                    self.total_timesteps += episode_info[\"l\"]\n",
    "                    \n",
    "                  \n",
    "                    self.log_file.write(f\"{self.total_timesteps},{episode_info['r']}\\n\")\n",
    "                    self.log_file.flush()  \n",
    "                    \n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Timestep: {self.total_timesteps}, Episode Reward: {episode_info['r']}\")\n",
    "        return True\n",
    "    \n",
    "    def close(self):\n",
    "        if self.log_file is not None:\n",
    "            self.log_file.close()\n",
    "\n",
    "\n",
    "env = gym.make(\"HalfCheetah-v5\")\n",
    "\n",
    "env = Monitor(env)\n",
    "\n",
    "\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "model = DDPG(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    action_noise=action_noise,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=700000,\n",
    "    learning_starts=10000,\n",
    "    batch_size=256,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    "    train_freq=(1, \"episode\"),\n",
    "    gradient_steps=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "os.makedirs(\"./DDPGlogs/\", exist_ok=True)\n",
    "\n",
    "# Initialize our custom callback to log rewards\n",
    "reward_logger = RewardLogger(log_file_path=\"./logs/ddpg_rewards.csv\", verbose=1)\n",
    "\n",
    "# Create a callback that saves the model every 100,000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=100000, \n",
    "    save_path='./DDPGlogs/',\n",
    "    name_prefix='ddpg_halfcheetah'\n",
    ")\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks = [checkpoint_callback, reward_logger]\n",
    "\n",
    "# Train the model with the callbacks\n",
    "model.learn(total_timesteps=700000, callback=callbacks)\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"ddpg_halfcheetah_final\")\n",
    "\n",
    "# Make sure to close the logger\n",
    "reward_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63b485d-996e-479e-87cb-dbac89467275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading Gymnasium-0.26.3-py3-none-any.whl (836 kB)\n",
      "     |████████████████████████████████| 836 kB 5.3 MB/s            \n",
      "\u001b[?25hCollecting gymnasium-notices>=0.0.1\n",
      "  Downloading gymnasium_notices-0.0.1-py3-none-any.whl (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/phalle.y/.venv/lib/python3.6/site-packages (from gymnasium) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/phalle.y/.venv/lib/python3.6/site-packages (from gymnasium) (1.6.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/phalle.y/.venv/lib/python3.6/site-packages (from gymnasium) (4.8.3)\n",
      "Requirement already satisfied: dataclasses==0.8 in /home/phalle.y/.venv/lib/python3.6/site-packages (from gymnasium) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/phalle.y/.venv/lib/python3.6/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/phalle.y/.venv/lib/python3.6/site-packages (from importlib-metadata>=4.8.0->gymnasium) (4.1.1)\n",
      "Installing collected packages: gymnasium-notices, gymnasium\n",
      "Successfully installed gymnasium-0.26.3 gymnasium-notices-0.0.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f11a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
