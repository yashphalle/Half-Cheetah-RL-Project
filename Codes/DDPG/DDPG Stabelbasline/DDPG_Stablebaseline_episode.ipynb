{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# stable baseline plotting rewards per episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M4sUmw2NU_cm",
        "outputId": "89ba6360-61c3-49a4-fc51-76d042875a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Episode 1: Reward = -157.582862\n",
            "Episode 2: Reward = -182.907936\n",
            "Episode 3: Reward = -275.733653\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -192     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 2760     |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "Episode 4: Reward = -149.852464\n",
            "Episode 5: Reward = -195.678011\n",
            "Episode 6: Reward = -415.289285\n",
            "Episode 7: Reward = -274.705571\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -246     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 3058     |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "Episode 8: Reward = -317.924181\n",
            "Episode 9: Reward = -346.319564\n",
            "Episode 10: Reward = -342.412125\n",
            "Episode 11: Reward = -15.537176\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -269     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 1000     |\n",
            "|    time_elapsed    | 11       |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1.28    |\n",
            "|    critic_loss     | 0.224    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1000     |\n",
            "---------------------------------\n",
            "Episode 12: Reward = -550.011647\n",
            "Episode 13: Reward = -538.250515\n",
            "Episode 14: Reward = -486.935254\n",
            "Episode 15: Reward = -27.349861\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -302     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 379      |\n",
            "|    time_elapsed    | 42       |\n",
            "|    total_timesteps | 16000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16.3    |\n",
            "|    critic_loss     | 0.88     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5000     |\n",
            "---------------------------------\n",
            "Episode 16: Reward = -563.332355\n",
            "Episode 17: Reward = -681.125412\n",
            "Episode 18: Reward = 125.15611\n",
            "Episode 19: Reward = 612.896673\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -186     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 287      |\n",
            "|    time_elapsed    | 69       |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -37.7    |\n",
            "|    critic_loss     | 1.4      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9000     |\n",
            "---------------------------------\n",
            "Episode 20: Reward = 1053.203991\n",
            "Episode 21: Reward = 1097.869\n",
            "Episode 22: Reward = 465.264544\n",
            "Episode 23: Reward = 299.154642\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | -33.6    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 246      |\n",
            "|    time_elapsed    | 97       |\n",
            "|    total_timesteps | 24000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -63.3    |\n",
            "|    critic_loss     | 1.79     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 13000    |\n",
            "---------------------------------\n",
            "Episode 24: Reward = 1060.410272\n",
            "Episode 25: Reward = 1124.060658\n",
            "Episode 26: Reward = 1244.06491\n",
            "Episode 27: Reward = 1299.645304\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 128      |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 224      |\n",
            "|    time_elapsed    | 124      |\n",
            "|    total_timesteps | 28000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -89.6    |\n",
            "|    critic_loss     | 2.3      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17000    |\n",
            "---------------------------------\n",
            "Episode 28: Reward = 720.452309\n",
            "Episode 29: Reward = 1359.489649\n",
            "Episode 30: Reward = 1429.104832\n",
            "Episode 31: Reward = 1488.983795\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 210      |\n",
            "|    time_elapsed    | 151      |\n",
            "|    total_timesteps | 32000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -115     |\n",
            "|    critic_loss     | 2.88     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 21000    |\n",
            "---------------------------------\n",
            "Episode 32: Reward = 1013.861083\n",
            "Episode 33: Reward = 73.396748\n",
            "Episode 34: Reward = 1535.027254\n",
            "Episode 35: Reward = 1701.24343\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 384      |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 200      |\n",
            "|    time_elapsed    | 179      |\n",
            "|    total_timesteps | 36000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -136     |\n",
            "|    critic_loss     | 3.58     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 25000    |\n",
            "---------------------------------\n",
            "Episode 36: Reward = 1659.438821\n",
            "Episode 37: Reward = 1649.115274\n",
            "Episode 38: Reward = 1467.241195\n",
            "Episode 39: Reward = 1587.887438\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 507      |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 193      |\n",
            "|    time_elapsed    | 206      |\n",
            "|    total_timesteps | 40000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -160     |\n",
            "|    critic_loss     | 4.1      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 29000    |\n",
            "---------------------------------\n",
            "Episode 40: Reward = 1746.954429\n",
            "Episode 41: Reward = 1624.889761\n",
            "Episode 42: Reward = 2183.448099\n",
            "Episode 43: Reward = 2163.458385\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 645      |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 188      |\n",
            "|    time_elapsed    | 233      |\n",
            "|    total_timesteps | 44000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 4.53     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 33000    |\n",
            "---------------------------------\n",
            "Episode 44: Reward = 2107.798263\n",
            "Episode 45: Reward = 2283.392485\n",
            "Episode 46: Reward = 2024.741188\n",
            "Episode 47: Reward = 1952.477626\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 762      |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 183      |\n",
            "|    time_elapsed    | 260      |\n",
            "|    total_timesteps | 48000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -202     |\n",
            "|    critic_loss     | 5.06     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 37000    |\n",
            "---------------------------------\n",
            "Episode 48: Reward = 1952.117591\n",
            "Episode 49: Reward = 2562.048308\n",
            "Episode 50: Reward = 2757.809299\n",
            "Episode 51: Reward = 2055.737931\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 892      |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 180      |\n",
            "|    time_elapsed    | 288      |\n",
            "|    total_timesteps | 52000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -222     |\n",
            "|    critic_loss     | 5.52     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 41000    |\n",
            "---------------------------------\n",
            "Episode 52: Reward = 2421.524846\n",
            "Episode 53: Reward = 2265.911352\n",
            "Episode 54: Reward = 2439.139401\n",
            "Episode 55: Reward = 2762.743774\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 999      |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 177      |\n",
            "|    time_elapsed    | 315      |\n",
            "|    total_timesteps | 56000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -241     |\n",
            "|    critic_loss     | 6.04     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 45000    |\n",
            "---------------------------------\n",
            "Episode 56: Reward = 2069.821895\n",
            "Episode 57: Reward = 2535.217211\n",
            "Episode 58: Reward = 2654.461124\n",
            "Episode 59: Reward = 2859.620333\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.11e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 175      |\n",
            "|    time_elapsed    | 341      |\n",
            "|    total_timesteps | 60000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -258     |\n",
            "|    critic_loss     | 6.57     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 49000    |\n",
            "---------------------------------\n",
            "Episode 60: Reward = 2559.46328\n",
            "Episode 61: Reward = 2711.432705\n",
            "Episode 62: Reward = 2701.983755\n",
            "Episode 63: Reward = 2860.590218\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.22e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 173      |\n",
            "|    time_elapsed    | 369      |\n",
            "|    total_timesteps | 64000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -274     |\n",
            "|    critic_loss     | 7.15     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 53000    |\n",
            "---------------------------------\n",
            "Episode 64: Reward = 3132.268886\n",
            "Episode 65: Reward = 2929.240491\n",
            "Episode 66: Reward = 3002.956015\n",
            "Episode 67: Reward = 3177.611761\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.33e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 171      |\n",
            "|    time_elapsed    | 396      |\n",
            "|    total_timesteps | 68000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -291     |\n",
            "|    critic_loss     | 7.57     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 57000    |\n",
            "---------------------------------\n",
            "Episode 68: Reward = 3197.139329\n",
            "Episode 69: Reward = 2952.197576\n",
            "Episode 70: Reward = -489.785157\n",
            "Episode 71: Reward = 3188.21201\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.38e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 169      |\n",
            "|    time_elapsed    | 423      |\n",
            "|    total_timesteps | 72000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -305     |\n",
            "|    critic_loss     | 7.95     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 61000    |\n",
            "---------------------------------\n",
            "Episode 72: Reward = 3187.597702\n",
            "Episode 73: Reward = 3653.495305\n",
            "Episode 74: Reward = 3150.816658\n",
            "Episode 75: Reward = 2467.440452\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.46e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 168      |\n",
            "|    time_elapsed    | 450      |\n",
            "|    total_timesteps | 76000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -319     |\n",
            "|    critic_loss     | 8.77     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 65000    |\n",
            "---------------------------------\n",
            "Episode 76: Reward = 2405.715816\n",
            "Episode 77: Reward = 2289.506965\n",
            "Episode 78: Reward = 3283.417965\n",
            "Episode 79: Reward = 3593.571995\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.54e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 167      |\n",
            "|    time_elapsed    | 477      |\n",
            "|    total_timesteps | 80000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -332     |\n",
            "|    critic_loss     | 9.57     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 69000    |\n",
            "---------------------------------\n",
            "Episode 80: Reward = 3542.383092\n",
            "Episode 81: Reward = 3737.129264\n",
            "Episode 82: Reward = 3925.416071\n",
            "Episode 83: Reward = 3806.559276\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.65e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 166      |\n",
            "|    time_elapsed    | 504      |\n",
            "|    total_timesteps | 84000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -347     |\n",
            "|    critic_loss     | 9.64     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 73000    |\n",
            "---------------------------------\n",
            "Episode 84: Reward = 3647.560641\n",
            "Episode 85: Reward = 3947.871398\n",
            "Episode 86: Reward = 4091.421006\n",
            "Episode 87: Reward = 3829.714664\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.75e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 165      |\n",
            "|    time_elapsed    | 531      |\n",
            "|    total_timesteps | 88000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -362     |\n",
            "|    critic_loss     | 9.65     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 77000    |\n",
            "---------------------------------\n",
            "Episode 88: Reward = 3703.549018\n",
            "Episode 89: Reward = 3562.314036\n",
            "Episode 90: Reward = 3900.952087\n",
            "Episode 91: Reward = 4361.164588\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.85e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 164      |\n",
            "|    time_elapsed    | 558      |\n",
            "|    total_timesteps | 92000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -377     |\n",
            "|    critic_loss     | 10.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 81000    |\n",
            "---------------------------------\n",
            "Episode 92: Reward = 4185.299183\n",
            "Episode 93: Reward = 4347.807968\n",
            "Episode 94: Reward = 4218.331098\n",
            "Episode 95: Reward = 4649.732633\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 1.95e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 163      |\n",
            "|    time_elapsed    | 585      |\n",
            "|    total_timesteps | 96000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -392     |\n",
            "|    critic_loss     | 10.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 85000    |\n",
            "---------------------------------\n",
            "Episode 96: Reward = 4168.12086\n",
            "Episode 97: Reward = 4160.162614\n",
            "Episode 98: Reward = 4357.836397\n",
            "Episode 99: Reward = 903.288948\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.01e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 163      |\n",
            "|    time_elapsed    | 612      |\n",
            "|    total_timesteps | 100000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -405     |\n",
            "|    critic_loss     | 11.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 89000    |\n",
            "---------------------------------\n",
            "Episode 100: Reward = 4345.873608\n",
            "Episode 101: Reward = 4508.514908\n",
            "Episode 102: Reward = 1774.628296\n",
            "Episode 103: Reward = 4107.111316\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.17e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 162      |\n",
            "|    time_elapsed    | 639      |\n",
            "|    total_timesteps | 104000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -418     |\n",
            "|    critic_loss     | 12.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 93000    |\n",
            "---------------------------------\n",
            "Episode 104: Reward = 4438.136294\n",
            "Episode 105: Reward = 4588.512981\n",
            "Episode 106: Reward = 4692.086532\n",
            "Episode 107: Reward = 4578.413563\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.37e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 162      |\n",
            "|    time_elapsed    | 666      |\n",
            "|    total_timesteps | 108000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -430     |\n",
            "|    critic_loss     | 13.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 97000    |\n",
            "---------------------------------\n",
            "Episode 108: Reward = 4694.804957\n",
            "Episode 109: Reward = 4826.99409\n",
            "Episode 110: Reward = 4680.517145\n",
            "Episode 111: Reward = 4084.22858\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.56e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 161      |\n",
            "|    time_elapsed    | 693      |\n",
            "|    total_timesteps | 112000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -444     |\n",
            "|    critic_loss     | 13.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 101000   |\n",
            "---------------------------------\n",
            "Episode 112: Reward = 4851.686681\n",
            "Episode 113: Reward = 4803.920987\n",
            "Episode 114: Reward = 4483.125462\n",
            "Episode 115: Reward = 4529.318987\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.77e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 161      |\n",
            "|    time_elapsed    | 720      |\n",
            "|    total_timesteps | 116000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -456     |\n",
            "|    critic_loss     | 13.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 105000   |\n",
            "---------------------------------\n",
            "Episode 116: Reward = 4977.507351\n",
            "Episode 117: Reward = 5058.158448\n",
            "Episode 118: Reward = 4875.486923\n",
            "Episode 119: Reward = 5049.140395\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 2.96e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 160      |\n",
            "|    time_elapsed    | 747      |\n",
            "|    total_timesteps | 120000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -470     |\n",
            "|    critic_loss     | 14.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 109000   |\n",
            "---------------------------------\n",
            "Episode 120: Reward = 5146.193456\n",
            "Episode 121: Reward = 5149.483866\n",
            "Episode 122: Reward = 5216.351639\n",
            "Episode 123: Reward = 5038.887886\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.13e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 160      |\n",
            "|    time_elapsed    | 773      |\n",
            "|    total_timesteps | 124000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -484     |\n",
            "|    critic_loss     | 14.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 113000   |\n",
            "---------------------------------\n",
            "Episode 124: Reward = 4977.202964\n",
            "Episode 125: Reward = 5239.820565\n",
            "Episode 126: Reward = 5136.037539\n",
            "Episode 127: Reward = 4621.625008\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.29e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 159      |\n",
            "|    time_elapsed    | 801      |\n",
            "|    total_timesteps | 128000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -498     |\n",
            "|    critic_loss     | 15       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 117000   |\n",
            "---------------------------------\n",
            "Episode 128: Reward = 5209.163125\n",
            "Episode 129: Reward = 4969.719091\n",
            "Episode 130: Reward = 5267.721128\n",
            "Episode 131: Reward = 5097.381773\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.44e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 159      |\n",
            "|    time_elapsed    | 827      |\n",
            "|    total_timesteps | 132000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -510     |\n",
            "|    critic_loss     | 14.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 121000   |\n",
            "---------------------------------\n",
            "Episode 132: Reward = 4525.792747\n",
            "Episode 133: Reward = 5366.925943\n",
            "Episode 134: Reward = 5177.791261\n",
            "Episode 135: Reward = 5321.247582\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.6e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 159      |\n",
            "|    time_elapsed    | 854      |\n",
            "|    total_timesteps | 136000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -522     |\n",
            "|    critic_loss     | 14.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 125000   |\n",
            "---------------------------------\n",
            "Episode 136: Reward = 5192.133382\n",
            "Episode 137: Reward = 5097.414228\n",
            "Episode 138: Reward = 5469.897949\n",
            "Episode 139: Reward = 5229.33676\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.74e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 881      |\n",
            "|    total_timesteps | 140000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -532     |\n",
            "|    critic_loss     | 14.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 129000   |\n",
            "---------------------------------\n",
            "Episode 140: Reward = 5305.88834\n",
            "Episode 141: Reward = 5402.715315\n",
            "Episode 142: Reward = 5492.296515\n",
            "Episode 143: Reward = 5369.184193\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.88e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 909      |\n",
            "|    total_timesteps | 144000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -542     |\n",
            "|    critic_loss     | 14.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 133000   |\n",
            "---------------------------------\n",
            "Episode 144: Reward = 5327.422106\n",
            "Episode 145: Reward = 5195.662659\n",
            "Episode 146: Reward = 5132.486647\n",
            "Episode 147: Reward = 5451.817641\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.01e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 935      |\n",
            "|    total_timesteps | 148000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -550     |\n",
            "|    critic_loss     | 14.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 137000   |\n",
            "---------------------------------\n",
            "Episode 148: Reward = 5263.605998\n",
            "Episode 149: Reward = 5153.090785\n",
            "Episode 150: Reward = 5723.821732\n",
            "Episode 151: Reward = 5584.838495\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.12e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 157      |\n",
            "|    time_elapsed    | 962      |\n",
            "|    total_timesteps | 152000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -558     |\n",
            "|    critic_loss     | 14.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 141000   |\n",
            "---------------------------------\n",
            "Episode 152: Reward = 4782.396454\n",
            "Episode 153: Reward = 5526.215275\n",
            "Episode 154: Reward = 5571.742195\n",
            "Episode 155: Reward = 5338.243962\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.25e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 157      |\n",
            "|    time_elapsed    | 989      |\n",
            "|    total_timesteps | 156000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -565     |\n",
            "|    critic_loss     | 15.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 145000   |\n",
            "---------------------------------\n",
            "Episode 156: Reward = 5703.321787\n",
            "Episode 157: Reward = 5679.815152\n",
            "Episode 158: Reward = 5683.421197\n",
            "Episode 159: Reward = 5505.659998\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.37e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 157      |\n",
            "|    time_elapsed    | 1016     |\n",
            "|    total_timesteps | 160000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -573     |\n",
            "|    critic_loss     | 15.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 149000   |\n",
            "---------------------------------\n",
            "Episode 160: Reward = 5889.917082\n",
            "Episode 161: Reward = 5763.450295\n",
            "Episode 162: Reward = 5434.408563\n",
            "Episode 163: Reward = 5407.234764\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.48e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 157      |\n",
            "|    time_elapsed    | 1043     |\n",
            "|    total_timesteps | 164000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -580     |\n",
            "|    critic_loss     | 16.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 153000   |\n",
            "---------------------------------\n",
            "Episode 164: Reward = 5661.772993\n",
            "Episode 165: Reward = 5705.467163\n",
            "Episode 166: Reward = 6030.101078\n",
            "Episode 167: Reward = 5896.195143\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.59e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 156      |\n",
            "|    time_elapsed    | 1070     |\n",
            "|    total_timesteps | 168000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -589     |\n",
            "|    critic_loss     | 16.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 157000   |\n",
            "---------------------------------\n",
            "Episode 168: Reward = 5745.303806\n",
            "Episode 169: Reward = 6003.057933\n",
            "Episode 170: Reward = 6032.457975\n",
            "Episode 171: Reward = 5782.042813\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.74e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 156      |\n",
            "|    time_elapsed    | 1098     |\n",
            "|    total_timesteps | 172000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -598     |\n",
            "|    critic_loss     | 16.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 161000   |\n",
            "---------------------------------\n",
            "Episode 172: Reward = 6251.591533\n",
            "Episode 173: Reward = 5859.101028\n",
            "Episode 174: Reward = 5721.702211\n",
            "Episode 175: Reward = 5721.878588\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.86e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 156      |\n",
            "|    time_elapsed    | 1125     |\n",
            "|    total_timesteps | 176000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -607     |\n",
            "|    critic_loss     | 17.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 165000   |\n",
            "---------------------------------\n",
            "Episode 176: Reward = 5987.73631\n",
            "Episode 177: Reward = 5858.629302\n",
            "Episode 178: Reward = 5760.475651\n",
            "Episode 179: Reward = 6189.419019\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.97e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 156      |\n",
            "|    time_elapsed    | 1152     |\n",
            "|    total_timesteps | 180000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -616     |\n",
            "|    critic_loss     | 18.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 169000   |\n",
            "---------------------------------\n",
            "Episode 180: Reward = 5940.358634\n",
            "Episode 181: Reward = 5638.749566\n",
            "Episode 182: Reward = 5939.87269\n",
            "Episode 183: Reward = 3035.943625\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.02e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 156      |\n",
            "|    time_elapsed    | 1179     |\n",
            "|    total_timesteps | 184000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -625     |\n",
            "|    critic_loss     | 20.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 173000   |\n",
            "---------------------------------\n",
            "Episode 184: Reward = 6122.344188\n",
            "Episode 185: Reward = 3764.926062\n",
            "Episode 186: Reward = 4610.784223\n",
            "Episode 187: Reward = 6206.812142\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.07e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 1206     |\n",
            "|    total_timesteps | 188000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -630     |\n",
            "|    critic_loss     | 20.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 177000   |\n",
            "---------------------------------\n",
            "Episode 188: Reward = 5876.2799\n",
            "Episode 189: Reward = 5828.77016\n",
            "Episode 190: Reward = 6149.029731\n",
            "Episode 191: Reward = 985.633791\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.1e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 1233     |\n",
            "|    total_timesteps | 192000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -636     |\n",
            "|    critic_loss     | 20.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 181000   |\n",
            "---------------------------------\n",
            "Episode 192: Reward = 6089.158606\n",
            "Episode 193: Reward = 6150.18695\n",
            "Episode 194: Reward = 5966.887405\n",
            "Episode 195: Reward = 6154.210075\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.15e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 1260     |\n",
            "|    total_timesteps | 196000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -641     |\n",
            "|    critic_loss     | 19.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 185000   |\n",
            "---------------------------------\n",
            "Episode 196: Reward = 4107.626667\n",
            "Episode 197: Reward = 6142.871064\n",
            "Episode 198: Reward = 6120.693416\n",
            "Episode 199: Reward = 6178.754079\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.24e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 1286     |\n",
            "|    total_timesteps | 200000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -647     |\n",
            "|    critic_loss     | 20.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 189000   |\n",
            "---------------------------------\n",
            "Episode 200: Reward = 4086.220684\n",
            "Episode 201: Reward = 6410.988868\n",
            "Episode 202: Reward = 6311.043621\n",
            "Episode 203: Reward = 6375.970684\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.35e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 204      |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 1314     |\n",
            "|    total_timesteps | 204000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -654     |\n",
            "|    critic_loss     | 21       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 193000   |\n",
            "---------------------------------\n",
            "Episode 204: Reward = 6426.55713\n",
            "Episode 205: Reward = 6552.119943\n",
            "Episode 206: Reward = 6509.745732\n",
            "Episode 207: Reward = 1047.631929\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.31e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 208      |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 1341     |\n",
            "|    total_timesteps | 208000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -663     |\n",
            "|    critic_loss     | 28       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 197000   |\n",
            "---------------------------------\n",
            "Episode 208: Reward = 798.938066\n",
            "Episode 209: Reward = 5166.977276\n",
            "Episode 210: Reward = 6254.186753\n",
            "Episode 211: Reward = 6333.514491\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.36e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 212      |\n",
            "|    fps             | 154      |\n",
            "|    time_elapsed    | 1368     |\n",
            "|    total_timesteps | 212000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -674     |\n",
            "|    critic_loss     | 33.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 201000   |\n",
            "---------------------------------\n",
            "Episode 212: Reward = 5155.009509\n",
            "Episode 213: Reward = 6099.198327\n",
            "Episode 214: Reward = 5971.166051\n",
            "Episode 215: Reward = -309.562273\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.28e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 216      |\n",
            "|    fps             | 154      |\n",
            "|    time_elapsed    | 1395     |\n",
            "|    total_timesteps | 216000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -686     |\n",
            "|    critic_loss     | 43.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 205000   |\n",
            "---------------------------------\n",
            "Episode 216: Reward = -723.649365\n",
            "Episode 217: Reward = -767.605994\n",
            "Episode 218: Reward = -414.17677\n",
            "Episode 219: Reward = -603.647423\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.05e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 220      |\n",
            "|    fps             | 154      |\n",
            "|    time_elapsed    | 1422     |\n",
            "|    total_timesteps | 220000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -810     |\n",
            "|    critic_loss     | 183      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 209000   |\n",
            "---------------------------------\n",
            "Episode 220: Reward = -405.343888\n",
            "Episode 221: Reward = -647.197285\n",
            "Episode 222: Reward = -319.165691\n",
            "Episode 223: Reward = -537.96455\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | 4.83e+03  |\n",
            "| time/              |           |\n",
            "|    episodes        | 224       |\n",
            "|    fps             | 154       |\n",
            "|    time_elapsed    | 1449      |\n",
            "|    total_timesteps | 224000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.01e+03 |\n",
            "|    critic_loss     | 117       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 213000    |\n",
            "----------------------------------\n",
            "Episode 224: Reward = -110.446405\n",
            "Episode 225: Reward = -100.571617\n",
            "Episode 226: Reward = -155.296719\n",
            "Episode 227: Reward = 167.159139\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | 4.63e+03  |\n",
            "| time/              |           |\n",
            "|    episodes        | 228       |\n",
            "|    fps             | 154       |\n",
            "|    time_elapsed    | 1476      |\n",
            "|    total_timesteps | 228000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.05e+03 |\n",
            "|    critic_loss     | 82.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 217000    |\n",
            "----------------------------------\n",
            "Episode 228: Reward = -167.12701\n",
            "Episode 229: Reward = -896.329414\n",
            "Episode 230: Reward = -665.463981\n",
            "Episode 231: Reward = -577.917321\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | 4.4e+03   |\n",
            "| time/              |           |\n",
            "|    episodes        | 232       |\n",
            "|    fps             | 154       |\n",
            "|    time_elapsed    | 1503      |\n",
            "|    total_timesteps | 232000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.03e+03 |\n",
            "|    critic_loss     | 70.3      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 221000    |\n",
            "----------------------------------\n",
            "Episode 232: Reward = -686.789787\n",
            "Episode 233: Reward = -514.694359\n",
            "Episode 234: Reward = -408.900034\n",
            "Episode 235: Reward = -410.882559\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 1e+03     |\n",
            "|    ep_rew_mean     | 4.17e+03  |\n",
            "| time/              |           |\n",
            "|    episodes        | 236       |\n",
            "|    fps             | 154       |\n",
            "|    time_elapsed    | 1531      |\n",
            "|    total_timesteps | 236000    |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.06e+03 |\n",
            "|    critic_loss     | 62.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 225000    |\n",
            "----------------------------------\n",
            "Episode 236: Reward = -1117.938423\n",
            "Episode 237: Reward = -509.467591\n",
            "Episode 238: Reward = -310.997071\n",
            "Episode 239: Reward = 2015.797853\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.96e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 240      |\n",
            "|    fps             | 154      |\n",
            "|    time_elapsed    | 1558     |\n",
            "|    total_timesteps | 240000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -1e+03   |\n",
            "|    critic_loss     | 52.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 229000   |\n",
            "---------------------------------\n",
            "Episode 240: Reward = -425.769692\n",
            "Episode 241: Reward = 84.270718\n",
            "Episode 242: Reward = -732.179086\n",
            "Episode 243: Reward = -177.985507\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.73e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 244      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1585     |\n",
            "|    total_timesteps | 244000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -958     |\n",
            "|    critic_loss     | 44.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 233000   |\n",
            "---------------------------------\n",
            "Episode 244: Reward = -886.545279\n",
            "Episode 245: Reward = -188.993078\n",
            "Episode 246: Reward = -130.545851\n",
            "Episode 247: Reward = -805.63022\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.5e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 248      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1612     |\n",
            "|    total_timesteps | 248000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -925     |\n",
            "|    critic_loss     | 37.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 237000   |\n",
            "---------------------------------\n",
            "Episode 248: Reward = -619.542955\n",
            "Episode 249: Reward = -665.777586\n",
            "Episode 250: Reward = -268.601907\n",
            "Episode 251: Reward = -307.053064\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.27e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 252      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1639     |\n",
            "|    total_timesteps | 252000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -868     |\n",
            "|    critic_loss     | 26.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 241000   |\n",
            "---------------------------------\n",
            "Episode 252: Reward = -469.020168\n",
            "Episode 253: Reward = -389.486195\n",
            "Episode 254: Reward = -277.129759\n",
            "Episode 255: Reward = -404.386905\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.04e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 256      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1666     |\n",
            "|    total_timesteps | 256000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -811     |\n",
            "|    critic_loss     | 24.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 245000   |\n",
            "---------------------------------\n",
            "Episode 256: Reward = -189.09511\n",
            "Episode 257: Reward = 3659.707671\n",
            "Episode 258: Reward = 6232.43898\n",
            "Episode 259: Reward = 5960.589219\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.02e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 260      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1693     |\n",
            "|    total_timesteps | 260000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -766     |\n",
            "|    critic_loss     | 25.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 249000   |\n",
            "---------------------------------\n",
            "Episode 260: Reward = 5108.992041\n",
            "Episode 261: Reward = 6399.673547\n",
            "Episode 262: Reward = 6106.246216\n",
            "Episode 263: Reward = 6317.696901\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.05e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 264      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1720     |\n",
            "|    total_timesteps | 264000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -733     |\n",
            "|    critic_loss     | 27       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 253000   |\n",
            "---------------------------------\n",
            "Episode 264: Reward = 6056.404245\n",
            "Episode 265: Reward = 6362.205724\n",
            "Episode 266: Reward = 6303.951014\n",
            "Episode 267: Reward = 6146.644743\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.07e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 268      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1747     |\n",
            "|    total_timesteps | 268000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -708     |\n",
            "|    critic_loss     | 26.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 257000   |\n",
            "---------------------------------\n",
            "Episode 268: Reward = 6560.377674\n",
            "Episode 269: Reward = 6099.914809\n",
            "Episode 270: Reward = 6647.144367\n",
            "Episode 271: Reward = 6641.987001\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.09e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 272      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1774     |\n",
            "|    total_timesteps | 272000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -687     |\n",
            "|    critic_loss     | 25.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 261000   |\n",
            "---------------------------------\n",
            "Episode 272: Reward = 6587.862365\n",
            "Episode 273: Reward = 5631.646821\n",
            "Episode 274: Reward = 6598.366246\n",
            "Episode 275: Reward = 6700.159112\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.11e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 276      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1801     |\n",
            "|    total_timesteps | 276000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -671     |\n",
            "|    critic_loss     | 24.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 265000   |\n",
            "---------------------------------\n",
            "Episode 276: Reward = 6726.718177\n",
            "Episode 277: Reward = 6444.116984\n",
            "Episode 278: Reward = 6504.922749\n",
            "Episode 279: Reward = 6689.920288\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.14e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 280      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1829     |\n",
            "|    total_timesteps | 280000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -661     |\n",
            "|    critic_loss     | 22.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 269000   |\n",
            "---------------------------------\n",
            "Episode 280: Reward = 6613.9623\n",
            "Episode 281: Reward = 6488.002455\n",
            "Episode 282: Reward = 6651.663321\n",
            "Episode 283: Reward = 6818.913888\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.2e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 284      |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 1855     |\n",
            "|    total_timesteps | 284000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -653     |\n",
            "|    critic_loss     | 21.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 273000   |\n",
            "---------------------------------\n",
            "Episode 284: Reward = 6783.96351\n",
            "Episode 285: Reward = 6704.236232\n",
            "Episode 286: Reward = 6671.405283\n",
            "Episode 287: Reward = 6886.415211\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.26e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 288      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 1882     |\n",
            "|    total_timesteps | 288000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -647     |\n",
            "|    critic_loss     | 20.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 277000   |\n",
            "---------------------------------\n",
            "Episode 288: Reward = 6609.695312\n",
            "Episode 289: Reward = 6638.355984\n",
            "Episode 290: Reward = 6493.036307\n",
            "Episode 291: Reward = 6789.192982\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.34e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 292      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 1909     |\n",
            "|    total_timesteps | 292000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -644     |\n",
            "|    critic_loss     | 18.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 281000   |\n",
            "---------------------------------\n",
            "Episode 292: Reward = 6654.363295\n",
            "Episode 293: Reward = 6837.471255\n",
            "Episode 294: Reward = 6980.527859\n",
            "Episode 295: Reward = 6862.359544\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.39e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 296      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 1936     |\n",
            "|    total_timesteps | 296000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -641     |\n",
            "|    critic_loss     | 17.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 285000   |\n",
            "---------------------------------\n",
            "Episode 296: Reward = 7062.734318\n",
            "Episode 297: Reward = 6781.984708\n",
            "Episode 298: Reward = 6984.58131\n",
            "Episode 299: Reward = 7126.740263\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.44e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 300      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 1963     |\n",
            "|    total_timesteps | 300000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -642     |\n",
            "|    critic_loss     | 17.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 289000   |\n",
            "---------------------------------\n",
            "Episode 300: Reward = 6908.383258\n",
            "Episode 301: Reward = 6893.024418\n",
            "Episode 302: Reward = 6667.71178\n",
            "Episode 303: Reward = 7026.013908\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.46e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 304      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 1990     |\n",
            "|    total_timesteps | 304000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -642     |\n",
            "|    critic_loss     | 17.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 293000   |\n",
            "---------------------------------\n",
            "Episode 304: Reward = 6797.714212\n",
            "Episode 305: Reward = 6736.488934\n",
            "Episode 306: Reward = 6775.554555\n",
            "Episode 307: Reward = 6903.567424\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.59e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 308      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2018     |\n",
            "|    total_timesteps | 308000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -644     |\n",
            "|    critic_loss     | 17.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 297000   |\n",
            "---------------------------------\n",
            "Episode 308: Reward = 6987.272621\n",
            "Episode 309: Reward = 6798.962178\n",
            "Episode 310: Reward = 7166.294583\n",
            "Episode 311: Reward = 6724.76366\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.63e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 312      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2045     |\n",
            "|    total_timesteps | 312000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -643     |\n",
            "|    critic_loss     | 16.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 301000   |\n",
            "---------------------------------\n",
            "Episode 312: Reward = 6774.915896\n",
            "Episode 313: Reward = 6826.527735\n",
            "Episode 314: Reward = 6674.105146\n",
            "Episode 315: Reward = 6727.932722\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 3.79e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 316      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2072     |\n",
            "|    total_timesteps | 316000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -645     |\n",
            "|    critic_loss     | 16.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 305000   |\n",
            "---------------------------------\n",
            "Episode 316: Reward = 6855.152997\n",
            "Episode 317: Reward = 6769.908454\n",
            "Episode 318: Reward = 7153.081046\n",
            "Episode 319: Reward = 6853.520924\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.09e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 320      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2099     |\n",
            "|    total_timesteps | 320000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -647     |\n",
            "|    critic_loss     | 16.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 309000   |\n",
            "---------------------------------\n",
            "Episode 320: Reward = 7142.14145\n",
            "Episode 321: Reward = 6988.582622\n",
            "Episode 322: Reward = 7025.884328\n",
            "Episode 323: Reward = 7225.902432\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.39e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 324      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2127     |\n",
            "|    total_timesteps | 324000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -649     |\n",
            "|    critic_loss     | 16.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 313000   |\n",
            "---------------------------------\n",
            "Episode 324: Reward = 6984.244262\n",
            "Episode 325: Reward = 6920.975148\n",
            "Episode 326: Reward = 7184.098616\n",
            "Episode 327: Reward = 7070.253336\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.68e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 328      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2153     |\n",
            "|    total_timesteps | 328000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -651     |\n",
            "|    critic_loss     | 16.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 317000   |\n",
            "---------------------------------\n",
            "Episode 328: Reward = 7167.136015\n",
            "Episode 329: Reward = 7112.616587\n",
            "Episode 330: Reward = 7154.320116\n",
            "Episode 331: Reward = 7177.150161\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 4.99e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 332      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2181     |\n",
            "|    total_timesteps | 332000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -655     |\n",
            "|    critic_loss     | 15.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 321000   |\n",
            "---------------------------------\n",
            "Episode 332: Reward = 7150.983077\n",
            "Episode 333: Reward = 7227.44298\n",
            "Episode 334: Reward = 7242.194917\n",
            "Episode 335: Reward = 7171.601505\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.3e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 336      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2208     |\n",
            "|    total_timesteps | 336000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -659     |\n",
            "|    critic_loss     | 16.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 325000   |\n",
            "---------------------------------\n",
            "Episode 336: Reward = 6889.9085\n",
            "Episode 337: Reward = 7088.788744\n",
            "Episode 338: Reward = 7173.575561\n",
            "Episode 339: Reward = 7279.224855\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.58e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 340      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2235     |\n",
            "|    total_timesteps | 340000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -663     |\n",
            "|    critic_loss     | 16.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 329000   |\n",
            "---------------------------------\n",
            "Episode 340: Reward = 7077.53799\n",
            "Episode 341: Reward = 6913.981487\n",
            "Episode 342: Reward = 7194.75414\n",
            "Episode 343: Reward = 7147.769404\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 5.88e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 344      |\n",
            "|    fps             | 152      |\n",
            "|    time_elapsed    | 2262     |\n",
            "|    total_timesteps | 344000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -666     |\n",
            "|    critic_loss     | 15.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 333000   |\n",
            "---------------------------------\n",
            "Episode 344: Reward = 7260.931963\n",
            "Episode 345: Reward = 7048.304736\n",
            "Episode 346: Reward = 7018.512673\n",
            "Episode 347: Reward = 7142.694287\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.19e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 348      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2289     |\n",
            "|    total_timesteps | 348000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -670     |\n",
            "|    critic_loss     | 15.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 337000   |\n",
            "---------------------------------\n",
            "Episode 348: Reward = 7387.169076\n",
            "Episode 349: Reward = 7179.900324\n",
            "Episode 350: Reward = 7179.041446\n",
            "Episode 351: Reward = 7029.460969\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.49e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 352      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2316     |\n",
            "|    total_timesteps | 352000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -673     |\n",
            "|    critic_loss     | 16       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 341000   |\n",
            "---------------------------------\n",
            "Episode 352: Reward = 7439.291747\n",
            "Episode 353: Reward = 7248.470827\n",
            "Episode 354: Reward = 7316.440822\n",
            "Episode 355: Reward = 7418.656682\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.8e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 356      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2343     |\n",
            "|    total_timesteps | 356000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -677     |\n",
            "|    critic_loss     | 15.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 345000   |\n",
            "---------------------------------\n",
            "Episode 356: Reward = 7325.022965\n",
            "Episode 357: Reward = 7496.452428\n",
            "Episode 358: Reward = 7159.362461\n",
            "Episode 359: Reward = 7143.567698\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.88e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 360      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2370     |\n",
            "|    total_timesteps | 360000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -680     |\n",
            "|    critic_loss     | 15.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 349000   |\n",
            "---------------------------------\n",
            "Episode 360: Reward = 7326.599045\n",
            "Episode 361: Reward = 7159.233027\n",
            "Episode 362: Reward = 7275.545224\n",
            "Episode 363: Reward = 7244.143294\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.92e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 364      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2398     |\n",
            "|    total_timesteps | 364000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -685     |\n",
            "|    critic_loss     | 15.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 353000   |\n",
            "---------------------------------\n",
            "Episode 364: Reward = 7408.394877\n",
            "Episode 365: Reward = 7450.595804\n",
            "Episode 366: Reward = 7375.541039\n",
            "Episode 367: Reward = 7476.256606\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.96e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 368      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2425     |\n",
            "|    total_timesteps | 368000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -688     |\n",
            "|    critic_loss     | 15.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 357000   |\n",
            "---------------------------------\n",
            "Episode 368: Reward = 7294.727402\n",
            "Episode 369: Reward = 7318.317136\n",
            "Episode 370: Reward = 7475.952548\n",
            "Episode 371: Reward = 7467.627374\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7e+03    |\n",
            "| time/              |          |\n",
            "|    episodes        | 372      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2452     |\n",
            "|    total_timesteps | 372000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -692     |\n",
            "|    critic_loss     | 15.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 361000   |\n",
            "---------------------------------\n",
            "Episode 372: Reward = 7689.699005\n",
            "Episode 373: Reward = 7041.884105\n",
            "Episode 374: Reward = 7326.787251\n",
            "Episode 375: Reward = 7298.22217\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.04e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 376      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2479     |\n",
            "|    total_timesteps | 376000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -696     |\n",
            "|    critic_loss     | 15.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 365000   |\n",
            "---------------------------------\n",
            "Episode 376: Reward = 7674.751984\n",
            "Episode 377: Reward = 7544.113695\n",
            "Episode 378: Reward = 7829.904406\n",
            "Episode 379: Reward = 7273.824908\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.08e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 380      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2506     |\n",
            "|    total_timesteps | 380000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -698     |\n",
            "|    critic_loss     | 15.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 369000   |\n",
            "---------------------------------\n",
            "Episode 380: Reward = 7702.102113\n",
            "Episode 381: Reward = 7374.220889\n",
            "Episode 382: Reward = 7490.939242\n",
            "Episode 383: Reward = 7538.036578\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.11e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 384      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2534     |\n",
            "|    total_timesteps | 384000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -701     |\n",
            "|    critic_loss     | 15.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 373000   |\n",
            "---------------------------------\n",
            "Episode 384: Reward = 7506.566183\n",
            "Episode 385: Reward = 7711.780045\n",
            "Episode 386: Reward = 7774.515955\n",
            "Episode 387: Reward = 7427.270682\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.15e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 388      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2561     |\n",
            "|    total_timesteps | 388000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -705     |\n",
            "|    critic_loss     | 15.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 377000   |\n",
            "---------------------------------\n",
            "Episode 388: Reward = 7610.250418\n",
            "Episode 389: Reward = 7805.777639\n",
            "Episode 390: Reward = 7568.428687\n",
            "Episode 391: Reward = 7735.042339\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.19e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 392      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2588     |\n",
            "|    total_timesteps | 392000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -710     |\n",
            "|    critic_loss     | 15.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 381000   |\n",
            "---------------------------------\n",
            "Episode 392: Reward = 7456.181437\n",
            "Episode 393: Reward = 7612.989682\n",
            "Episode 394: Reward = 7501.596873\n",
            "Episode 395: Reward = 7647.593238\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.21e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 396      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2616     |\n",
            "|    total_timesteps | 396000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -714     |\n",
            "|    critic_loss     | 16       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 385000   |\n",
            "---------------------------------\n",
            "Episode 396: Reward = 7427.348379\n",
            "Episode 397: Reward = 7741.97569\n",
            "Episode 398: Reward = 7897.64038\n",
            "Episode 399: Reward = 7654.536575\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.24e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 400      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2643     |\n",
            "|    total_timesteps | 400000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -719     |\n",
            "|    critic_loss     | 19.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 389000   |\n",
            "---------------------------------\n",
            "Episode 400: Reward = 7477.436755\n",
            "Episode 401: Reward = 7854.599021\n",
            "Episode 402: Reward = 7030.835328\n",
            "Episode 403: Reward = -510.248419\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.11e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 404      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2670     |\n",
            "|    total_timesteps | 404000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -728     |\n",
            "|    critic_loss     | 21.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 393000   |\n",
            "---------------------------------\n",
            "Episode 404: Reward = -534.85904\n",
            "Episode 405: Reward = -548.738808\n",
            "Episode 406: Reward = -531.470529\n",
            "Episode 407: Reward = -549.095345\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.89e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 408      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2697     |\n",
            "|    total_timesteps | 408000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -729     |\n",
            "|    critic_loss     | 16.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 397000   |\n",
            "---------------------------------\n",
            "Episode 408: Reward = 7394.006616\n",
            "Episode 409: Reward = 7594.174329\n",
            "Episode 410: Reward = 7795.220912\n",
            "Episode 411: Reward = 7813.344704\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.93e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 412      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2725     |\n",
            "|    total_timesteps | 412000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -727     |\n",
            "|    critic_loss     | 16.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 401000   |\n",
            "---------------------------------\n",
            "Episode 412: Reward = 7776.16134\n",
            "Episode 413: Reward = 7744.60687\n",
            "Episode 414: Reward = 7775.198536\n",
            "Episode 415: Reward = 7759.834533\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.97e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 416      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2751     |\n",
            "|    total_timesteps | 416000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -728     |\n",
            "|    critic_loss     | 17.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 405000   |\n",
            "---------------------------------\n",
            "Episode 416: Reward = 7903.061347\n",
            "Episode 417: Reward = 6372.09407\n",
            "Episode 418: Reward = 7963.746417\n",
            "Episode 419: Reward = 8127.418559\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 6.99e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 420      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2778     |\n",
            "|    total_timesteps | 420000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -730     |\n",
            "|    critic_loss     | 17.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 409000   |\n",
            "---------------------------------\n",
            "Episode 420: Reward = 7874.337872\n",
            "Episode 421: Reward = 8031.184192\n",
            "Episode 422: Reward = 7994.742259\n",
            "Episode 423: Reward = 8009.139757\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.02e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 424      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2805     |\n",
            "|    total_timesteps | 424000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -733     |\n",
            "|    critic_loss     | 17.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 413000   |\n",
            "---------------------------------\n",
            "Episode 424: Reward = 7385.125622\n",
            "Episode 425: Reward = 7783.434114\n",
            "Episode 426: Reward = 8154.39658\n",
            "Episode 427: Reward = 8112.517864\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.05e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 428      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2832     |\n",
            "|    total_timesteps | 428000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -737     |\n",
            "|    critic_loss     | 18.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 417000   |\n",
            "---------------------------------\n",
            "Episode 428: Reward = 6902.631767\n",
            "Episode 429: Reward = 7733.723274\n",
            "Episode 430: Reward = 7673.147507\n",
            "Episode 431: Reward = 8098.324101\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.08e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 432      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2859     |\n",
            "|    total_timesteps | 432000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -741     |\n",
            "|    critic_loss     | 18.4     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 421000   |\n",
            "---------------------------------\n",
            "Episode 432: Reward = 8218.919902\n",
            "Episode 433: Reward = 8160.719144\n",
            "Episode 434: Reward = 7443.657533\n",
            "Episode 435: Reward = 7946.709283\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.11e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 436      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2886     |\n",
            "|    total_timesteps | 436000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -745     |\n",
            "|    critic_loss     | 18.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 425000   |\n",
            "---------------------------------\n",
            "Episode 436: Reward = 8288.077158\n",
            "Episode 437: Reward = 8126.31246\n",
            "Episode 438: Reward = 7987.214841\n",
            "Episode 439: Reward = 7946.415911\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.15e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 440      |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2913     |\n",
            "|    total_timesteps | 440000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -749     |\n",
            "|    critic_loss     | 18.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 429000   |\n",
            "---------------------------------\n",
            "Episode 440: Reward = 8098.177773\n",
            "Episode 441: Reward = 8223.087347\n",
            "Episode 442: Reward = 7734.36646\n",
            "Episode 443: Reward = 8058.022144\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.19e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 444      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 2940     |\n",
            "|    total_timesteps | 444000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -752     |\n",
            "|    critic_loss     | 18.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 433000   |\n",
            "---------------------------------\n",
            "Episode 444: Reward = 8456.002587\n",
            "Episode 445: Reward = 8358.747842\n",
            "Episode 446: Reward = 8151.052484\n",
            "Episode 447: Reward = 8266.887718\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.23e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 448      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 2967     |\n",
            "|    total_timesteps | 448000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -756     |\n",
            "|    critic_loss     | 18.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 437000   |\n",
            "---------------------------------\n",
            "Episode 448: Reward = 8311.027787\n",
            "Episode 449: Reward = 8154.859584\n",
            "Episode 450: Reward = 8507.485942\n",
            "Episode 451: Reward = 8218.812759\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.27e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 452      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 2994     |\n",
            "|    total_timesteps | 452000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -761     |\n",
            "|    critic_loss     | 19.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 441000   |\n",
            "---------------------------------\n",
            "Episode 452: Reward = 8109.506407\n",
            "Episode 453: Reward = 8122.545106\n",
            "Episode 454: Reward = 8165.778388\n",
            "Episode 455: Reward = 8334.413222\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.31e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 456      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3021     |\n",
            "|    total_timesteps | 456000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -767     |\n",
            "|    critic_loss     | 19.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 445000   |\n",
            "---------------------------------\n",
            "Episode 456: Reward = 8294.568356\n",
            "Episode 457: Reward = 8128.349794\n",
            "Episode 458: Reward = 8357.549716\n",
            "Episode 459: Reward = 7986.406411\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.34e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 460      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3049     |\n",
            "|    total_timesteps | 460000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -772     |\n",
            "|    critic_loss     | 19.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 449000   |\n",
            "---------------------------------\n",
            "Episode 460: Reward = 7512.904137\n",
            "Episode 461: Reward = 8484.865252\n",
            "Episode 462: Reward = 8373.090885\n",
            "Episode 463: Reward = 8390.688707\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.39e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 464      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3075     |\n",
            "|    total_timesteps | 464000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -776     |\n",
            "|    critic_loss     | 19.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 453000   |\n",
            "---------------------------------\n",
            "Episode 464: Reward = 8610.889063\n",
            "Episode 465: Reward = 8312.343337\n",
            "Episode 466: Reward = 7479.790282\n",
            "Episode 467: Reward = 8605.486637\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.42e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 468      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3102     |\n",
            "|    total_timesteps | 468000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -781     |\n",
            "|    critic_loss     | 20.7     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 457000   |\n",
            "---------------------------------\n",
            "Episode 468: Reward = 8476.927372\n",
            "Episode 469: Reward = 8602.22779\n",
            "Episode 470: Reward = 8540.50965\n",
            "Episode 471: Reward = 8445.688543\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.46e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 472      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3130     |\n",
            "|    total_timesteps | 472000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -786     |\n",
            "|    critic_loss     | 21       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 461000   |\n",
            "---------------------------------\n",
            "Episode 472: Reward = 8278.671472\n",
            "Episode 473: Reward = 8278.392425\n",
            "Episode 474: Reward = 8016.124185\n",
            "Episode 475: Reward = 8748.665798\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.5e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 476      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3157     |\n",
            "|    total_timesteps | 476000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -790     |\n",
            "|    critic_loss     | 21.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 465000   |\n",
            "---------------------------------\n",
            "Episode 476: Reward = 8187.577844\n",
            "Episode 477: Reward = 8281.601564\n",
            "Episode 478: Reward = 8436.762877\n",
            "Episode 479: Reward = 8320.822555\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.53e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 480      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3183     |\n",
            "|    total_timesteps | 480000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -793     |\n",
            "|    critic_loss     | 20.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 469000   |\n",
            "---------------------------------\n",
            "Episode 480: Reward = 8954.573265\n",
            "Episode 481: Reward = 8572.940047\n",
            "Episode 482: Reward = 8621.883803\n",
            "Episode 483: Reward = 8613.465108\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.58e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 484      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3210     |\n",
            "|    total_timesteps | 484000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -800     |\n",
            "|    critic_loss     | 21.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 473000   |\n",
            "---------------------------------\n",
            "Episode 484: Reward = 8561.927389\n",
            "Episode 485: Reward = 8405.578338\n",
            "Episode 486: Reward = 8652.320025\n",
            "Episode 487: Reward = 8810.449239\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.62e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 488      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3238     |\n",
            "|    total_timesteps | 488000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -804     |\n",
            "|    critic_loss     | 21.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 477000   |\n",
            "---------------------------------\n",
            "Episode 488: Reward = 8436.262631\n",
            "Episode 489: Reward = 8713.738283\n",
            "Episode 490: Reward = 3210.080493\n",
            "Episode 491: Reward = 8387.429486\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.6e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 492      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3264     |\n",
            "|    total_timesteps | 492000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -809     |\n",
            "|    critic_loss     | 22.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 481000   |\n",
            "---------------------------------\n",
            "Episode 492: Reward = 8827.350984\n",
            "Episode 493: Reward = 3676.958113\n",
            "Episode 494: Reward = 7381.049823\n",
            "Episode 495: Reward = 8739.61425\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.59e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 496      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3291     |\n",
            "|    total_timesteps | 496000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -812     |\n",
            "|    critic_loss     | 22.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 485000   |\n",
            "---------------------------------\n",
            "Episode 496: Reward = 8666.68283\n",
            "Episode 497: Reward = 8159.682666\n",
            "Episode 498: Reward = 8671.011014\n",
            "Episode 499: Reward = 8366.026591\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.62e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 500      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3318     |\n",
            "|    total_timesteps | 500000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -813     |\n",
            "|    critic_loss     | 22.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 489000   |\n",
            "---------------------------------\n",
            "Episode 500: Reward = 8908.436294\n",
            "Episode 501: Reward = 8907.83205\n",
            "Episode 502: Reward = 6963.261773\n",
            "Episode 503: Reward = 8641.599674\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 7.81e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 504      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3345     |\n",
            "|    total_timesteps | 504000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -815     |\n",
            "|    critic_loss     | 22.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 493000   |\n",
            "---------------------------------\n",
            "Episode 504: Reward = 8911.465937\n",
            "Episode 505: Reward = 8728.929662\n",
            "Episode 506: Reward = 9042.975897\n",
            "Episode 507: Reward = 8559.680788\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 8.11e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 508      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3371     |\n",
            "|    total_timesteps | 508000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -817     |\n",
            "|    critic_loss     | 23.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 497000   |\n",
            "---------------------------------\n",
            "Episode 508: Reward = 8587.515753\n",
            "Episode 509: Reward = 9115.815933\n",
            "Episode 510: Reward = 8149.942951\n",
            "Episode 511: Reward = 8830.224337\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 8.15e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 512      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3398     |\n",
            "|    total_timesteps | 512000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -820     |\n",
            "|    critic_loss     | 23.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 501000   |\n",
            "---------------------------------\n",
            "Episode 512: Reward = 9173.960692\n",
            "Episode 513: Reward = 9322.04729\n",
            "Episode 514: Reward = 9137.047317\n",
            "Episode 515: Reward = 8774.173807\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 8.2e+03  |\n",
            "| time/              |          |\n",
            "|    episodes        | 516      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3424     |\n",
            "|    total_timesteps | 516000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -823     |\n",
            "|    critic_loss     | 24       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 505000   |\n",
            "---------------------------------\n",
            "Episode 516: Reward = 8893.349533\n",
            "Episode 517: Reward = 9074.15983\n",
            "Episode 518: Reward = 8898.420724\n",
            "Episode 519: Reward = 9057.840451\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1e+03    |\n",
            "|    ep_rew_mean     | 8.26e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 520      |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 3451     |\n",
            "|    total_timesteps | 520000   |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -827     |\n",
            "|    critic_loss     | 24.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 509000   |\n",
            "---------------------------------\n",
            "Episode 520: Reward = 9231.270214\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-322c5289c146>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# Save the final model after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ddpg/ddpg.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     ) -> SelfDDPG:\n\u001b[0;32m--> 123\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/td3/td3.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ) -> SelfTD3:\n\u001b[0;32m--> 222\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             rollout = self.collect_rollouts(\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mtrain_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;31m# Rescale and perform action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/mujoco/half_cheetah_v5.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mx_position_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mx_position_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mx_velocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_position_after\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_position_before\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/mujoco/mujoco_env.py\u001b[0m in \u001b[0;36mdo_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Check control input is contained in the action space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             raise ValueError(\n\u001b[1;32m    198\u001b[0m                 \u001b[0;34mf\"Action dimension mismatch. Expected {(self.model.nu,)}, found {np.array(ctrl).shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from stable_baselines3 import DDPG\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Custom callback to log reward per episode\n",
        "class EpisodeRewardLogger(BaseCallback):\n",
        "    def __init__(self, log_file_path, verbose=0):\n",
        "        super(EpisodeRewardLogger, self).__init__(verbose)\n",
        "        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
        "        self.log_file = open(log_file_path, \"w\")\n",
        "        # CSV header: episode,reward\n",
        "        self.log_file.write(\"episode,reward\\n\")\n",
        "        self.episode_num = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Dummy implementation to satisfy abstract class requirements.\n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        Called at the end of a rollout (here, an episode because we use train_freq=(1, \"episode\")).\n",
        "        Extracts reward info from Monitor (ep_info_buffer) and logs it.\n",
        "        \"\"\"\n",
        "        if len(self.model.ep_info_buffer) > 0:\n",
        "            ep_info = self.model.ep_info_buffer[-1]\n",
        "            reward = ep_info.get(\"r\", 0)\n",
        "            self.episode_num += 1\n",
        "            self.log_file.write(f\"{self.episode_num},{reward}\\n\")\n",
        "            self.log_file.flush()\n",
        "            if self.verbose > 0:\n",
        "                print(f\"Episode {self.episode_num}: Reward = {reward}\")\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        if self.log_file is not None:\n",
        "            self.log_file.close()\n",
        "\n",
        "# -------------------------\n",
        "# Environment and Model Setup\n",
        "# -------------------------\n",
        "# Create a function that builds the environment.\n",
        "# We pass render_mode=None to avoid opening a GUI window.\n",
        "def make_env():\n",
        "    env = gym.make(\"HalfCheetah-v5\", render_mode=None)\n",
        "    env = Monitor(env)\n",
        "    return env\n",
        "\n",
        "# Wrap the environment in DummyVecEnv to use vectorized environments.\n",
        "env = DummyVecEnv([make_env])\n",
        "\n",
        "n_actions = env.action_space.shape[-1]\n",
        "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
        "\n",
        "# Use CUDA if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = DDPG(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    action_noise=action_noise,\n",
        "    learning_rate=1e-3,\n",
        "    buffer_size=700000,\n",
        "    learning_starts=10000,\n",
        "    batch_size=256,\n",
        "    tau=0.005,\n",
        "    gamma=0.99,\n",
        "    train_freq=(1, \"episode\"),  # Training per episode\n",
        "    gradient_steps=-1,\n",
        "    verbose=1,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "os.makedirs(\"./DDPGlogs/\", exist_ok=True)\n",
        "\n",
        "# Initialize the episode reward logger callback\n",
        "episode_logger = EpisodeRewardLogger(log_file_path=\"./logs/ddpg_episode_log.csv\", verbose=1)\n",
        "\n",
        "# Create a callback to periodically save the model (every 100,000 timesteps)\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=100000,\n",
        "    save_path='./DDPGlogs/',\n",
        "    name_prefix='ddpg_halfcheetah'\n",
        ")\n",
        "\n",
        "# Combine callbacks\n",
        "callbacks = [checkpoint_callback, episode_logger]\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "model.learn(total_timesteps=700000, callback=callbacks)\n",
        "\n",
        "# Save the final model after training\n",
        "model.save(\"ddpg_halfcheetah_final\")\n",
        "\n",
        "# Close the logger to save the file\n",
        "episode_logger._on_training_end()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkm6hWweVFxH",
        "outputId": "c9002a1f-f193-422f-aaf8-1fb887044be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable_baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable_baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable_baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable_baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable_baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable_baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable_baselines3-2.6.0\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"stable_baselines3\"\n",
        "!pip install \"gymnasium[mujoco]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbX2b2FQVQFv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
