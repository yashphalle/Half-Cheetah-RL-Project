{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR7IbFzSM-_M",
        "outputId": "7324173d-3b38-4f87-c5c4-327bc5c8c219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f15475efc397>:119: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  actions = torch.tensor(actions, dtype=torch.float).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward = -322.59, Avg Q Loss = 0.063979, Avg Mu Loss = 0.279278\n",
            "Episode 2: Reward = -401.36, Avg Q Loss = 0.036174, Avg Mu Loss = 0.027727\n",
            "Episode 3: Reward = -445.02, Avg Q Loss = 0.041199, Avg Mu Loss = -0.326236\n",
            "Episode 4: Reward = -523.70, Avg Q Loss = 0.088967, Avg Mu Loss = -0.469968\n",
            "Episode 5: Reward = -352.16, Avg Q Loss = 0.135112, Avg Mu Loss = -1.046863\n",
            "Episode 6: Reward = -329.09, Avg Q Loss = 0.171037, Avg Mu Loss = -1.538507\n",
            "Episode 7: Reward = -231.84, Avg Q Loss = 0.202725, Avg Mu Loss = -1.999841\n",
            "Episode 8: Reward = 237.54, Avg Q Loss = 0.308300, Avg Mu Loss = -2.257400\n",
            "Episode 9: Reward = 200.15, Avg Q Loss = 0.516101, Avg Mu Loss = -2.721211\n",
            "Episode 10: Reward = 250.21, Avg Q Loss = 0.609636, Avg Mu Loss = -3.388768\n",
            "Episode 11: Reward = 442.36, Avg Q Loss = 0.638714, Avg Mu Loss = -4.121386\n",
            "Episode 12: Reward = 687.12, Avg Q Loss = 0.624729, Avg Mu Loss = -5.174935\n",
            "Episode 13: Reward = 757.36, Avg Q Loss = 0.636483, Avg Mu Loss = -6.449167\n",
            "Episode 14: Reward = 713.57, Avg Q Loss = 0.650427, Avg Mu Loss = -7.757500\n",
            "Episode 15: Reward = 726.80, Avg Q Loss = 0.687246, Avg Mu Loss = -9.116576\n",
            "Episode 16: Reward = 700.95, Avg Q Loss = 0.689203, Avg Mu Loss = -10.544769\n",
            "Episode 17: Reward = 627.27, Avg Q Loss = 0.690564, Avg Mu Loss = -11.938084\n",
            "Episode 18: Reward = 699.39, Avg Q Loss = 0.696640, Avg Mu Loss = -13.334027\n",
            "Episode 19: Reward = 208.85, Avg Q Loss = 0.751361, Avg Mu Loss = -14.621310\n",
            "Episode 20: Reward = 662.61, Avg Q Loss = 0.799326, Avg Mu Loss = -15.911833\n",
            "Episode 21: Reward = 652.45, Avg Q Loss = 0.816499, Avg Mu Loss = -17.140390\n",
            "Episode 22: Reward = 629.65, Avg Q Loss = 0.962706, Avg Mu Loss = -18.499291\n",
            "Episode 23: Reward = 694.05, Avg Q Loss = 0.960581, Avg Mu Loss = -20.193923\n",
            "Episode 24: Reward = 768.65, Avg Q Loss = 1.005472, Avg Mu Loss = -22.092860\n",
            "Episode 25: Reward = 988.11, Avg Q Loss = 1.087348, Avg Mu Loss = -24.096046\n",
            "Episode 26: Reward = 1310.87, Avg Q Loss = 1.290033, Avg Mu Loss = -26.292999\n",
            "Episode 27: Reward = 1437.96, Avg Q Loss = 1.506315, Avg Mu Loss = -28.773483\n",
            "Episode 28: Reward = 1943.59, Avg Q Loss = 1.825015, Avg Mu Loss = -31.618371\n",
            "Episode 29: Reward = 2044.60, Avg Q Loss = 2.122564, Avg Mu Loss = -34.957133\n",
            "Episode 30: Reward = 1984.88, Avg Q Loss = 2.462334, Avg Mu Loss = -38.661769\n",
            "Episode 31: Reward = 2054.07, Avg Q Loss = 2.880108, Avg Mu Loss = -42.614467\n",
            "Episode 32: Reward = 2324.56, Avg Q Loss = 3.424958, Avg Mu Loss = -46.644135\n",
            "Episode 33: Reward = 1921.00, Avg Q Loss = 3.850945, Avg Mu Loss = -50.675224\n",
            "Episode 34: Reward = 1902.21, Avg Q Loss = 4.398758, Avg Mu Loss = -54.667830\n",
            "Episode 35: Reward = 2243.78, Avg Q Loss = 4.895346, Avg Mu Loss = -59.142983\n",
            "Episode 36: Reward = 2321.90, Avg Q Loss = 5.625376, Avg Mu Loss = -63.669723\n",
            "Episode 37: Reward = 2634.13, Avg Q Loss = 6.262072, Avg Mu Loss = -68.515675\n",
            "Episode 38: Reward = 2644.62, Avg Q Loss = 6.864821, Avg Mu Loss = -73.647615\n",
            "Episode 39: Reward = 3030.89, Avg Q Loss = 7.223775, Avg Mu Loss = -79.272033\n",
            "Episode 40: Reward = 2673.66, Avg Q Loss = 7.728196, Avg Mu Loss = -85.038796\n",
            "Episode 41: Reward = 3181.12, Avg Q Loss = 8.480195, Avg Mu Loss = -90.914880\n",
            "Episode 42: Reward = 3240.16, Avg Q Loss = 8.993933, Avg Mu Loss = -97.149160\n",
            "Episode 43: Reward = 1756.51, Avg Q Loss = 9.818652, Avg Mu Loss = -103.169715\n",
            "Episode 44: Reward = 3273.62, Avg Q Loss = 10.953923, Avg Mu Loss = -108.362343\n",
            "Episode 45: Reward = 2771.10, Avg Q Loss = 11.180866, Avg Mu Loss = -114.447066\n",
            "Episode 46: Reward = 3314.52, Avg Q Loss = 11.453999, Avg Mu Loss = -120.338901\n",
            "Episode 47: Reward = 3143.30, Avg Q Loss = 11.687447, Avg Mu Loss = -126.273787\n",
            "Episode 48: Reward = 3359.85, Avg Q Loss = 11.841071, Avg Mu Loss = -132.160362\n",
            "Episode 49: Reward = 3734.85, Avg Q Loss = 12.232921, Avg Mu Loss = -138.170108\n",
            "Episode 50: Reward = 3753.39, Avg Q Loss = 12.818114, Avg Mu Loss = -144.566521\n",
            "Episode 51: Reward = 3863.57, Avg Q Loss = 13.358832, Avg Mu Loss = -151.476795\n",
            "Episode 52: Reward = 3627.79, Avg Q Loss = 13.997398, Avg Mu Loss = -158.458071\n",
            "Episode 53: Reward = 3583.13, Avg Q Loss = 14.676615, Avg Mu Loss = -165.494170\n",
            "Episode 54: Reward = 3555.63, Avg Q Loss = 15.524153, Avg Mu Loss = -172.865434\n",
            "Episode 55: Reward = 3438.45, Avg Q Loss = 16.605063, Avg Mu Loss = -180.070875\n",
            "Episode 56: Reward = 3408.72, Avg Q Loss = 17.377071, Avg Mu Loss = -186.729187\n",
            "Episode 57: Reward = 3502.68, Avg Q Loss = 18.310955, Avg Mu Loss = -193.235408\n",
            "Episode 58: Reward = 3490.75, Avg Q Loss = 19.016707, Avg Mu Loss = -199.590505\n",
            "Episode 59: Reward = 3561.15, Avg Q Loss = 20.293888, Avg Mu Loss = -205.826397\n",
            "Episode 60: Reward = 3869.70, Avg Q Loss = 21.176823, Avg Mu Loss = -212.043934\n",
            "Episode 61: Reward = 3679.02, Avg Q Loss = 22.145350, Avg Mu Loss = -218.338523\n",
            "Episode 62: Reward = 4436.02, Avg Q Loss = 22.985242, Avg Mu Loss = -224.272829\n",
            "Episode 63: Reward = 3837.60, Avg Q Loss = 23.771080, Avg Mu Loss = -231.347021\n",
            "Episode 64: Reward = 4438.69, Avg Q Loss = 24.169289, Avg Mu Loss = -239.197929\n",
            "Episode 65: Reward = 4171.33, Avg Q Loss = 24.876623, Avg Mu Loss = -246.278821\n",
            "Episode 66: Reward = 4093.72, Avg Q Loss = 24.946007, Avg Mu Loss = -253.313919\n",
            "Episode 67: Reward = 4042.09, Avg Q Loss = 24.757290, Avg Mu Loss = -260.169170\n",
            "Episode 68: Reward = 2065.66, Avg Q Loss = 24.559015, Avg Mu Loss = -266.240526\n",
            "Episode 69: Reward = 3629.52, Avg Q Loss = 24.309946, Avg Mu Loss = -269.305201\n",
            "Episode 70: Reward = 3636.85, Avg Q Loss = 24.184923, Avg Mu Loss = -274.910600\n",
            "Episode 71: Reward = 4037.34, Avg Q Loss = 23.870673, Avg Mu Loss = -280.829129\n",
            "Episode 72: Reward = 4298.41, Avg Q Loss = 23.367009, Avg Mu Loss = -287.019962\n",
            "Episode 73: Reward = 4359.81, Avg Q Loss = 23.136840, Avg Mu Loss = -293.311062\n",
            "Episode 74: Reward = 4409.95, Avg Q Loss = 22.736249, Avg Mu Loss = -299.544012\n",
            "Episode 75: Reward = 4326.96, Avg Q Loss = 22.328118, Avg Mu Loss = -305.719665\n",
            "Episode 76: Reward = 4221.29, Avg Q Loss = 22.186018, Avg Mu Loss = -312.436148\n",
            "Episode 77: Reward = 4433.67, Avg Q Loss = 21.650751, Avg Mu Loss = -318.520153\n",
            "Episode 78: Reward = 4705.98, Avg Q Loss = 21.076237, Avg Mu Loss = -325.137610\n",
            "Episode 79: Reward = 4680.32, Avg Q Loss = 20.500418, Avg Mu Loss = -331.683077\n",
            "Episode 80: Reward = 5013.01, Avg Q Loss = 19.937891, Avg Mu Loss = -338.593635\n",
            "Episode 81: Reward = 4804.93, Avg Q Loss = 18.860260, Avg Mu Loss = -345.280979\n",
            "Episode 82: Reward = 4918.10, Avg Q Loss = 18.139151, Avg Mu Loss = -351.927374\n",
            "Episode 83: Reward = 4706.56, Avg Q Loss = 17.675819, Avg Mu Loss = -358.237828\n",
            "Episode 84: Reward = 4713.53, Avg Q Loss = 17.060475, Avg Mu Loss = -365.516331\n",
            "Episode 85: Reward = 4645.42, Avg Q Loss = 16.683492, Avg Mu Loss = -371.422572\n",
            "Episode 86: Reward = 4504.66, Avg Q Loss = 16.829790, Avg Mu Loss = -377.135449\n",
            "Episode 87: Reward = 4813.04, Avg Q Loss = 16.540390, Avg Mu Loss = -383.240911\n",
            "Episode 88: Reward = 4529.94, Avg Q Loss = 16.295543, Avg Mu Loss = -390.390835\n",
            "Episode 89: Reward = 4355.70, Avg Q Loss = 16.113862, Avg Mu Loss = -401.439042\n",
            "Episode 90: Reward = 4386.78, Avg Q Loss = 15.767165, Avg Mu Loss = -407.478003\n",
            "Episode 91: Reward = 4651.09, Avg Q Loss = 15.481018, Avg Mu Loss = -413.510628\n",
            "Episode 92: Reward = 5055.54, Avg Q Loss = 15.012453, Avg Mu Loss = -419.478543\n",
            "Episode 93: Reward = 4935.94, Avg Q Loss = 14.599931, Avg Mu Loss = -425.722955\n",
            "Episode 94: Reward = 5219.15, Avg Q Loss = 14.411993, Avg Mu Loss = -431.919890\n",
            "Episode 95: Reward = 5037.64, Avg Q Loss = 14.138449, Avg Mu Loss = -437.977291\n",
            "Episode 96: Reward = 4882.85, Avg Q Loss = 13.895549, Avg Mu Loss = -444.111348\n",
            "Episode 97: Reward = 4737.38, Avg Q Loss = 13.877052, Avg Mu Loss = -449.712184\n",
            "Episode 98: Reward = 5238.14, Avg Q Loss = 13.914632, Avg Mu Loss = -455.046721\n",
            "Episode 99: Reward = 4811.30, Avg Q Loss = 13.670808, Avg Mu Loss = -460.419452\n",
            "Episode 100: Reward = 4660.98, Avg Q Loss = 13.804139, Avg Mu Loss = -465.212358\n",
            "Episode 101: Reward = 4884.05, Avg Q Loss = 14.445416, Avg Mu Loss = -469.744490\n",
            "Episode 102: Reward = 4704.78, Avg Q Loss = 14.903062, Avg Mu Loss = -474.330218\n",
            "Episode 103: Reward = 5055.16, Avg Q Loss = 15.618524, Avg Mu Loss = -479.211982\n",
            "Episode 104: Reward = 3804.04, Avg Q Loss = 16.221838, Avg Mu Loss = -483.986517\n",
            "Episode 105: Reward = 4467.04, Avg Q Loss = 17.008100, Avg Mu Loss = -488.539482\n",
            "Episode 106: Reward = 4244.07, Avg Q Loss = 17.171392, Avg Mu Loss = -493.635777\n",
            "Episode 107: Reward = 4398.72, Avg Q Loss = 18.058516, Avg Mu Loss = -497.745912\n",
            "Episode 108: Reward = 4904.55, Avg Q Loss = 18.531249, Avg Mu Loss = -502.199310\n",
            "Episode 109: Reward = 3639.32, Avg Q Loss = 19.276250, Avg Mu Loss = -507.276351\n",
            "Episode 110: Reward = 3959.44, Avg Q Loss = 19.349757, Avg Mu Loss = -512.653210\n",
            "Episode 111: Reward = 4980.98, Avg Q Loss = 19.838373, Avg Mu Loss = -517.290342\n",
            "Episode 112: Reward = 4753.46, Avg Q Loss = 19.973204, Avg Mu Loss = -522.165556\n",
            "Episode 113: Reward = 4845.62, Avg Q Loss = 19.903191, Avg Mu Loss = -526.291975\n",
            "Episode 114: Reward = 4391.17, Avg Q Loss = 20.038070, Avg Mu Loss = -529.828927\n",
            "Episode 115: Reward = 4546.39, Avg Q Loss = 20.338943, Avg Mu Loss = -532.922854\n",
            "Episode 116: Reward = 4713.07, Avg Q Loss = 20.681014, Avg Mu Loss = -536.545060\n",
            "Episode 117: Reward = 4671.22, Avg Q Loss = 20.505618, Avg Mu Loss = -540.559462\n",
            "Episode 118: Reward = 4641.28, Avg Q Loss = 20.714949, Avg Mu Loss = -544.343370\n",
            "Episode 119: Reward = 4693.95, Avg Q Loss = 20.435819, Avg Mu Loss = -548.372869\n",
            "Episode 120: Reward = 4983.41, Avg Q Loss = 19.913987, Avg Mu Loss = -553.580220\n",
            "Episode 121: Reward = 5067.38, Avg Q Loss = 19.640032, Avg Mu Loss = -558.477726\n",
            "Episode 122: Reward = 5010.47, Avg Q Loss = 18.854875, Avg Mu Loss = -563.130021\n",
            "Episode 123: Reward = 4892.12, Avg Q Loss = 18.591112, Avg Mu Loss = -567.712418\n",
            "Episode 124: Reward = 4494.79, Avg Q Loss = 18.120815, Avg Mu Loss = -572.203854\n",
            "Episode 125: Reward = 5179.83, Avg Q Loss = 17.995213, Avg Mu Loss = -577.156385\n",
            "Episode 126: Reward = 5265.54, Avg Q Loss = 17.080337, Avg Mu Loss = -582.484097\n",
            "Episode 127: Reward = 5325.12, Avg Q Loss = 16.138624, Avg Mu Loss = -587.860068\n",
            "Episode 128: Reward = 4936.81, Avg Q Loss = 15.392092, Avg Mu Loss = -592.897925\n",
            "Episode 129: Reward = -1041.55, Avg Q Loss = 24.327499, Avg Mu Loss = -591.737721\n",
            "Episode 130: Reward = 4948.67, Avg Q Loss = 27.254075, Avg Mu Loss = -590.156542\n",
            "Episode 131: Reward = 4539.77, Avg Q Loss = 24.464866, Avg Mu Loss = -593.856181\n",
            "Episode 132: Reward = 4756.69, Avg Q Loss = 23.978406, Avg Mu Loss = -595.540647\n",
            "Episode 133: Reward = 5111.57, Avg Q Loss = 22.416023, Avg Mu Loss = -598.388898\n",
            "Episode 134: Reward = 4937.53, Avg Q Loss = 21.564502, Avg Mu Loss = -601.483914\n",
            "Episode 135: Reward = 5294.62, Avg Q Loss = 21.641751, Avg Mu Loss = -604.726926\n",
            "Episode 136: Reward = 5132.54, Avg Q Loss = 21.134642, Avg Mu Loss = -607.695194\n",
            "Episode 137: Reward = 5238.91, Avg Q Loss = 20.818651, Avg Mu Loss = -610.235200\n",
            "Episode 138: Reward = 5021.38, Avg Q Loss = 20.362920, Avg Mu Loss = -613.119508\n",
            "Episode 139: Reward = 5109.19, Avg Q Loss = 20.590678, Avg Mu Loss = -614.935416\n",
            "Episode 140: Reward = 4994.35, Avg Q Loss = 20.967093, Avg Mu Loss = -616.824136\n",
            "Episode 141: Reward = 4792.26, Avg Q Loss = 21.072589, Avg Mu Loss = -618.683054\n",
            "Episode 142: Reward = 4714.57, Avg Q Loss = 21.163328, Avg Mu Loss = -620.093433\n",
            "Episode 143: Reward = 5008.77, Avg Q Loss = 21.051271, Avg Mu Loss = -621.665354\n",
            "Episode 144: Reward = 4279.85, Avg Q Loss = 20.717808, Avg Mu Loss = -623.630848\n",
            "Episode 145: Reward = 5338.13, Avg Q Loss = 21.182142, Avg Mu Loss = -624.476939\n",
            "Episode 146: Reward = 4825.21, Avg Q Loss = 21.221189, Avg Mu Loss = -625.851711\n",
            "Episode 147: Reward = 4894.26, Avg Q Loss = 22.013207, Avg Mu Loss = -626.151675\n",
            "Episode 148: Reward = 5274.68, Avg Q Loss = 22.149635, Avg Mu Loss = -627.209585\n",
            "Episode 149: Reward = 4634.35, Avg Q Loss = 21.354581, Avg Mu Loss = -636.483800\n",
            "Episode 150: Reward = 4967.95, Avg Q Loss = 20.530690, Avg Mu Loss = -646.583934\n",
            "Episode 151: Reward = 5007.66, Avg Q Loss = 19.337675, Avg Mu Loss = -648.163555\n",
            "Episode 152: Reward = 5161.71, Avg Q Loss = 18.220470, Avg Mu Loss = -650.383029\n",
            "Episode 153: Reward = 5107.53, Avg Q Loss = 18.367646, Avg Mu Loss = -651.630150\n",
            "Episode 154: Reward = 3880.42, Avg Q Loss = 21.232001, Avg Mu Loss = -652.826111\n",
            "Episode 155: Reward = 1376.35, Avg Q Loss = 23.611333, Avg Mu Loss = -651.441896\n",
            "Episode 156: Reward = 3952.74, Avg Q Loss = 25.953972, Avg Mu Loss = -645.621932\n",
            "Episode 157: Reward = 4287.07, Avg Q Loss = 26.777556, Avg Mu Loss = -645.598087\n",
            "Episode 158: Reward = 4600.98, Avg Q Loss = 27.306372, Avg Mu Loss = -646.094265\n",
            "Episode 159: Reward = 5025.42, Avg Q Loss = 27.251412, Avg Mu Loss = -646.999986\n",
            "Episode 160: Reward = 4987.67, Avg Q Loss = 26.892573, Avg Mu Loss = -648.275195\n",
            "Episode 161: Reward = 5284.91, Avg Q Loss = 26.802999, Avg Mu Loss = -650.194364\n",
            "Episode 162: Reward = 5105.30, Avg Q Loss = 26.324997, Avg Mu Loss = -652.222158\n",
            "Episode 163: Reward = 4923.22, Avg Q Loss = 25.638818, Avg Mu Loss = -653.554745\n",
            "Episode 164: Reward = 5166.00, Avg Q Loss = 24.801308, Avg Mu Loss = -655.522341\n",
            "Episode 165: Reward = 4926.68, Avg Q Loss = 23.745644, Avg Mu Loss = -657.496065\n",
            "Episode 166: Reward = 4571.88, Avg Q Loss = 23.297453, Avg Mu Loss = -658.885510\n",
            "Episode 167: Reward = 4938.07, Avg Q Loss = 22.409223, Avg Mu Loss = -660.608281\n",
            "Episode 168: Reward = 4993.22, Avg Q Loss = 21.538158, Avg Mu Loss = -661.959559\n",
            "Episode 169: Reward = 5123.90, Avg Q Loss = 21.336633, Avg Mu Loss = -663.237461\n",
            "Episode 170: Reward = 4795.42, Avg Q Loss = 20.614897, Avg Mu Loss = -664.778138\n",
            "Episode 171: Reward = 5044.15, Avg Q Loss = 20.108219, Avg Mu Loss = -665.827370\n",
            "Episode 172: Reward = 4937.50, Avg Q Loss = 19.462127, Avg Mu Loss = -667.086867\n",
            "Episode 173: Reward = 5072.79, Avg Q Loss = 19.063195, Avg Mu Loss = -668.225924\n",
            "Episode 174: Reward = 5233.36, Avg Q Loss = 17.569139, Avg Mu Loss = -670.144082\n",
            "Episode 175: Reward = 5249.62, Avg Q Loss = 15.730624, Avg Mu Loss = -675.108338\n",
            "Episode 176: Reward = 5334.01, Avg Q Loss = 13.653729, Avg Mu Loss = -684.701240\n",
            "Episode 177: Reward = 5496.32, Avg Q Loss = 12.308183, Avg Mu Loss = -686.696439\n",
            "Episode 178: Reward = 4608.23, Avg Q Loss = 11.405330, Avg Mu Loss = -688.187497\n",
            "Episode 179: Reward = 4407.31, Avg Q Loss = 12.157034, Avg Mu Loss = -687.890550\n",
            "Episode 180: Reward = 4571.16, Avg Q Loss = 12.629305, Avg Mu Loss = -687.476370\n",
            "Episode 181: Reward = 4959.70, Avg Q Loss = 13.197832, Avg Mu Loss = -686.962174\n",
            "Episode 182: Reward = 5409.03, Avg Q Loss = 13.819659, Avg Mu Loss = -686.531936\n",
            "Episode 183: Reward = 5625.53, Avg Q Loss = 13.868110, Avg Mu Loss = -686.963937\n",
            "Episode 184: Reward = 5241.65, Avg Q Loss = 14.021222, Avg Mu Loss = -687.337584\n",
            "Episode 185: Reward = 5126.26, Avg Q Loss = 14.408085, Avg Mu Loss = -687.213759\n",
            "Episode 186: Reward = 5239.43, Avg Q Loss = 14.906259, Avg Mu Loss = -687.319519\n",
            "Episode 187: Reward = 5528.98, Avg Q Loss = 15.081512, Avg Mu Loss = -687.982260\n",
            "Episode 188: Reward = 5192.81, Avg Q Loss = 15.276943, Avg Mu Loss = -688.326097\n",
            "Episode 189: Reward = 4940.91, Avg Q Loss = 15.607166, Avg Mu Loss = -688.411517\n",
            "Episode 190: Reward = 5461.93, Avg Q Loss = 15.914593, Avg Mu Loss = -689.025866\n",
            "Episode 191: Reward = 5405.79, Avg Q Loss = 15.887983, Avg Mu Loss = -689.932196\n",
            "Episode 192: Reward = 5336.50, Avg Q Loss = 15.722985, Avg Mu Loss = -690.662022\n",
            "Episode 193: Reward = 5337.10, Avg Q Loss = 15.693804, Avg Mu Loss = -691.338649\n",
            "Episode 194: Reward = 5318.50, Avg Q Loss = 15.971631, Avg Mu Loss = -691.565094\n",
            "Episode 195: Reward = 5387.06, Avg Q Loss = 15.956572, Avg Mu Loss = -691.982994\n",
            "Episode 196: Reward = 4936.00, Avg Q Loss = 16.069627, Avg Mu Loss = -692.109786\n",
            "Episode 197: Reward = 4712.63, Avg Q Loss = 15.789645, Avg Mu Loss = -691.805505\n",
            "Episode 198: Reward = 5380.80, Avg Q Loss = 15.839166, Avg Mu Loss = -691.789299\n",
            "Episode 199: Reward = 5154.23, Avg Q Loss = 14.755038, Avg Mu Loss = -693.804996\n",
            "Episode 200: Reward = 5368.04, Avg Q Loss = 13.584467, Avg Mu Loss = -695.316271\n",
            "Episode 201: Reward = 5295.23, Avg Q Loss = 12.934773, Avg Mu Loss = -696.554056\n",
            "Episode 202: Reward = 5190.86, Avg Q Loss = 12.469858, Avg Mu Loss = -697.170699\n",
            "Episode 203: Reward = 5121.96, Avg Q Loss = 12.434803, Avg Mu Loss = -697.318392\n",
            "Episode 204: Reward = 5322.57, Avg Q Loss = 12.020757, Avg Mu Loss = -697.561176\n",
            "Episode 205: Reward = 5151.51, Avg Q Loss = 11.730243, Avg Mu Loss = -698.385362\n",
            "Episode 206: Reward = 5319.20, Avg Q Loss = 11.135300, Avg Mu Loss = -699.058636\n",
            "Episode 207: Reward = 5146.69, Avg Q Loss = 10.760349, Avg Mu Loss = -699.291510\n",
            "Episode 208: Reward = 5587.50, Avg Q Loss = 10.459016, Avg Mu Loss = -699.645242\n",
            "Episode 209: Reward = 5103.75, Avg Q Loss = 10.445881, Avg Mu Loss = -699.983781\n",
            "Episode 210: Reward = 5313.89, Avg Q Loss = 11.125553, Avg Mu Loss = -699.383924\n",
            "Episode 211: Reward = 5516.14, Avg Q Loss = 11.294762, Avg Mu Loss = -699.201648\n",
            "Episode 212: Reward = 5032.29, Avg Q Loss = 11.898434, Avg Mu Loss = -699.140583\n",
            "Episode 213: Reward = 5640.44, Avg Q Loss = 12.028739, Avg Mu Loss = -699.101713\n",
            "Episode 214: Reward = 4904.79, Avg Q Loss = 12.031159, Avg Mu Loss = -699.197746\n",
            "Episode 215: Reward = 5158.52, Avg Q Loss = 12.518685, Avg Mu Loss = -698.835378\n",
            "Episode 216: Reward = 5196.80, Avg Q Loss = 12.459047, Avg Mu Loss = -698.786334\n",
            "Episode 217: Reward = 5625.96, Avg Q Loss = 12.026588, Avg Mu Loss = -699.532785\n",
            "Episode 218: Reward = 5284.39, Avg Q Loss = 11.752771, Avg Mu Loss = -700.136406\n",
            "Episode 219: Reward = 5407.24, Avg Q Loss = 11.551554, Avg Mu Loss = -700.406443\n",
            "Episode 220: Reward = 5180.50, Avg Q Loss = 11.224439, Avg Mu Loss = -700.708781\n",
            "Episode 221: Reward = 5132.84, Avg Q Loss = 10.987068, Avg Mu Loss = -700.780106\n",
            "Episode 222: Reward = 5215.74, Avg Q Loss = 10.767435, Avg Mu Loss = -700.955261\n",
            "Episode 223: Reward = 5559.63, Avg Q Loss = 10.615686, Avg Mu Loss = -701.301574\n",
            "Episode 224: Reward = 5136.29, Avg Q Loss = 10.629530, Avg Mu Loss = -701.608608\n",
            "Episode 225: Reward = 5453.81, Avg Q Loss = 10.655580, Avg Mu Loss = -701.663485\n",
            "Episode 226: Reward = 5503.79, Avg Q Loss = 10.595597, Avg Mu Loss = -701.915766\n",
            "Episode 227: Reward = 5059.50, Avg Q Loss = 10.716360, Avg Mu Loss = -702.159667\n",
            "Episode 228: Reward = 5195.86, Avg Q Loss = 11.042112, Avg Mu Loss = -701.624876\n",
            "Episode 229: Reward = 5374.99, Avg Q Loss = 11.039836, Avg Mu Loss = -701.486999\n",
            "Episode 230: Reward = 5504.03, Avg Q Loss = 10.821516, Avg Mu Loss = -702.069163\n",
            "Episode 231: Reward = 5186.49, Avg Q Loss = 10.531251, Avg Mu Loss = -702.111149\n",
            "Episode 232: Reward = 5203.06, Avg Q Loss = 10.366319, Avg Mu Loss = -702.329184\n",
            "Episode 233: Reward = 4949.17, Avg Q Loss = 10.171687, Avg Mu Loss = -702.273608\n",
            "Episode 234: Reward = 4968.55, Avg Q Loss = 10.160261, Avg Mu Loss = -701.955732\n",
            "Episode 235: Reward = 5229.57, Avg Q Loss = 10.035528, Avg Mu Loss = -701.986534\n",
            "Episode 236: Reward = 5311.88, Avg Q Loss = 10.042471, Avg Mu Loss = -702.309883\n",
            "Episode 237: Reward = 5411.40, Avg Q Loss = 10.228257, Avg Mu Loss = -701.996840\n",
            "Episode 238: Reward = 5551.62, Avg Q Loss = 10.368951, Avg Mu Loss = -701.644461\n",
            "Episode 239: Reward = 5521.47, Avg Q Loss = 10.516959, Avg Mu Loss = -701.341708\n",
            "Episode 240: Reward = 5323.78, Avg Q Loss = 10.761772, Avg Mu Loss = -701.216394\n",
            "Episode 241: Reward = 5324.21, Avg Q Loss = 11.017391, Avg Mu Loss = -700.904351\n",
            "Episode 242: Reward = 5605.94, Avg Q Loss = 11.107202, Avg Mu Loss = -700.724369\n",
            "Episode 243: Reward = 5337.80, Avg Q Loss = 11.291304, Avg Mu Loss = -700.288314\n",
            "Episode 244: Reward = 5382.12, Avg Q Loss = 11.449935, Avg Mu Loss = -699.880544\n",
            "Episode 245: Reward = 5437.65, Avg Q Loss = 11.749357, Avg Mu Loss = -699.570885\n",
            "Episode 246: Reward = 5634.44, Avg Q Loss = 11.814972, Avg Mu Loss = -699.457034\n",
            "Episode 247: Reward = 5397.13, Avg Q Loss = 11.524825, Avg Mu Loss = -699.580062\n",
            "Episode 248: Reward = 5568.73, Avg Q Loss = 11.343984, Avg Mu Loss = -699.942622\n",
            "Episode 249: Reward = 5347.49, Avg Q Loss = 11.344818, Avg Mu Loss = -700.083922\n",
            "Episode 250: Reward = 5660.42, Avg Q Loss = 11.176145, Avg Mu Loss = -700.165281\n",
            "Episode 251: Reward = 5430.67, Avg Q Loss = 11.261466, Avg Mu Loss = -700.389508\n",
            "Episode 252: Reward = 5517.47, Avg Q Loss = 11.186519, Avg Mu Loss = -700.442008\n",
            "Episode 253: Reward = 5683.71, Avg Q Loss = 11.141778, Avg Mu Loss = -700.976322\n",
            "Episode 254: Reward = 5841.17, Avg Q Loss = 11.091806, Avg Mu Loss = -701.875971\n",
            "Episode 255: Reward = 5613.70, Avg Q Loss = 11.277072, Avg Mu Loss = -702.565128\n",
            "Episode 256: Reward = 5487.75, Avg Q Loss = 11.238961, Avg Mu Loss = -703.345742\n",
            "Episode 257: Reward = 5393.40, Avg Q Loss = 11.299363, Avg Mu Loss = -703.663345\n",
            "Episode 258: Reward = 5406.35, Avg Q Loss = 11.367333, Avg Mu Loss = -703.654871\n",
            "Episode 259: Reward = 5408.98, Avg Q Loss = 11.405700, Avg Mu Loss = -703.770171\n",
            "Episode 260: Reward = 5004.38, Avg Q Loss = 11.724616, Avg Mu Loss = -703.485698\n",
            "Episode 261: Reward = 4220.35, Avg Q Loss = 12.526761, Avg Mu Loss = -702.846085\n",
            "Episode 262: Reward = 5207.10, Avg Q Loss = 13.341136, Avg Mu Loss = -701.861367\n",
            "Episode 263: Reward = 5278.44, Avg Q Loss = 13.516922, Avg Mu Loss = -701.611736\n",
            "Episode 264: Reward = 5545.15, Avg Q Loss = 13.542665, Avg Mu Loss = -701.594243\n",
            "Episode 265: Reward = 5590.80, Avg Q Loss = 13.142761, Avg Mu Loss = -701.956868\n",
            "Episode 266: Reward = 5301.19, Avg Q Loss = 13.335764, Avg Mu Loss = -701.869632\n",
            "Episode 267: Reward = 5657.99, Avg Q Loss = 13.274510, Avg Mu Loss = -701.948335\n",
            "Episode 268: Reward = 5625.96, Avg Q Loss = 13.121483, Avg Mu Loss = -702.383072\n",
            "Episode 269: Reward = 5325.86, Avg Q Loss = 13.413630, Avg Mu Loss = -702.456887\n",
            "Episode 270: Reward = 5397.96, Avg Q Loss = 13.276735, Avg Mu Loss = -702.427048\n",
            "Episode 271: Reward = 5604.09, Avg Q Loss = 13.283673, Avg Mu Loss = -702.635211\n",
            "Episode 272: Reward = 5246.20, Avg Q Loss = 13.282369, Avg Mu Loss = -702.573538\n",
            "Episode 273: Reward = 5800.57, Avg Q Loss = 13.166524, Avg Mu Loss = -702.766911\n",
            "Episode 274: Reward = 5572.86, Avg Q Loss = 13.402154, Avg Mu Loss = -702.624418\n",
            "Episode 275: Reward = 5579.92, Avg Q Loss = 13.185503, Avg Mu Loss = -702.717835\n",
            "Episode 276: Reward = 5333.24, Avg Q Loss = 13.419853, Avg Mu Loss = -702.690980\n",
            "Episode 277: Reward = 5074.79, Avg Q Loss = 13.291034, Avg Mu Loss = -702.270097\n",
            "Episode 278: Reward = 5326.81, Avg Q Loss = 13.224767, Avg Mu Loss = -702.366288\n",
            "Episode 279: Reward = 5704.68, Avg Q Loss = 12.814831, Avg Mu Loss = -702.803174\n",
            "Episode 280: Reward = 5193.96, Avg Q Loss = 12.602637, Avg Mu Loss = -703.441268\n",
            "Episode 281: Reward = 5366.44, Avg Q Loss = 11.952067, Avg Mu Loss = -704.164518\n",
            "Episode 282: Reward = 5108.06, Avg Q Loss = 11.595663, Avg Mu Loss = -705.008293\n",
            "Episode 283: Reward = 5223.98, Avg Q Loss = 11.781694, Avg Mu Loss = -705.371140\n",
            "Episode 284: Reward = 5160.30, Avg Q Loss = 12.150849, Avg Mu Loss = -705.270096\n",
            "Episode 285: Reward = 5666.56, Avg Q Loss = 12.586463, Avg Mu Loss = -704.833747\n",
            "Episode 286: Reward = 5505.77, Avg Q Loss = 12.617254, Avg Mu Loss = -705.184330\n",
            "Episode 287: Reward = 5260.81, Avg Q Loss = 12.816038, Avg Mu Loss = -705.279397\n",
            "Episode 288: Reward = 5896.19, Avg Q Loss = 13.088183, Avg Mu Loss = -705.144887\n",
            "Episode 289: Reward = 5597.35, Avg Q Loss = 13.079906, Avg Mu Loss = -705.602451\n",
            "Episode 290: Reward = 5424.20, Avg Q Loss = 13.538075, Avg Mu Loss = -705.665993\n",
            "Episode 291: Reward = 5829.74, Avg Q Loss = 13.788347, Avg Mu Loss = -706.189047\n",
            "Episode 292: Reward = 5274.01, Avg Q Loss = 13.973193, Avg Mu Loss = -706.510008\n",
            "Episode 293: Reward = 5521.59, Avg Q Loss = 14.256804, Avg Mu Loss = -706.315649\n",
            "Episode 294: Reward = 5377.51, Avg Q Loss = 14.430159, Avg Mu Loss = -706.451398\n",
            "Episode 295: Reward = 5886.00, Avg Q Loss = 14.342436, Avg Mu Loss = -706.900598\n",
            "Episode 296: Reward = 5768.94, Avg Q Loss = 14.391237, Avg Mu Loss = -707.450900\n",
            "Episode 297: Reward = 5625.09, Avg Q Loss = 14.437220, Avg Mu Loss = -708.378728\n",
            "Episode 298: Reward = 5551.92, Avg Q Loss = 14.632910, Avg Mu Loss = -709.101750\n",
            "Episode 299: Reward = 5508.08, Avg Q Loss = 14.743533, Avg Mu Loss = -709.212383\n",
            "Episode 300: Reward = 5185.59, Avg Q Loss = 14.874797, Avg Mu Loss = -709.165009\n",
            "Episode 301: Reward = 5256.46, Avg Q Loss = 14.870669, Avg Mu Loss = -709.344938\n",
            "Episode 302: Reward = 5364.06, Avg Q Loss = 14.830520, Avg Mu Loss = -709.435973\n",
            "Episode 303: Reward = 5403.87, Avg Q Loss = 14.235530, Avg Mu Loss = -709.604459\n",
            "Episode 304: Reward = 5373.80, Avg Q Loss = 13.785777, Avg Mu Loss = -709.841474\n",
            "Episode 305: Reward = 5440.74, Avg Q Loss = 13.638536, Avg Mu Loss = -709.962219\n",
            "Episode 306: Reward = 5605.07, Avg Q Loss = 13.875382, Avg Mu Loss = -709.747848\n",
            "Episode 307: Reward = 5527.52, Avg Q Loss = 13.944717, Avg Mu Loss = -709.664135\n",
            "Episode 308: Reward = 5731.74, Avg Q Loss = 13.785690, Avg Mu Loss = -709.829546\n",
            "Episode 309: Reward = 5465.07, Avg Q Loss = 13.895501, Avg Mu Loss = -709.771888\n",
            "Episode 310: Reward = 5482.99, Avg Q Loss = 13.569084, Avg Mu Loss = -710.115991\n",
            "Episode 311: Reward = 5514.53, Avg Q Loss = 13.758763, Avg Mu Loss = -709.574970\n",
            "Episode 312: Reward = 4996.84, Avg Q Loss = 13.940160, Avg Mu Loss = -709.635750\n",
            "Episode 313: Reward = 4949.95, Avg Q Loss = 14.315682, Avg Mu Loss = -709.315301\n",
            "Episode 314: Reward = 5412.47, Avg Q Loss = 14.219715, Avg Mu Loss = -709.043515\n",
            "Episode 315: Reward = 5091.36, Avg Q Loss = 14.345463, Avg Mu Loss = -708.429636\n",
            "Episode 316: Reward = 5106.22, Avg Q Loss = 14.177163, Avg Mu Loss = -707.740053\n",
            "Episode 317: Reward = 4861.60, Avg Q Loss = 14.530333, Avg Mu Loss = -707.176248\n",
            "Episode 318: Reward = 4716.16, Avg Q Loss = 15.156730, Avg Mu Loss = -706.060320\n",
            "Episode 319: Reward = 4356.42, Avg Q Loss = 15.974938, Avg Mu Loss = -704.633629\n",
            "Episode 320: Reward = 4763.16, Avg Q Loss = 16.699005, Avg Mu Loss = -703.175854\n",
            "Episode 321: Reward = 5282.52, Avg Q Loss = 17.092746, Avg Mu Loss = -702.670827\n",
            "Episode 322: Reward = 5318.03, Avg Q Loss = 17.118304, Avg Mu Loss = -702.538388\n",
            "Episode 323: Reward = 5423.25, Avg Q Loss = 17.083199, Avg Mu Loss = -702.760691\n",
            "Episode 324: Reward = 5267.35, Avg Q Loss = 17.103804, Avg Mu Loss = -702.776599\n",
            "Episode 325: Reward = 5306.27, Avg Q Loss = 17.147783, Avg Mu Loss = -702.787059\n",
            "Episode 326: Reward = 5430.99, Avg Q Loss = 16.686927, Avg Mu Loss = -703.222214\n",
            "Episode 327: Reward = 5486.25, Avg Q Loss = 16.731966, Avg Mu Loss = -703.392114\n",
            "Episode 328: Reward = 5227.38, Avg Q Loss = 16.576658, Avg Mu Loss = -703.330226\n",
            "Episode 329: Reward = 5261.15, Avg Q Loss = 16.301609, Avg Mu Loss = -703.515229\n",
            "Episode 330: Reward = 5363.79, Avg Q Loss = 15.978810, Avg Mu Loss = -703.700420\n",
            "Episode 331: Reward = 5588.14, Avg Q Loss = 15.556922, Avg Mu Loss = -704.171660\n",
            "Episode 332: Reward = 5818.95, Avg Q Loss = 15.140868, Avg Mu Loss = -704.846555\n",
            "Episode 333: Reward = 5420.87, Avg Q Loss = 14.605955, Avg Mu Loss = -705.967469\n",
            "Episode 334: Reward = 5265.98, Avg Q Loss = 14.263386, Avg Mu Loss = -706.400622\n",
            "Episode 335: Reward = 5613.85, Avg Q Loss = 13.733198, Avg Mu Loss = -707.107706\n",
            "Episode 336: Reward = 5735.52, Avg Q Loss = 13.523255, Avg Mu Loss = -708.103904\n",
            "Episode 337: Reward = 5448.96, Avg Q Loss = 13.024275, Avg Mu Loss = -708.766036\n",
            "Episode 338: Reward = 5507.26, Avg Q Loss = 12.177861, Avg Mu Loss = -710.197622\n",
            "Episode 339: Reward = 5475.53, Avg Q Loss = 11.151696, Avg Mu Loss = -711.688952\n",
            "Episode 340: Reward = 5652.92, Avg Q Loss = 10.174834, Avg Mu Loss = -713.347129\n",
            "Episode 341: Reward = 5862.23, Avg Q Loss = 9.543352, Avg Mu Loss = -714.335913\n",
            "Episode 342: Reward = 5903.76, Avg Q Loss = 9.241533, Avg Mu Loss = -715.089932\n",
            "Episode 343: Reward = 5500.53, Avg Q Loss = 9.266523, Avg Mu Loss = -715.606702\n",
            "Episode 344: Reward = 5520.78, Avg Q Loss = 9.425464, Avg Mu Loss = -715.731515\n",
            "Episode 345: Reward = 5263.97, Avg Q Loss = 10.386342, Avg Mu Loss = -715.801393\n",
            "Episode 346: Reward = 5684.89, Avg Q Loss = 10.429106, Avg Mu Loss = -715.904497\n",
            "Episode 347: Reward = 5673.71, Avg Q Loss = 10.316104, Avg Mu Loss = -716.068267\n",
            "Episode 348: Reward = 5631.31, Avg Q Loss = 10.299579, Avg Mu Loss = -716.274259\n",
            "Episode 349: Reward = 5618.91, Avg Q Loss = 10.293543, Avg Mu Loss = -716.528228\n",
            "Episode 350: Reward = 5125.63, Avg Q Loss = 10.975856, Avg Mu Loss = -716.388860\n",
            "Episode 351: Reward = 5205.84, Avg Q Loss = 11.054972, Avg Mu Loss = -716.074801\n",
            "Episode 352: Reward = 5605.23, Avg Q Loss = 11.221926, Avg Mu Loss = -715.526189\n",
            "Episode 353: Reward = 5398.40, Avg Q Loss = 11.614831, Avg Mu Loss = -715.121549\n",
            "Episode 354: Reward = 5643.25, Avg Q Loss = 11.746381, Avg Mu Loss = -714.889463\n",
            "Episode 355: Reward = 5412.38, Avg Q Loss = 11.893400, Avg Mu Loss = -714.736995\n",
            "Episode 356: Reward = 5401.19, Avg Q Loss = 12.052214, Avg Mu Loss = -714.437418\n",
            "Episode 357: Reward = 5672.68, Avg Q Loss = 12.051686, Avg Mu Loss = -714.330307\n",
            "Episode 358: Reward = 5273.79, Avg Q Loss = 12.171007, Avg Mu Loss = -714.406219\n",
            "Episode 359: Reward = 5336.86, Avg Q Loss = 12.381452, Avg Mu Loss = -714.371808\n",
            "Episode 360: Reward = 5108.88, Avg Q Loss = 12.727331, Avg Mu Loss = -714.008121\n",
            "Episode 361: Reward = 5520.61, Avg Q Loss = 12.962190, Avg Mu Loss = -713.830715\n",
            "Episode 362: Reward = 5570.63, Avg Q Loss = 12.917006, Avg Mu Loss = -713.735692\n",
            "Episode 363: Reward = 5783.36, Avg Q Loss = 12.794101, Avg Mu Loss = -713.829803\n",
            "Episode 364: Reward = 5831.49, Avg Q Loss = 12.433644, Avg Mu Loss = -714.275042\n",
            "Episode 365: Reward = 5696.32, Avg Q Loss = 11.867848, Avg Mu Loss = -714.996828\n",
            "Episode 366: Reward = 5512.39, Avg Q Loss = 11.831163, Avg Mu Loss = -715.248180\n",
            "Episode 367: Reward = 5280.47, Avg Q Loss = 11.931052, Avg Mu Loss = -714.933863\n",
            "Episode 368: Reward = 5571.95, Avg Q Loss = 11.851932, Avg Mu Loss = -714.851883\n",
            "Episode 369: Reward = 5808.71, Avg Q Loss = 11.375394, Avg Mu Loss = -715.180354\n",
            "Episode 370: Reward = 5333.31, Avg Q Loss = 11.070927, Avg Mu Loss = -715.306893\n",
            "Episode 371: Reward = 5333.95, Avg Q Loss = 10.656261, Avg Mu Loss = -715.547640\n",
            "Episode 372: Reward = 5550.14, Avg Q Loss = 10.520227, Avg Mu Loss = -715.437184\n",
            "Episode 373: Reward = 5362.30, Avg Q Loss = 10.416341, Avg Mu Loss = -715.549982\n",
            "Episode 374: Reward = 5248.25, Avg Q Loss = 10.380007, Avg Mu Loss = -715.461302\n",
            "Episode 375: Reward = 5546.15, Avg Q Loss = 10.620285, Avg Mu Loss = -715.201422\n",
            "Episode 376: Reward = 5261.86, Avg Q Loss = 10.659780, Avg Mu Loss = -714.727508\n",
            "Episode 377: Reward = 5130.56, Avg Q Loss = 10.836250, Avg Mu Loss = -714.109287\n",
            "Episode 378: Reward = 5455.49, Avg Q Loss = 11.032093, Avg Mu Loss = -713.554264\n",
            "Episode 379: Reward = 5538.38, Avg Q Loss = 11.406880, Avg Mu Loss = -713.363922\n",
            "Episode 380: Reward = 5960.42, Avg Q Loss = 11.431156, Avg Mu Loss = -713.587859\n",
            "Episode 381: Reward = 5175.64, Avg Q Loss = 11.168310, Avg Mu Loss = -713.743727\n",
            "Episode 382: Reward = 5192.49, Avg Q Loss = 11.326524, Avg Mu Loss = -713.036978\n",
            "Episode 383: Reward = 5167.02, Avg Q Loss = 11.341228, Avg Mu Loss = -712.413771\n",
            "Episode 384: Reward = 5972.89, Avg Q Loss = 11.349969, Avg Mu Loss = -712.191470\n",
            "Episode 385: Reward = 4864.15, Avg Q Loss = 11.381218, Avg Mu Loss = -711.514925\n",
            "Episode 386: Reward = 5799.89, Avg Q Loss = 11.424729, Avg Mu Loss = -710.932794\n",
            "Episode 387: Reward = 5962.34, Avg Q Loss = 11.325297, Avg Mu Loss = -711.392931\n",
            "Episode 388: Reward = 5447.62, Avg Q Loss = 11.266643, Avg Mu Loss = -711.281696\n",
            "Episode 389: Reward = 5536.67, Avg Q Loss = 11.180107, Avg Mu Loss = -710.855342\n",
            "Episode 390: Reward = 5688.14, Avg Q Loss = 11.024083, Avg Mu Loss = -710.836681\n",
            "Episode 391: Reward = 5623.66, Avg Q Loss = 10.751552, Avg Mu Loss = -710.592955\n",
            "Episode 392: Reward = 5692.21, Avg Q Loss = 10.976014, Avg Mu Loss = -710.564841\n",
            "Episode 393: Reward = 5816.84, Avg Q Loss = 10.935265, Avg Mu Loss = -710.526240\n",
            "Episode 394: Reward = 5619.60, Avg Q Loss = 11.342179, Avg Mu Loss = -710.652330\n",
            "Episode 395: Reward = 5158.68, Avg Q Loss = 11.623111, Avg Mu Loss = -710.323393\n",
            "Episode 396: Reward = 4798.42, Avg Q Loss = 12.237771, Avg Mu Loss = -709.828607\n",
            "Episode 397: Reward = 4932.23, Avg Q Loss = 12.992852, Avg Mu Loss = -708.819084\n",
            "Episode 398: Reward = 5213.22, Avg Q Loss = 13.232799, Avg Mu Loss = -708.441617\n",
            "Episode 399: Reward = 5285.28, Avg Q Loss = 13.264146, Avg Mu Loss = -708.092682\n",
            "Episode 400: Reward = 5151.41, Avg Q Loss = 13.464341, Avg Mu Loss = -707.323550\n",
            "Episode 401: Reward = 5395.54, Avg Q Loss = 13.397544, Avg Mu Loss = -706.764930\n",
            "Episode 402: Reward = 5174.22, Avg Q Loss = 13.154836, Avg Mu Loss = -706.763847\n",
            "Episode 403: Reward = 5355.50, Avg Q Loss = 12.897585, Avg Mu Loss = -706.593290\n",
            "Episode 404: Reward = 5246.09, Avg Q Loss = 12.989656, Avg Mu Loss = -706.066304\n",
            "Episode 405: Reward = 3512.34, Avg Q Loss = 13.828061, Avg Mu Loss = -704.986128\n",
            "Episode 406: Reward = 5159.28, Avg Q Loss = 14.430236, Avg Mu Loss = -701.504967\n",
            "Episode 407: Reward = 5369.96, Avg Q Loss = 14.254091, Avg Mu Loss = -700.507273\n",
            "Episode 408: Reward = 4971.53, Avg Q Loss = 14.175387, Avg Mu Loss = -700.105876\n",
            "Episode 409: Reward = 5164.16, Avg Q Loss = 14.465291, Avg Mu Loss = -699.282209\n",
            "Episode 410: Reward = 5573.98, Avg Q Loss = 14.598172, Avg Mu Loss = -698.987793\n",
            "Episode 411: Reward = 5321.13, Avg Q Loss = 14.790382, Avg Mu Loss = -698.709837\n",
            "Episode 412: Reward = 5524.18, Avg Q Loss = 14.928290, Avg Mu Loss = -698.301680\n",
            "Episode 413: Reward = 4973.65, Avg Q Loss = 15.063407, Avg Mu Loss = -697.666060\n",
            "Episode 414: Reward = 225.16, Avg Q Loss = 15.477961, Avg Mu Loss = -692.032909\n",
            "Episode 415: Reward = 5466.95, Avg Q Loss = 14.763864, Avg Mu Loss = -686.034711\n",
            "Episode 416: Reward = 5182.28, Avg Q Loss = 14.354765, Avg Mu Loss = -685.598507\n",
            "Episode 417: Reward = 5207.17, Avg Q Loss = 13.730090, Avg Mu Loss = -685.551198\n",
            "Episode 418: Reward = 1326.68, Avg Q Loss = 13.877591, Avg Mu Loss = -681.883666\n",
            "Episode 419: Reward = 5409.48, Avg Q Loss = 14.087103, Avg Mu Loss = -675.646026\n",
            "Episode 420: Reward = 5348.46, Avg Q Loss = 14.126955, Avg Mu Loss = -676.002448\n",
            "Episode 421: Reward = 3378.72, Avg Q Loss = 14.353071, Avg Mu Loss = -674.967553\n",
            "Episode 422: Reward = 5112.92, Avg Q Loss = 15.849150, Avg Mu Loss = -671.591472\n",
            "Episode 423: Reward = 5183.83, Avg Q Loss = 15.695115, Avg Mu Loss = -670.461666\n",
            "Episode 424: Reward = 5656.13, Avg Q Loss = 15.279357, Avg Mu Loss = -670.416510\n",
            "Episode 425: Reward = 5674.42, Avg Q Loss = 15.277042, Avg Mu Loss = -671.191585\n",
            "Episode 426: Reward = 5441.68, Avg Q Loss = 15.109867, Avg Mu Loss = -674.762133\n",
            "Episode 427: Reward = 5436.72, Avg Q Loss = 15.240171, Avg Mu Loss = -674.708607\n",
            "Episode 428: Reward = 363.07, Avg Q Loss = 15.618949, Avg Mu Loss = -669.440233\n",
            "Episode 429: Reward = 5657.73, Avg Q Loss = 15.468521, Avg Mu Loss = -662.946834\n",
            "Episode 430: Reward = 5645.65, Avg Q Loss = 15.045493, Avg Mu Loss = -662.141913\n",
            "Episode 431: Reward = 5763.01, Avg Q Loss = 14.993249, Avg Mu Loss = -661.699838\n",
            "Episode 432: Reward = 5424.23, Avg Q Loss = 15.349917, Avg Mu Loss = -661.482007\n",
            "Episode 433: Reward = 5763.81, Avg Q Loss = 15.226700, Avg Mu Loss = -661.453746\n",
            "Episode 434: Reward = 5074.22, Avg Q Loss = 15.448678, Avg Mu Loss = -667.568278\n",
            "Episode 435: Reward = 1250.13, Avg Q Loss = 16.363230, Avg Mu Loss = -670.420374\n",
            "Episode 436: Reward = 4662.44, Avg Q Loss = 16.867277, Avg Mu Loss = -663.258572\n",
            "Episode 437: Reward = 4922.00, Avg Q Loss = 18.158482, Avg Mu Loss = -662.128793\n",
            "Episode 438: Reward = 5156.37, Avg Q Loss = 19.428404, Avg Mu Loss = -665.896616\n",
            "Episode 439: Reward = 5077.51, Avg Q Loss = 20.549055, Avg Mu Loss = -671.980727\n",
            "Episode 440: Reward = 5665.90, Avg Q Loss = 20.549023, Avg Mu Loss = -671.545513\n",
            "Episode 441: Reward = 5378.53, Avg Q Loss = 20.536681, Avg Mu Loss = -672.650355\n",
            "Episode 442: Reward = 5387.01, Avg Q Loss = 19.535009, Avg Mu Loss = -676.437379\n",
            "Episode 443: Reward = 5276.06, Avg Q Loss = 19.082386, Avg Mu Loss = -676.608052\n",
            "Episode 444: Reward = 5281.97, Avg Q Loss = 19.168935, Avg Mu Loss = -676.756425\n",
            "Episode 445: Reward = 5267.81, Avg Q Loss = 18.409921, Avg Mu Loss = -676.378998\n",
            "Episode 446: Reward = 5532.13, Avg Q Loss = 18.377685, Avg Mu Loss = -676.691587\n",
            "Episode 447: Reward = 3256.13, Avg Q Loss = 18.524697, Avg Mu Loss = -675.422439\n",
            "Episode 448: Reward = 5249.62, Avg Q Loss = 18.550182, Avg Mu Loss = -675.913467\n",
            "Episode 449: Reward = 5495.11, Avg Q Loss = 18.314731, Avg Mu Loss = -683.535234\n",
            "Episode 450: Reward = 5792.42, Avg Q Loss = 18.054027, Avg Mu Loss = -683.801498\n",
            "Episode 451: Reward = 5039.87, Avg Q Loss = 17.663428, Avg Mu Loss = -683.801505\n",
            "Episode 452: Reward = 5405.42, Avg Q Loss = 17.013673, Avg Mu Loss = -683.221809\n",
            "Episode 453: Reward = 5044.78, Avg Q Loss = 17.767733, Avg Mu Loss = -682.501096\n",
            "Episode 454: Reward = 5488.20, Avg Q Loss = 17.087701, Avg Mu Loss = -682.479364\n",
            "Episode 455: Reward = 5555.50, Avg Q Loss = 16.579496, Avg Mu Loss = -686.894248\n",
            "Episode 456: Reward = 5407.17, Avg Q Loss = 16.044775, Avg Mu Loss = -694.514626\n",
            "Episode 457: Reward = 5340.89, Avg Q Loss = 15.005100, Avg Mu Loss = -695.670674\n",
            "Episode 458: Reward = 5485.29, Avg Q Loss = 14.516844, Avg Mu Loss = -695.981342\n",
            "Episode 459: Reward = 5563.88, Avg Q Loss = 13.523297, Avg Mu Loss = -697.020296\n",
            "Episode 460: Reward = 5384.56, Avg Q Loss = 13.498062, Avg Mu Loss = -696.914555\n",
            "Episode 461: Reward = 485.59, Avg Q Loss = 13.696561, Avg Mu Loss = -691.082918\n",
            "Episode 462: Reward = 5154.15, Avg Q Loss = 13.312757, Avg Mu Loss = -683.245922\n",
            "Episode 463: Reward = 5059.82, Avg Q Loss = 13.284515, Avg Mu Loss = -682.987128\n",
            "Episode 464: Reward = 5159.48, Avg Q Loss = 13.550874, Avg Mu Loss = -682.114564\n",
            "Episode 465: Reward = 1969.68, Avg Q Loss = 13.748704, Avg Mu Loss = -679.158654\n",
            "Episode 466: Reward = 5204.92, Avg Q Loss = 13.956501, Avg Mu Loss = -671.837370\n",
            "Episode 467: Reward = 5488.08, Avg Q Loss = 14.016145, Avg Mu Loss = -673.065374\n",
            "Episode 468: Reward = 108.25, Avg Q Loss = 13.914362, Avg Mu Loss = -671.249778\n",
            "Episode 469: Reward = 5128.51, Avg Q Loss = 13.632261, Avg Mu Loss = -662.900736\n",
            "Episode 470: Reward = 5639.25, Avg Q Loss = 14.004639, Avg Mu Loss = -662.463529\n",
            "Episode 471: Reward = 885.15, Avg Q Loss = 14.530704, Avg Mu Loss = -656.783309\n",
            "Episode 472: Reward = 3688.37, Avg Q Loss = 15.446704, Avg Mu Loss = -647.125467\n",
            "Episode 473: Reward = 3999.83, Avg Q Loss = 16.497878, Avg Mu Loss = -641.556909\n",
            "Episode 474: Reward = 4383.33, Avg Q Loss = 17.684454, Avg Mu Loss = -637.314413\n",
            "Episode 475: Reward = 4760.85, Avg Q Loss = 19.950849, Avg Mu Loss = -634.757905\n",
            "Episode 476: Reward = 4481.25, Avg Q Loss = 21.116469, Avg Mu Loss = -633.018128\n",
            "Episode 477: Reward = 4621.66, Avg Q Loss = 22.712025, Avg Mu Loss = -631.183390\n",
            "Episode 478: Reward = 4953.81, Avg Q Loss = 23.799150, Avg Mu Loss = -629.247631\n",
            "Episode 479: Reward = 4908.49, Avg Q Loss = 24.615481, Avg Mu Loss = -627.444477\n",
            "Episode 480: Reward = 5154.52, Avg Q Loss = 23.400928, Avg Mu Loss = -626.548328\n",
            "Episode 481: Reward = 5236.45, Avg Q Loss = 23.354088, Avg Mu Loss = -632.721215\n",
            "Episode 482: Reward = 5174.16, Avg Q Loss = 23.973249, Avg Mu Loss = -640.583689\n",
            "Episode 483: Reward = 4563.62, Avg Q Loss = 24.026132, Avg Mu Loss = -639.586750\n",
            "Episode 484: Reward = 4630.61, Avg Q Loss = 25.545348, Avg Mu Loss = -639.176603\n",
            "Episode 485: Reward = 5045.58, Avg Q Loss = 24.734703, Avg Mu Loss = -640.581252\n",
            "Episode 486: Reward = 5171.45, Avg Q Loss = 24.780693, Avg Mu Loss = -648.190632\n",
            "Episode 487: Reward = 5018.17, Avg Q Loss = 24.298911, Avg Mu Loss = -647.725661\n",
            "Episode 488: Reward = 4940.53, Avg Q Loss = 23.463645, Avg Mu Loss = -653.586285\n",
            "Episode 489: Reward = 5363.64, Avg Q Loss = 23.185272, Avg Mu Loss = -662.846662\n",
            "Episode 490: Reward = 5085.04, Avg Q Loss = 22.702137, Avg Mu Loss = -662.268625\n",
            "Episode 491: Reward = 4956.62, Avg Q Loss = 22.953139, Avg Mu Loss = -666.938126\n",
            "Episode 492: Reward = 5107.00, Avg Q Loss = 22.008068, Avg Mu Loss = -676.992471\n",
            "Episode 493: Reward = 5075.40, Avg Q Loss = 20.700192, Avg Mu Loss = -681.516913\n",
            "Episode 494: Reward = 5555.37, Avg Q Loss = 19.100794, Avg Mu Loss = -685.738810\n",
            "Episode 495: Reward = 5163.73, Avg Q Loss = 16.948076, Avg Mu Loss = -686.051545\n",
            "Episode 496: Reward = 5661.82, Avg Q Loss = 15.852188, Avg Mu Loss = -686.128601\n",
            "Episode 497: Reward = 4982.07, Avg Q Loss = 14.951950, Avg Mu Loss = -686.161309\n",
            "Episode 498: Reward = 5311.26, Avg Q Loss = 14.979441, Avg Mu Loss = -686.047539\n",
            "Episode 499: Reward = 5200.42, Avg Q Loss = 14.857842, Avg Mu Loss = -685.540333\n",
            "Episode 500: Reward = 5317.61, Avg Q Loss = 14.631189, Avg Mu Loss = -685.030581\n",
            "Training Done. Models and log saved.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import os\n",
        "\n",
        "# ----------------------------\n",
        "# Custom Reward Wrapper\n",
        "# ----------------------------\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, original_reward, terminated, truncated, info = self.env.step(action)\n",
        "        reward = self.modified_reward_function(observation, action, original_reward)\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def modified_reward_function(self, observation, action, original_reward):\n",
        "        # Custom reward components:\n",
        "        # 1. Forward velocity reward (using x-coordinate velocity, index 8)\n",
        "        forward_reward = 1.0 * observation[8]\n",
        "\n",
        "        # 2. Penalize excessive vertical movement (z-coordinate, index 0)\n",
        "        height_penalty = -0.05 * abs(observation[0] - 0.5)\n",
        "\n",
        "        # 3. Penalize excessive rotations for stability (angle of second rotor, index 2)\n",
        "        rotation_penalty = -0.1 * abs(observation[2])\n",
        "\n",
        "        # 4. Energy efficiency - penalize excessive joint movements (angular velocities indices 10-16)\n",
        "        energy_penalty = -0.001 * sum(abs(observation[i]) for i in range(10, 17))\n",
        "\n",
        "        # 5. Smooth control - penalize large action changes\n",
        "        control_penalty = -0.01 * np.sum(np.square(action))\n",
        "\n",
        "        # Balance the original reward with custom components (weight of original reward: 0.5)\n",
        "        original_reward_weight = 0.5\n",
        "        reward = (\n",
        "            forward_reward +\n",
        "            height_penalty +\n",
        "            rotation_penalty +\n",
        "            energy_penalty +\n",
        "            control_penalty +\n",
        "            original_reward_weight * original_reward\n",
        "        )\n",
        "\n",
        "        return reward\n",
        "\n",
        "# ----------------------------\n",
        "# Create the Environment\n",
        "# ----------------------------\n",
        "# Create the HalfCheetah environment (using v5 here, adjust if needed)\n",
        "env = gym.make(\"HalfCheetah-v5\", render_mode=None)\n",
        "# Wrap the environment to override the reward function\n",
        "env = CustomRewardWrapper(env)\n",
        "\n",
        "# Get state and action dimensions from the environment\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "# Set up device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# Define the Networks\n",
        "# ----------------------------\n",
        "# Critic network (QNet)\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        # Input dimension: state_dim + action_dim\n",
        "        self.hidden = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        x = torch.cat((s, a), dim=-1)\n",
        "        x = self.hidden(x)\n",
        "        x = F.relu(x)\n",
        "        return self.output(x)\n",
        "\n",
        "# Actor network (PolicyNet)\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        # Input dimension: state_dim; output dimension: action_dim\n",
        "        self.hidden = nn.Linear(state_dim, hidden_dim)\n",
        "        self.output = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, s):\n",
        "        x = self.hidden(s)\n",
        "        x = F.relu(x)\n",
        "        x = self.output(x)\n",
        "        return torch.tanh(x)\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Networks and Targets\n",
        "# ----------------------------\n",
        "q_origin_model = QNet(state_dim, action_dim).to(device)\n",
        "q_target_model = QNet(state_dim, action_dim).to(device)\n",
        "_ = q_target_model.requires_grad_(False)\n",
        "\n",
        "mu_origin_model = PolicyNet(state_dim, action_dim).to(device)\n",
        "mu_target_model = PolicyNet(state_dim, action_dim).to(device)\n",
        "_ = mu_target_model.requires_grad_(False)\n",
        "\n",
        "# ----------------------------\n",
        "# Hyperparameters and Optimizers\n",
        "# ----------------------------\n",
        "gamma = 0.99\n",
        "opt_q = torch.optim.AdamW(q_origin_model.parameters(), lr=0.0005)\n",
        "opt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n",
        "\n",
        "def optimize(states, actions, rewards, next_states, dones):\n",
        "    # Convert lists to tensors.\n",
        "    states = torch.tensor(np.stack(states), dtype=torch.float).to(device)\n",
        "    actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float).unsqueeze(dim=1).to(device)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
        "    dones = torch.tensor(dones, dtype=torch.float).unsqueeze(dim=1).to(device)\n",
        "\n",
        "    # Compute target for the critic network.\n",
        "    with torch.no_grad():\n",
        "        next_actions = mu_target_model(next_states)\n",
        "        target = rewards + (1 - dones) * gamma * q_target_model(next_states, next_actions)\n",
        "\n",
        "    # Compute Critic loss.\n",
        "    q_value = q_origin_model(states, actions)\n",
        "    q_loss = F.mse_loss(q_value, target.detach())\n",
        "    opt_q.zero_grad()\n",
        "    q_loss.backward()\n",
        "    opt_q.step()\n",
        "\n",
        "    # Compute Actor loss: maximize Q(s, mu(s)) -> minimize -Q(s, mu(s))\n",
        "    mu_value = mu_origin_model(states)\n",
        "    q_value_for_mu = q_origin_model(states, mu_value)\n",
        "    mu_loss = -q_value_for_mu.mean()\n",
        "    opt_mu.zero_grad()\n",
        "    mu_loss.backward()\n",
        "    opt_mu.step()\n",
        "\n",
        "    # Ensure gradients remain enabled.\n",
        "    for p in q_origin_model.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    return q_loss.item(), mu_loss.item()\n",
        "\n",
        "tau = 0.002\n",
        "def update_target():\n",
        "    # Soft update target networks.\n",
        "    for var, var_target in zip(q_origin_model.parameters(), q_target_model.parameters()):\n",
        "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
        "    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n",
        "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
        "\n",
        "# ----------------------------\n",
        "# Replay Buffer\n",
        "# ----------------------------\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size: int):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def add(self, item):\n",
        "        if len(self.buffer) == self.buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(item)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        items = random.sample(self.buffer, batch_size)\n",
        "        states   = [i[0] for i in items]\n",
        "        actions  = [i[1] for i in items]\n",
        "        rewards  = [i[2] for i in items]\n",
        "        n_states = [i[3] for i in items]\n",
        "        dones    = [i[4] for i in items]\n",
        "        return states, actions, rewards, n_states, dones\n",
        "\n",
        "    def length(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "buffer = ReplayBuffer(buffer_size=20000)\n",
        "\n",
        "# ----------------------------\n",
        "# Ornstein-Uhlenbeck Noise for Exploration\n",
        "# ----------------------------\n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "    def __init__(self, mu, sigma, theta=0.15, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "ou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim), sigma=np.ones(action_dim) * 0.05)\n",
        "\n",
        "def pick_sample(s):\n",
        "    with torch.no_grad():\n",
        "        s_np = np.array(s)\n",
        "        s_batch = np.expand_dims(s_np, axis=0)\n",
        "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
        "        action_det = mu_origin_model(s_batch).squeeze(0)\n",
        "        noise = ou_action_noise()\n",
        "        action = action_det.cpu().numpy() + noise\n",
        "        action = np.clip(action, -1.0, 1.0)\n",
        "        return action\n",
        "\n",
        "# ----------------------------\n",
        "# Training Loop Parameters and Logging\n",
        "# ----------------------------\n",
        "batch_size = 250\n",
        "num_episodes = 500\n",
        "\n",
        "log_file_path = \"training_log.txt\"\n",
        "log_file = open(log_file_path, \"w\")\n",
        "log_file.write(\"episode,reward,avg_q_loss,avg_mu_loss\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    reward_records = []\n",
        "    for ep in range(num_episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        cum_reward = 0\n",
        "        episode_q_loss = 0\n",
        "        episode_mu_loss = 0\n",
        "        training_steps = 0\n",
        "\n",
        "        while not done:\n",
        "            a = pick_sample(s)\n",
        "            s_next, r, term, trunc, _ = env.step(a)\n",
        "            done = term or trunc\n",
        "            buffer.add([s, a, r, s_next, float(term)])\n",
        "            cum_reward += r\n",
        "\n",
        "            if buffer.length() >= batch_size:\n",
        "                states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n",
        "                q_loss_val, mu_loss_val = optimize(states, actions, rewards, n_states, dones)\n",
        "                update_target()\n",
        "                episode_q_loss += q_loss_val\n",
        "                episode_mu_loss += mu_loss_val\n",
        "                training_steps += 1\n",
        "\n",
        "            s = s_next\n",
        "\n",
        "        if training_steps > 0:\n",
        "            avg_q_loss = episode_q_loss / training_steps\n",
        "            avg_mu_loss = episode_mu_loss / training_steps\n",
        "        else:\n",
        "            avg_q_loss = 0.0\n",
        "            avg_mu_loss = 0.0\n",
        "\n",
        "        reward_records.append(cum_reward)\n",
        "        log_line = f\"{ep+1},{cum_reward}\\n\"\n",
        "        log_file.write(log_line)\n",
        "        log_file.flush()\n",
        "        print(f\"Episode {ep+1}: Reward = {cum_reward:.2f}, Avg Q Loss = {avg_q_loss:.6f}, Avg Mu Loss = {avg_mu_loss:.6f}\")\n",
        "\n",
        "    os.makedirs(\"saved_models\", exist_ok=True)\n",
        "    torch.save(mu_origin_model.state_dict(), os.path.join(\"saved_models\", \"mu_origin_model.pth\"))\n",
        "    torch.save(q_origin_model.state_dict(), os.path.join(\"saved_models\", \"q_origin_model.pth\"))\n",
        "    print(\"Training Done. Models and log saved.\")\n",
        "\n",
        "    log_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gymnasium[mujoco]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JCTZ_lyND8-",
        "outputId": "2132487c-edc6-41d1-feac-854e22ff0886"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_sIwEQG6NM3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}